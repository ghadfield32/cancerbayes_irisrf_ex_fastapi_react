{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2da9509",
   "metadata": {},
   "source": [
    "1) create a fake dataset like I was just sent out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d11f7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample CSV saved at notebooks/5080_gpu/interview_prep/data/fake_basketball.csv\n",
      "   player_id      name  points  assists  rebounds  minutes     team\n",
      "0          1  Player_1      11      8.0         6       28  Celtics\n",
      "1          2  Player_2      14      2.0         3       22   Lakers\n",
      "2          3  Player_3       8      3.0         4       37  Celtics\n",
      "3          4  Player_4      13      5.0         6       16     Mavs\n",
      "4          5  Player_5      16      6.0         5       24     Mavs\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# --- 1. Create some fake basketball-style data ---\n",
    "np.random.seed(42)\n",
    "n = 200\n",
    "data = {\n",
    "    \"player_id\": np.arange(1, n+1),\n",
    "    \"name\": [f\"Player_{i}\" for i in range(1, n+1)],\n",
    "    \"points\": np.random.poisson(12, n),\n",
    "    \"assists\": np.random.poisson(4, n),\n",
    "    \"rebounds\": np.random.poisson(6, n),\n",
    "    \"minutes\": np.random.randint(10, 40, n),\n",
    "    \"team\": np.random.choice([\"Heat\", \"Mavs\", \"Lakers\", \"Celtics\"], n),\n",
    "}\n",
    "\n",
    "df_fake = pd.DataFrame(data)\n",
    "\n",
    "# Add some missing values + outliers\n",
    "df_fake.loc[5:10, \"assists\"] = np.nan\n",
    "df_fake.loc[15, \"points\"] = 100   # outlier\n",
    "df_fake.loc[25, \"rebounds\"] = 0   # edge case\n",
    "\n",
    "# --- 2. Save as if we just got it from somewhere ---\n",
    "Path(\"data\").mkdir(exist_ok=True)\n",
    "csv_path = Path(\"notebooks/5080_gpu/interview_prep/data/fake_basketball.csv\")\n",
    "df_fake.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"Sample CSV saved at {csv_path}\")\n",
    "print(df_fake.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7bc466",
   "metadata": {},
   "source": [
    "# upgraded fake dataset for PER, VORP, EWA, PER, PIE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8036e27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# --- 0) Config & helpers ---\n",
    "rng = np.random.default_rng(42)\n",
    "TEAMS = [\"HEA\", \"MAV\", \"LAL\", \"BOS\"]     # tiny league for demo\n",
    "N_PLAYERS = 200\n",
    "SEASONS = [\"2023-24\", \"2024-25\"]\n",
    "GAMES_PER_SEASON = 120  # total games in this tiny league\n",
    "\n",
    "def season_from_date(dt: pd.Timestamp) -> str:\n",
    "    y = dt.year if dt.month >= 8 else dt.year - 1\n",
    "    return f\"{y}-{str((y+1) % 100).zfill(2)}\"\n",
    "\n",
    "# --- 1) Players master (id, name, team, position) ---\n",
    "positions = [\"PG\", \"SG\", \"SF\", \"PF\", \"C\"]\n",
    "players = pd.DataFrame({\n",
    "    \"player_id\": np.arange(1, N_PLAYERS+1),\n",
    "    \"name\": [f\"Player_{i}\" for i in range(1, N_PLAYERS+1)],\n",
    "    \"team_id\": rng.choice(TEAMS, N_PLAYERS),\n",
    "    \"position\": rng.choice(positions, N_PLAYERS, p=[0.22, 0.22, 0.20, 0.20, 0.16])\n",
    "})\n",
    "\n",
    "# --- 2) Schedule + team-game totals ---\n",
    "rows_team_game = []\n",
    "rows_player_game = []\n",
    "\n",
    "for season in SEASONS:\n",
    "    # pick a date window for the season\n",
    "    start = pd.Timestamp(int(season[:4]), 10, 1)\n",
    "    dates = start + pd.to_timedelta(rng.integers(0, 180, size=GAMES_PER_SEASON), unit=\"D\")\n",
    "\n",
    "    for gid, gdate in enumerate(dates, start=1):\n",
    "        # two distinct teams\n",
    "        home, away = rng.choice(TEAMS, 2, replace=False)\n",
    "        for is_home, team, opp in [(1, home, away), (0, away, home)]:\n",
    "            # generate plausible team totals\n",
    "            fga = rng.integers(75, 100)\n",
    "            three_pa = rng.integers(int(0.28*fga), int(0.45*fga))\n",
    "            two_pa = max(fga - three_pa, 0)\n",
    "\n",
    "            two_fg = rng.binomial(two_pa, rng.uniform(0.48, 0.58))\n",
    "            three_pm = rng.binomial(three_pa, rng.uniform(0.33, 0.41))\n",
    "            fgm = two_fg + three_pm\n",
    "\n",
    "            fta = rng.integers(15, 30)\n",
    "            ftm = rng.binomial(fta, rng.uniform(0.72, 0.85))\n",
    "\n",
    "            orb = rng.integers(8, 15)\n",
    "            drb = rng.integers(25, 35)\n",
    "            trb = orb + drb\n",
    "\n",
    "            ast = rng.integers(18, 30)\n",
    "            stl = rng.integers(5, 10)\n",
    "            blk = rng.integers(3, 8)\n",
    "            tov = rng.integers(10, 18)\n",
    "            pf  = rng.integers(15, 24)\n",
    "            pts = int(2*two_fg + 3*three_pm + ftm)\n",
    "\n",
    "            rows_team_game.append({\n",
    "                \"team_id\": team, \"opp_team_id\": opp,\n",
    "                \"game_id\": f\"{season.replace('-','')}_{gid}\",\n",
    "                \"game_date\": gdate, \"season\": season,\n",
    "                \"is_home\": bool(is_home),\n",
    "                \"minutes_team\": 240,\n",
    "                \"FGM\": fgm, \"FGA\": fga, \"3PM\": three_pm, \"3PA\": three_pa,\n",
    "                \"FTM\": ftm, \"FTA\": fta,\n",
    "                \"ORB\": orb, \"DRB\": drb, \"TRB\": trb,\n",
    "                \"AST\": ast, \"STL\": stl, \"BLK\": blk, \"TOV\": tov, \"PF\": pf, \"PTS\": pts\n",
    "            })\n",
    "\n",
    "            # distribute team totals to 8-10 players (very rough minutes share)\n",
    "            roster = players.loc[players[\"team_id\"] == team, \"player_id\"].tolist()\n",
    "            if len(roster) < 10:\n",
    "                roster = rng.choice(players[\"player_id\"], 10, replace=False).tolist()\n",
    "            game_players = rng.choice(roster, size=rng.integers(8, 11), replace=False)\n",
    "\n",
    "            minute_shares = rng.dirichlet(np.ones(len(game_players))) * 240\n",
    "            # clip to realistic on-court minutes\n",
    "            minute_shares = np.clip(np.round(minute_shares, 1), 8, 42)\n",
    "            minute_shares *= 240.0 / minute_shares.sum()\n",
    "\n",
    "            def split_stat(total, shares):\n",
    "                # integer-ish split with sum preserved\n",
    "                raw = shares / shares.sum() * total\n",
    "                vals = np.floor(raw).astype(int)\n",
    "                for k in range(total - vals.sum()):\n",
    "                    vals[k % len(vals)] += 1\n",
    "                return vals\n",
    "\n",
    "            # allocate counting stats roughly by minutes\n",
    "            alloc_cols = [\"FGM\",\"FGA\",\"3PM\",\"3PA\",\"FTM\",\"FTA\",\"ORB\",\"DRB\",\"AST\",\"STL\",\"BLK\",\"TOV\",\"PF\",\"PTS\"]\n",
    "            alloc = {c: split_stat(int(rows_team_game[-1][c]), minute_shares) for c in alloc_cols}\n",
    "            # derive TRB per player from ORB+DRB later\n",
    "\n",
    "            for pid, mp, i in zip(game_players, minute_shares, range(len(game_players))):\n",
    "                p_orb = int(alloc[\"ORB\"][i]); p_drb = int(alloc[\"DRB\"][i])\n",
    "                rows_player_game.append({\n",
    "                    \"player_id\": pid,\n",
    "                    \"name\": players.loc[players.player_id==pid, \"name\"].iloc[0],\n",
    "                    \"team_id\": team, \"opp_team_id\": opp,\n",
    "                    \"game_id\": f\"{season.replace('-','')}_{gid}\",\n",
    "                    \"game_date\": gdate, \"season\": season,\n",
    "                    \"home_away\": \"H\" if is_home else \"A\",\n",
    "                    \"minutes\": float(np.round(mp,1)),\n",
    "                    \"FGM\": int(alloc[\"FGM\"][i]), \"FGA\": int(alloc[\"FGA\"][i]),\n",
    "                    \"3PM\": int(alloc[\"3PM\"][i]), \"3PA\": int(alloc[\"3PA\"][i]),\n",
    "                    \"FTM\": int(alloc[\"FTM\"][i]), \"FTA\": int(alloc[\"FTA\"][i]),\n",
    "                    \"ORB\": p_orb, \"DRB\": p_drb, \"TRB\": p_orb + p_drb,\n",
    "                    \"AST\": int(alloc[\"AST\"][i]), \"STL\": int(alloc[\"STL\"][i]),\n",
    "                    \"BLK\": int(alloc[\"BLK\"][i]), \"TOV\": int(alloc[\"TOV\"][i]),\n",
    "                    \"PF\":  int(alloc[\"PF\"][i]),  \"PTS\": int(alloc[\"PTS\"][i]),\n",
    "                })\n",
    "\n",
    "team_game_totals = pd.DataFrame(rows_team_game)\n",
    "player_game_box = pd.DataFrame(rows_player_game)\n",
    "\n",
    "# --- 3) League season totals (for PER constants later) ---\n",
    "league_season_totals = (team_game_totals\n",
    "    .groupby(\"season\", as_index=False)\n",
    "    .agg(lg_FG=(\"FGM\",\"sum\"), lg_FGA=(\"FGA\",\"sum\"),\n",
    "         lg_3P=(\"3PM\",\"sum\"), lg_FT=(\"FTM\",\"sum\"), lg_FTA=(\"FTA\",\"sum\"),\n",
    "         lg_ORB=(\"ORB\",\"sum\"), lg_TRB=(\"TRB\",\"sum\"),\n",
    "         lg_TOV=(\"TOV\",\"sum\"), lg_AST=(\"AST\",\"sum\"), lg_PTS=(\"PTS\",\"sum\"))\n",
    ")\n",
    "\n",
    "# --- 4) Team-season possessions & pace (Oliver/BBR style, needs opponent stats) ---\n",
    "# merge opponent stats per game\n",
    "opp = team_game_totals.copy()\n",
    "opp_cols = [c for c in opp.columns if c not in {\"team_id\",\"is_home\"}]\n",
    "opp = opp.rename(columns={c: f\"opp_{c}\" for c in opp_cols if c not in {\"opp_team_id\",\"game_id\",\"game_date\",\"season\"}})\n",
    "merged = (team_game_totals\n",
    "          .merge(opp, left_on=[\"opp_team_id\",\"game_id\"],\n",
    "                       right_on=[\"team_id\",\"opp_game_id\"], suffixes=(\"\",\"_o\")))\n",
    "\n",
    "# Possessions estimate (BBR/Oliver)\n",
    "tm = merged\n",
    "tm_poss = 0.5 * (\n",
    "    (tm[\"FGA\"] + 0.4*tm[\"FTA\"]\n",
    "     - 1.07*(tm[\"ORB\"]/(tm[\"ORB\"] + tm[\"opp_DRB\"]))*(tm[\"FGA\"] - tm[\"FGM\"])\n",
    "     + tm[\"TOV\"])\n",
    "    +\n",
    "    (tm[\"opp_FGA\"] + 0.4*tm[\"opp_FTA\"]\n",
    "     - 1.07*(tm[\"opp_ORB\"]/(tm[\"opp_ORB\"] + tm[\"DRB\"]))*(tm[\"opp_FGA\"] - tm[\"opp_FGM\"])\n",
    "     + tm[\"opp_TOV\"])\n",
    ")\n",
    "\n",
    "team_game_totals[\"possessions\"] = tm_poss.values\n",
    "\n",
    "team_season_pace = (team_game_totals\n",
    "    .groupby([\"team_id\",\"season\"], as_index=False)\n",
    "    .agg(games=(\"game_id\",\"nunique\"),\n",
    "         team_possessions=(\"possessions\",\"sum\"))\n",
    ")\n",
    "team_season_pace[\"pace\"] = 48.0 * team_season_pace[\"team_possessions\"] / team_season_pace[\"games\"]\n",
    "\n",
    "# --- 5) Player-season totals (for EWA/VORP aggregation later) ---\n",
    "player_season_totals = (player_game_box\n",
    "    .merge(players[[\"player_id\",\"position\"]], on=\"player_id\", how=\"left\")\n",
    "    .groupby([\"player_id\",\"name\",\"position\",\"team_id\",\"season\"], as_index=False)\n",
    "    .agg(games=(\"game_id\",\"nunique\"),\n",
    "         minutes=(\"minutes\",\"sum\"),\n",
    "         FGM=(\"FGM\",\"sum\"), FGA=(\"FGA\",\"sum\"), PM3=(\"3PM\",\"sum\"), PA3=(\"3PA\",\"sum\"),\n",
    "         FTM=(\"FTM\",\"sum\"), FTA=(\"FTA\",\"sum\"),\n",
    "         ORB=(\"ORB\",\"sum\"), DRB=(\"DRB\",\"sum\"), TRB=(\"TRB\",\"sum\"),\n",
    "         AST=(\"AST\",\"sum\"), STL=(\"STL\",\"sum\"), BLK=(\"BLK\",\"sum\"),\n",
    "         TOV=(\"TOV\",\"sum\"), PF=(\"PF\",\"sum\"), PTS=(\"PTS\",\"sum\"))\n",
    ")\n",
    "\n",
    "# --- 6) Save everything alongside your original path ---\n",
    "Path(\"notebooks/5080_gpu/interview_prep/data\").mkdir(parents=True, exist_ok=True)\n",
    "base = Path(\"notebooks/5080_gpu/interview_prep/data\")\n",
    "\n",
    "player_game_box.to_csv(base/\"player_game_box.csv\", index=False)\n",
    "team_game_totals.to_csv(base/\"team_game_totals.csv\", index=False)\n",
    "league_season_totals.to_csv(base/\"league_season_totals.csv\", index=False)\n",
    "team_season_pace.to_csv(base/\"team_season_pace.csv\", index=False)\n",
    "player_season_totals.to_csv(base/\"player_season_totals.csv\", index=False)\n",
    "\n",
    "print(\"Wrote:\")\n",
    "for f in [\"player_game_box.csv\",\"team_game_totals.csv\",\"league_season_totals.csv\",\n",
    "          \"team_season_pace.csv\",\"player_season_totals.csv\"]:\n",
    "    print(\"-\", base/f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0739ca29",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c2c61d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing notebooks/5080_gpu/interview_prep/data/schema.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile notebooks/5080_gpu/interview_prep/data/schema.yaml\n",
    "# column roles for fake_basketball.csv\n",
    "y_variable: points\n",
    "numerical: [assists, rebounds, minutes]\n",
    "nominal: [team, name, player_id]\n",
    "ordinal: []          # add later if needed\n",
    "id_cols: [player_id] # optional: kept for joins/reporting, dropped from X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8430836e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile notebooks/5080_gpu/interview_prep/data/config.yaml\n",
    "\n",
    "fake_bball_basic_csv_path = Path(\"notebooks/5080_gpu/interview_prep/data/fake_basketball.csv\")\n",
    "yaml_path = Path(\"notebooks/5080_gpu/interview_prep/data/schema.yaml\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549a5178",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile notebooks/5080_gpu/interview_prep/data/schema.yaml\n",
    "\n",
    "y_variable: \n",
    "ordinal: []\n",
    "nominal: []\n",
    "numerical: []\n",
    "id_cols: []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467b32cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample data dtypes =======player_id      int64\n",
      "name          object\n",
      "points         int64\n",
      "assists      float64\n",
      "rebounds       int64\n",
      "minutes        int64\n",
      "team          object\n",
      "dtype: object and head    player_id      name  points  assists  rebounds  minutes     team\n",
      "0          1  Player_1      11      8.0         6       28  Celtics\n",
      "1          2  Player_2      14      2.0         3       22   Lakers\n",
      "2          3  Player_3       8      3.0         4       37  Celtics\n",
      "3          4  Player_4      13      5.0         6       16     Mavs\n",
      "4          5  Player_5      16      6.0         5       24     Mavs\n",
      "updated data dtypes =======player_id    string[python]\n",
      "name         string[python]\n",
      "points              float64\n",
      "assists             float64\n",
      "rebounds            float64\n",
      "minutes             float64\n",
      "team         string[python]\n",
      "dtype: object and head   player_id      name  points  assists  rebounds  minutes     team\n",
      "0         1  Player_1    11.0      8.0       6.0     28.0  Celtics\n",
      "1         2  Player_2    14.0      2.0       3.0     22.0   Lakers\n",
      "2         3  Player_3     8.0      3.0       4.0     37.0  Celtics\n",
      "3         4  Player_4    13.0      5.0       6.0     16.0     Mavs\n",
      "4         5  Player_5    16.0      6.0       5.0     24.0     Mavs\n",
      "data columns======Index(['player_id', 'name', 'points', 'assists', 'rebounds', 'minutes',\n",
      "       'team'],\n",
      "      dtype='object') data dtypes========player_id    string[python]\n",
      "name         string[python]\n",
      "points              float64\n",
      "assists             float64\n",
      "rebounds            float64\n",
      "minutes             float64\n",
      "team         string[python]\n",
      "dtype: object data head()========  player_id      name  points  assists  rebounds  minutes     team\n",
      "0         1  Player_1    11.0      8.0       6.0     28.0  Celtics\n",
      "1         2  Player_2    14.0      2.0       3.0     22.0   Lakers\n",
      "2         3  Player_3     8.0      3.0       4.0     37.0  Celtics\n",
      "3         4  Player_4    13.0      5.0       6.0     16.0     Mavs\n",
      "4         5  Player_5    16.0      6.0       5.0     24.0     Mavs\n",
      "           total data rows  null percentages\n",
      "player_id              200               0.0\n",
      "name                   200               0.0\n",
      "points                 200               0.0\n",
      "assists                200               3.0\n",
      "rebounds               200               0.0\n",
      "minutes                200               0.0\n",
      "team                   200               0.0\n",
      "        team        name player_id\n",
      "0    Celtics    Player_1         1\n",
      "1     Lakers    Player_2         2\n",
      "2    Celtics    Player_3         3\n",
      "3       Mavs    Player_4         4\n",
      "4       Mavs    Player_5         5\n",
      "..       ...         ...       ...\n",
      "195   Lakers  Player_196       196\n",
      "196  Celtics  Player_197       197\n",
      "197     Mavs  Player_198       198\n",
      "198     Heat  Player_199       199\n",
      "199  Celtics  Player_200       200\n",
      "\n",
      "[200 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "csv_path = Path(\"notebooks/5080_gpu/interview_prep/data/fake_basketball.csv\")\n",
    "yaml_path = Path(\"notebooks/5080_gpu/interview_prep/data/schema.yaml\")\n",
    "\n",
    "Questions:\n",
    "XX“Given a CSV dataset, how would you explore and summarize it?”\n",
    "\n",
    "“Given a DataFrame, how would you handle missing values?”\n",
    "\n",
    "“How would you detect and address outliers in a dataset?”\n",
    "\n",
    "“Perform univariate, bivariate, and multivariate analysis on given columns.”\n",
    "\n",
    "“Given a dataset, how would you normalize or standardize its features?”\n",
    "\n",
    "“Write a function to compute summary statistics (mean, median, std, etc.) of a column.”\n",
    "\n",
    "“Given a dataset, how would you identify the type of each variable and choose feature encoding?”\n",
    "\n",
    "“Given a dataset and a target variable, how would you check for relationships or correlations?”\n",
    "\n",
    "“Write code to detect missingness patterns and decide how to impute.”\n",
    "\n",
    "“Describe the full data-analysis pipeline: from loading to insight delivery—then code accordingly.”\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from omegaconf import OmegaConf\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def load_data(csv_path, sample_size=100, debug=False):\n",
    "    \n",
    "    # 1) pull in sample data for data type check and transform if needed\n",
    "    sample_data = pd.read_csv(csv_path, nrows=sample_size)\n",
    "    if debug:\n",
    "        print(f\"sample data dtypes ======={sample_data.dtypes} and head {sample_data.head()}\")\n",
    "\n",
    "    dtype_map = {}\n",
    "    for c in sample_data.select_dtypes(include=[\"object\"]):\n",
    "        dtype_map[c] = \"string\"\n",
    "    for c in sample_data.select_dtypes(include=[\"int64\"]):\n",
    "        dtype_map[c] = \"float64\"\n",
    "    if \"player_id\" in sample_data.columns:\n",
    "        dtype_map[\"player_id\"] = \"string\"\n",
    "\n",
    "    #2) update load in data with updated dtypes\n",
    "    data = pd.read_csv(csv_path, dtype= dtype_map)\n",
    "    if debug:\n",
    "        print(f\"updated data dtypes ======={data.dtypes} and head {data.head()}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "def eda(data, debug=False):\n",
    "    if debug:\n",
    "        print(f\"data columns======{data.columns} data dtypes========{data.dtypes} data head()========{data.head()}\")\n",
    "    \n",
    "    report = {}\n",
    "    \n",
    "    nulls = data.isnull().sum()\n",
    "    total_rows = len(data)\n",
    "    null_perc = (nulls / total_rows) *100\n",
    "    \n",
    "    report[\"total data rows\"] = total_rows\n",
    "    report[\"null percentages\"] = null_perc\n",
    "    \n",
    "    report_df = pd.DataFrame(report)\n",
    "    \n",
    "    return report_df\n",
    "\n",
    "\n",
    "def feature_engineering(data, debug=False):\n",
    "    \n",
    "    return\n",
    "    \n",
    "\n",
    "class ColumnSchema(BaseModel):\n",
    "    y_variable: str\n",
    "    nominal: List[str]\n",
    "    ordinal: List[str]\n",
    "    numerical: List[str]\n",
    "    id_cols: List[str]\n",
    "    \n",
    "    \n",
    "def load_schema(yaml_path, debug=False):\n",
    "    cfg = OmegaConf.load(str(yaml_path))\n",
    "    if debug:\n",
    "        print(f\"check on config if needed before dict form=========={cfg}\")\n",
    "\n",
    "    cfg_dict = OmegaConf.to_container(cfg, resolve=True)\n",
    "    if debug:\n",
    "        print(f\"check on config if needed after dict form=========={cfg_dict}\")\n",
    "\n",
    "    return ColumnSchema(**cfg_dict)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # XX“Given a CSV dataset, how would you explore and summarize it?”\n",
    "    csv_path = Path(\"notebooks/5080_gpu/interview_prep/data/fake_basketball.csv\")\n",
    "    yaml_path = Path(\"notebooks/5080_gpu/interview_prep/data/schema.yaml\")\n",
    "\n",
    "    data = load_data(csv_path,debug=True)\n",
    "\n",
    "    reports = eda(data, debug=True)\n",
    "    print(reports)\n",
    "    \n",
    "    schema = load_schema(yaml_path)\n",
    "    \n",
    "    y_var = data[schema.y_variable]\n",
    "    ord = data[schema.ordinal]\n",
    "    nom = data[schema.nominal]\n",
    "    num = data[schema.numerical]\n",
    "    id_cols = data[schema.id_cols]\n",
    "    \n",
    "    categoricals = pd.concat([nom, ord], axis=1)\n",
    "    print(categoricals)\n",
    "    \n",
    "    \n",
    "    # \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4186b3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best imputer: SimpleImputer(add_indicator=True, strategy='median')\n",
      "Best score (MAE): 3.510450550399642\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa: F401\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "\n",
    "# assume you already loaded: data, schema (with .numerical / .nominal / .ordinal)\n",
    "num_cols = schema.numerical\n",
    "cat_cols = schema.nominal + schema.ordinal\n",
    "\n",
    "# --- Define interchangeable numerical imputers ---\n",
    "num_imputer_options = {\n",
    "    \"median\": SimpleImputer(strategy=\"median\", add_indicator=True),  # fast, robust\n",
    "    \"knn\": Pipeline([\n",
    "        (\"scale_for_knn\", StandardScaler(with_mean=True, with_std=True)),\n",
    "        (\"impute\", KNNImputer(n_neighbors=5)),\n",
    "        (\"ind\", SimpleImputer(strategy=\"constant\", fill_value=0, add_indicator=True)),\n",
    "    ]),\n",
    "    \"iterative\": IterativeImputer(\n",
    "        estimator=RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "        random_state=42,\n",
    "        max_iter=10,\n",
    "        sample_posterior=False\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Pick a default; GridSearch will override\n",
    "num_imputer = num_imputer_options[\"median\"]\n",
    "\n",
    "numeric_pipe = Pipeline([\n",
    "    (\"imputer\", num_imputer),\n",
    "    (\"scaler\", StandardScaler(with_mean=True, with_std=True))\n",
    "])\n",
    "\n",
    "categorical_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n",
    "])\n",
    "\n",
    "preproc = ColumnTransformer([\n",
    "    (\"num\", numeric_pipe, num_cols),\n",
    "    (\"cat\", categorical_pipe, cat_cols)\n",
    "])\n",
    "\n",
    "# downstream estimator — replace with your task model\n",
    "model = Ridge(random_state=42)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"preproc\", preproc),\n",
    "    (\"model\", model),\n",
    "])\n",
    "\n",
    "# --- Search over imputation strategies cleanly (no leakage) ---\n",
    "param_grid = [\n",
    "    {\n",
    "        \"preproc__num__imputer\": [num_imputer_options[\"median\"]],\n",
    "        \"model__alpha\": [0.1, 1.0, 10.0],\n",
    "    },\n",
    "    {\n",
    "        \"preproc__num__imputer\": [num_imputer_options[\"knn\"]],\n",
    "        \"preproc__num__imputer__impute__n_neighbors\": [3, 5, 7],\n",
    "        \"model__alpha\": [0.1, 1.0, 10.0],\n",
    "    },\n",
    "    {\n",
    "        \"preproc__num__imputer\": [num_imputer_options[\"iterative\"]],\n",
    "        \"preproc__num__imputer__max_iter\": [5, 10],\n",
    "        \"model__alpha\": [0.1, 1.0, 10.0],\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "gs = GridSearchCV(pipe, param_grid=param_grid, cv=cv, scoring=\"neg_mean_absolute_error\", n_jobs=-1)\n",
    "gs.fit(data[num_cols + cat_cols], data[schema.y_variable])\n",
    "\n",
    "best_pipe = gs.best_estimator_\n",
    "print(\"Best imputer:\", gs.best_params_.get(\"preproc__num__imputer\"))\n",
    "print(\"Best score (MAE):\", -gs.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff7aa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "“How would you detect and address outliers in a dataset?”\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1aeb70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "“Perform univariate, bivariate, and multivariate analysis on given columns.”\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fbc525",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "“Given a dataset, how would you normalize or standardize its features?”\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7b1def",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "“Write a function to compute summary statistics (mean, median, std, etc.) of a column.”\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0a9c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "“Given a dataset, how would you identify the type of each variable and choose feature encoding?”\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358e96a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "“Given a dataset and a target variable, how would you check for relationships or correlations?”\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46db109a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "“Write code to detect missingness patterns and decide how to impute.”\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be245bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "“Describe the full data-analysis pipeline: from loading to insight delivery—then code accordingly.”\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba09904",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "here you go — 10 fresh, live-coding-friendly prompts you can answer with your current fake\\_basketball dataset (`player_id, name, team, points, assists, rebounds, minutes`). they’re simple, pythonic, and reusable.\n",
    "\n",
    "## 10 new questions (not in your list)\n",
    "\n",
    "1. Who are the top 3 scorers **per team** by **points per-36** (filter to minutes ≥ 15)?\n",
    "\n",
    "2. Build a simple **composite impact score** per player using standardized per-36 stats:\n",
    "   `impact = z(points/36) + 0.7*z(assists/36) + 0.7*z(rebounds/36)`. Who are the top 10?\n",
    "\n",
    "3. Do players **outperform their expected points** for their minutes/assists/rebounds?\n",
    "   Fit a quick linear model `points ~ minutes + assists + rebounds` and list top 10 **positive residuals**.\n",
    "\n",
    "4. Which team is **most balanced vs. star-heavy** by scoring?\n",
    "   Compute **coefficient of variation** (std/mean) of `points` per team and rank.\n",
    "\n",
    "5. Bucket players into **minutes tiers**: `[10–19, 20–29, 30–40]`.\n",
    "   What are the mean/median of `points/assists/rebounds` per tier?\n",
    "\n",
    "6. “Three-above-median” players: per team, who is **above the team median** in **points, assists, and rebounds** simultaneously?\n",
    "\n",
    "7. Write a reusable helper `top_k(df, by, k, group=None)` and use it to return the **top 2 rebounders per team**.\n",
    "\n",
    "8. What’s the **team effect** on scoring after controlling for other stats?\n",
    "   One-hot encode `team`, fit a Ridge `points ~ minutes + assists + rebounds + team_*`, and show team coefficients.\n",
    "\n",
    "9. Give a quick **bootstrap 95% CI** for **mean points per team** (1,000 resamples). Which teams have clearly higher means?\n",
    "\n",
    "10. Detect potential **duplicate identity issues**: do we have any duplicate `(player_id, name)` rows? If so, keep the one with the **max minutes**.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ee709b",
   "metadata": {},
   "source": [
    "here you go — 10 fresh, live-coding-friendly prompts you can answer with your current fake\\_basketball dataset (`player_id, name, team, points, assists, rebounds, minutes`). they’re simple, pythonic, and reusable.\n",
    "\n",
    "## 10 new questions (not in your list)\n",
    "\n",
    "1. Who are the top 3 scorers **per team** by **points per-36** (filter to minutes ≥ 15)?\n",
    "\n",
    "2. Build a simple **composite impact score** per player using standardized per-36 stats:\n",
    "   `impact = z(points/36) + 0.7*z(assists/36) + 0.7*z(rebounds/36)`. Who are the top 10?\n",
    "\n",
    "3. Do players **outperform their expected points** for their minutes/assists/rebounds?\n",
    "   Fit a quick linear model `points ~ minutes + assists + rebounds` and list top 10 **positive residuals**.\n",
    "\n",
    "4. Which team is **most balanced vs. star-heavy** by scoring?\n",
    "   Compute **coefficient of variation** (std/mean) of `points` per team and rank.\n",
    "\n",
    "5. Bucket players into **minutes tiers**: `[10–19, 20–29, 30–40]`.\n",
    "   What are the mean/median of `points/assists/rebounds` per tier?\n",
    "\n",
    "6. “Three-above-median” players: per team, who is **above the team median** in **points, assists, and rebounds** simultaneously?\n",
    "\n",
    "7. Write a reusable helper `top_k(df, by, k, group=None)` and use it to return the **top 2 rebounders per team**.\n",
    "\n",
    "8. What’s the **team effect** on scoring after controlling for other stats?\n",
    "   One-hot encode `team`, fit a Ridge `points ~ minutes + assists + rebounds + team_*`, and show team coefficients.\n",
    "\n",
    "9. Give a quick **bootstrap 95% CI** for **mean points per team** (1,000 resamples). Which teams have clearly higher means?\n",
    "\n",
    "10. Detect potential **duplicate identity issues**: do we have any duplicate `(player_id, name)` rows? If so, keep the one with the **max minutes**.\n",
    "\n",
    "---\n",
    "\n",
    "## Answers & quick steps (copy-ready)\n",
    "\n",
    "> Assumes `import pandas as pd, numpy as np` and `df = pd.read_csv(csv_path)` are already done.\n",
    "\n",
    "### 1) Top 3 points per-36 by team (minutes ≥ 15)\n",
    "\n",
    "**Steps:** compute per-36, filter, groupby-rank, keep top 3.\n",
    "\n",
    "```python\n",
    "df1 = df.copy()\n",
    "df1[\"pts_per36\"] = df1[\"points\"] * 36 / df1[\"minutes\"]\n",
    "ans1 = (df1.loc[df1[\"minutes\"] >= 15]\n",
    "          .assign(rk=lambda x: x.groupby(\"team\")[\"pts_per36\"].rank(ascending=False, method=\"first\"))\n",
    "          .query(\"rk <= 3\")\n",
    "          .sort_values([\"team\",\"rk\"])[[\"team\",\"name\",\"pts_per36\",\"rk\"]])\n",
    "ans1.head(12)\n",
    "```\n",
    "\n",
    "### 2) Composite impact (standardized per-36) — top 10\n",
    "\n",
    "**Steps:** build per-36, z-score, combine, sort.\n",
    "\n",
    "```python\n",
    "df2 = df.copy()\n",
    "for c, newc in [(\"points\",\"pts36\"),(\"assists\",\"ast36\"),(\"rebounds\",\"reb36\")]:\n",
    "    df2[newc] = df2[c] * 36 / df2[\"minutes\"]\n",
    "for c in [\"pts36\",\"ast36\",\"reb36\"]:\n",
    "    mu, sd = df2[c].mean(), df2[c].std(ddof=0)\n",
    "    df2[c+\"_z\"] = (df2[c] - mu) / sd\n",
    "df2[\"impact\"] = df2[\"pts36_z\"] + 0.7*df2[\"ast36_z\"] + 0.7*df2[\"reb36_z\"]\n",
    "ans2 = df2.sort_values(\"impact\", ascending=False)[[\"name\",\"team\",\"impact\",\"pts36\",\"ast36\",\"reb36\"]].head(10)\n",
    "ans2\n",
    "```\n",
    "\n",
    "### 3) “Over-expected” scorers via residuals\n",
    "\n",
    "**Steps:** fit quick linear model, compute residuals, rank.\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "X = df[[\"minutes\",\"assists\",\"rebounds\"]]\n",
    "y = df[\"points\"]\n",
    "m = LinearRegression().fit(X, y)\n",
    "df3 = df.assign(pred=m.predict(X), resid=lambda x: x[\"points\"] - x[\"pred\"])\n",
    "ans3 = df3.sort_values(\"resid\", ascending=False)[[\"name\",\"team\",\"points\",\"pred\",\"resid\"]].head(10)\n",
    "ans3\n",
    "```\n",
    "\n",
    "### 4) Team scoring parity (coefficient of variation)\n",
    "\n",
    "**Steps:** groupby team; cv = std/mean; sort.\n",
    "\n",
    "```python\n",
    "g = df.groupby(\"team\")[\"points\"]\n",
    "ans4 = (g.agg(mean=\"mean\", std=\"std\")\n",
    "          .assign(cv=lambda x: x[\"std\"]/x[\"mean\"])\n",
    "          .sort_values(\"cv\", ascending=True))  # lower = more balanced\n",
    "ans4\n",
    "```\n",
    "\n",
    "### 5) Minutes tiers summary table\n",
    "\n",
    "**Steps:** cut into bins, aggregate.\n",
    "\n",
    "```python\n",
    "bins = [10, 20, 30, 40]\n",
    "labels = [\"10–19\",\"20–29\",\"30–40\"]\n",
    "df5 = df.assign(min_tier=pd.cut(df[\"minutes\"], bins=bins, labels=labels, include_lowest=True, right=False))\n",
    "ans5 = (df5.groupby(\"min_tier\")[[\"points\",\"assists\",\"rebounds\"]]\n",
    "          .agg([\"mean\",\"median\",\"count\"]).round(2))\n",
    "ans5\n",
    "```\n",
    "\n",
    "### 6) Three-above-median players (per team)\n",
    "\n",
    "**Steps:** compute team medians, compare per row, filter.\n",
    "\n",
    "```python\n",
    "med = df.groupby(\"team\")[[\"points\",\"assists\",\"rebounds\"]].median().rename(columns=lambda c: c+\"_med\")\n",
    "df6 = df.join(med, on=\"team\")\n",
    "mask = (df6[\"points\"]>df6[\"points_med\"]) & (df6[\"assists\"]>df6[\"assists_med\"]) & (df6[\"rebounds\"]>df6[\"rebounds_med\"])\n",
    "ans6 = df6.loc[mask, [\"team\",\"name\",\"points\",\"assists\",\"rebounds\"]].sort_values([\"team\",\"points\"], ascending=[True,False])\n",
    "ans6\n",
    "```\n",
    "\n",
    "### 7) Reusable `top_k` + example (top 2 rebounders per team)\n",
    "\n",
    "**Steps:** write helper; call with group.\n",
    "\n",
    "```python\n",
    "def top_k(d, by, k=3, group=None, ascending=False):\n",
    "    x = d.copy()\n",
    "    if group:\n",
    "        x = x.assign(_rk=x.groupby(group)[by].rank(ascending=ascending, method=\"first\"))\n",
    "        return x.query(f\"_rk <= {k}\").sort_values(group + [by], ascending=[True, not ascending])\n",
    "    else:\n",
    "        return x.sort_values(by, ascending=not ascending).head(k)\n",
    "\n",
    "ans7 = top_k(df.assign(reb_per36=df[\"rebounds\"]*36/df[\"minutes\"]),\n",
    "             by=\"reb_per36\", k=2, group=[\"team\"], ascending=False)[[\"team\",\"name\",\"reb_per36\"]]\n",
    "ans7\n",
    "```\n",
    "\n",
    "### 8) Team effect on points (Ridge with one-hot team)\n",
    "\n",
    "**Steps:** one-hot team, fit Ridge, show team coefs.\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "X = df[[\"minutes\",\"assists\",\"rebounds\",\"team\"]]\n",
    "y = df[\"points\"]\n",
    "\n",
    "pre = ColumnTransformer([\n",
    "    (\"cat\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\"), [\"team\"])\n",
    "], remainder=\"passthrough\")\n",
    "\n",
    "ridge = Pipeline([(\"pre\", pre), (\"model\", Ridge(alpha=1.0, random_state=42))]).fit(X, y)\n",
    "feat_names = list(ridge.named_steps[\"pre\"].get_feature_names_out()) + [\"minutes\",\"assists\",\"rebounds\"]\n",
    "coefs = pd.Series(ridge.named_steps[\"model\"].coef_, index=feat_names).sort_values(ascending=False)\n",
    "ans8 = coefs\n",
    "ans8.head(12)\n",
    "```\n",
    "\n",
    "### 9) Bootstrap 95% CI for mean points per team\n",
    "\n",
    "**Steps:** resample rows within team, compute mean for each resample, take percentiles.\n",
    "\n",
    "```python\n",
    "rng = np.random.default_rng(42)\n",
    "def ci95(a, B=1000):\n",
    "    boot = [rng.choice(a, size=a.size, replace=True).mean() for _ in range(B)]\n",
    "    return np.percentile(boot, [2.5, 97.5])\n",
    "\n",
    "ans9 = (df.groupby(\"team\")[\"points\"]\n",
    "          .apply(lambda s: pd.Series(ci95(s.values), index=[\"lo\",\"hi\"]))\n",
    "          .join(df.groupby(\"team\")[\"points\"].mean().rename(\"mean\"))\n",
    "          .reindex(columns=[\"mean\",\"lo\",\"hi\"]).round(2))\n",
    "ans9\n",
    "```\n",
    "\n",
    "### 10) Duplicate identity check & resolution\n",
    "\n",
    "**Steps:** find duplicates of `(player_id, name)`; keep max minutes.\n",
    "\n",
    "```python\n",
    "dupes = df.duplicated(subset=[\"player_id\",\"name\"], keep=False)\n",
    "ans10_dupe_rows = df.loc[dupes].sort_values([\"player_id\",\"minutes\"], ascending=[True,False])\n",
    "resolved = (df.sort_values(\"minutes\", ascending=False)\n",
    "              .drop_duplicates(subset=[\"player_id\",\"name\"], keep=\"first\"))\n",
    "ans10 = {\"duplicate_rows\": ans10_dupe_rows, \"resolved_table\": resolved.head(10)}\n",
    "ans10\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "want me to drop these into your notebook cells as a neat section (Questions, then Answers) or wire them behind small functions so you can call `answer_1(df)`, `answer_2(df)`, etc.?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
