{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d72bd9ba",
   "metadata": {},
   "source": [
    "# Additional Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7de83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile api/src/data_engineering.py\n",
    "# api/src/data_engineering.py\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def load_my_dataset(path: str) -> pd.DataFrame:\n",
    "    # 1) read raw CSV/DB\n",
    "    df = pd.read_csv(path)\n",
    "    # 2) clean / filter / featureâ€‘engineer\n",
    "    df = df.dropna(subset=[\"important_feature\"])\n",
    "    df[\"new_feature\"] = df[\"colA\"] / df[\"colB\"]\n",
    "    return df\n",
    "\n",
    "def split_features_target(df: pd.DataFrame, target_col: str, test_size=0.2, random_state=0):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    return train_test_split(X, y, test_size=test_size, random_state=random_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75e9930",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile api/src/trainers/my_trainer.py\n",
    "# api/src/trainers/my_trainer.py\n",
    "\n",
    "from .base import BaseTrainer, TrainResult\n",
    "import mlflow\n",
    "from ..data_engineering import load_my_dataset, split_features_target\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class MyDatasetTrainer(BaseTrainer):\n",
    "    name = \"my_dataset_model\"\n",
    "    model_type = \"classification\"\n",
    "\n",
    "    def default_hyperparams(self):\n",
    "        return {\n",
    "            \"n_estimators\": 100,\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"random_state\": 42,\n",
    "        }\n",
    "\n",
    "    def train(self, data_path: str = \"data/my.csv\", **overrides) -> TrainResult:\n",
    "        # Merge overrides\n",
    "        hp = self.merge_hyperparams(overrides)\n",
    "        # Load & split\n",
    "        df = load_my_dataset(data_path)\n",
    "        X_tr, X_te, y_tr, y_te = split_features_target(df, target_col=\"label\", test_size=0.2, random_state=hp[\"random_state\"])\n",
    "        # Fit model\n",
    "        model = GradientBoostingClassifier(\n",
    "            n_estimators=hp[\"n_estimators\"],\n",
    "            learning_rate=hp[\"learning_rate\"],\n",
    "            random_state=hp[\"random_state\"],\n",
    "        ).fit(X_tr, y_tr)\n",
    "        # Evaluate\n",
    "        preds = model.predict(X_te)\n",
    "        metrics = {\"accuracy\": accuracy_score(y_te, preds)}\n",
    "        # Log to MLflow\n",
    "        with mlflow.start_run(run_name=self.name) as run:\n",
    "            mlflow.log_params(hp)\n",
    "            mlflow.log_metrics(metrics)\n",
    "            # log model artifact (sklearn flavor)\n",
    "            mlflow.sklearn.log_model(model, artifact_path=\"model\", registered_model_name=self.name)\n",
    "        return TrainResult(run_id=run.info.run_id, metrics=metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b87746",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile api/src/registry/registry.py\n",
    "from src.registry.registry import register\n",
    "from src.trainers.my_trainer import MyDatasetTrainer\n",
    "from src.registry.types import TrainerSpec\n",
    "\n",
    "spec = TrainerSpec(\n",
    "  name=MyDatasetTrainer.name,\n",
    "  cls=MyDatasetTrainer,\n",
    "  default_params=MyDatasetTrainer().default_hyperparams()\n",
    ")\n",
    "register(spec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c13d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile api/app/schemas/my_dataset.py\n",
    "# api/app/schemas/my_dataset.py\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "class MyDataFeatures(BaseModel):\n",
    "    feature1: float = Field(..., description=\"â€¦\")\n",
    "    feature2: float = Field(..., description=\"â€¦\")\n",
    "    # â€¦etc\n",
    "\n",
    "class MyDataTrainRequest(BaseModel):\n",
    "    data_path: str = Field(\"data/my.csv\", description=\"Path to CSV\")\n",
    "    hyperparams: dict[str, float] | None = None\n",
    "    async_training: bool = False\n",
    "\n",
    "class MyDataPredictRequest(BaseModel):\n",
    "    samples: List[MyDataFeatures]\n",
    "\n",
    "class MyDataPredictResponse(BaseModel):\n",
    "    predictions: List[str]\n",
    "    probabilities: List[float]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4e0ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile api/app/new_main.py\n",
    "from .schemas.my_dataset import (\n",
    "  MyDataTrainRequest, MyDataPredictRequest, MyDataPredictResponse\n",
    ")\n",
    "from .services.ml.model_service import model_service\n",
    "\n",
    "@app.post(\"/api/v1/my/train\", status_code=202)\n",
    "async def train_mydata(req: MyDataTrainRequest, bt: BackgroundTasks):\n",
    "    if req.async_training:\n",
    "        bt.add_task(model_service.train_via_registry, \"my_dataset_model\", req.hyperparams)\n",
    "        return {\"status\": \"queued\"}\n",
    "    else:\n",
    "        run_id = await model_service.train_via_registry(\"my_dataset_model\", req.hyperparams)\n",
    "        return {\"status\": \"completed\", \"run_id\": run_id}\n",
    "\n",
    "@app.post(\"/api/v1/my/predict\", response_model=MyDataPredictResponse)\n",
    "async def predict_mydata(req: MyDataPredictRequest):\n",
    "    # assuming model_service.predict_generic exists or add a custom wrapper\n",
    "    preds, probs = await model_service.predict_generic(\"my_dataset_model\", [f.dict() for f in req.samples])\n",
    "    return MyDataPredictResponse(predictions=preds, probabilities=probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc58720",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile api/src/trainers/__init__.py\n",
    "# api/src/trainers/__init__.py\n",
    "from .base import BaseTrainer, TrainResult\n",
    "\n",
    "__all__ = [\"BaseTrainer\", \"TrainResult\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8c89e8",
   "metadata": {},
   "source": [
    "# original working Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e486b60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile api/src/trainers/base.py\n",
    "# api/src/trainers/base.py\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, Optional, Protocol\n",
    "import mlflow\n",
    "\n",
    "@dataclass\n",
    "class TrainResult:\n",
    "    run_id: str\n",
    "    metrics: Dict[str, float]\n",
    "    artifacts: Dict[str, str] = field(default_factory=dict)\n",
    "\n",
    "class SupportsPyFunc(Protocol):\n",
    "    # Minimal protocol if custom loader is needed later\n",
    "    def predict(self, X): ...\n",
    "\n",
    "class BaseTrainer:\n",
    "    \"\"\"\n",
    "    Minimal trainer abstraction:\n",
    "      * implement `train(**hyperparams)` returning TrainResult\n",
    "      * optionally override default_hyperparams()\n",
    "    \"\"\"\n",
    "    name: str\n",
    "    model_type: str = \"generic\"\n",
    "\n",
    "    def default_hyperparams(self) -> Dict[str, Any]:\n",
    "        return {}\n",
    "\n",
    "    def merge_hyperparams(self, overrides: Dict[str, Any] | None) -> Dict[str, Any]:\n",
    "        params = self.default_hyperparams().copy()\n",
    "        if overrides:\n",
    "            params.update({k: v for k, v in overrides.items() if v is not None})\n",
    "        return params\n",
    "\n",
    "    def train(self, **hyperparams) -> TrainResult:  # pragma: no cover - interface\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # Optional hook â€“ if a trainer needs a special load path\n",
    "    def load_pyfunc(self, run_uri: str):\n",
    "        return mlflow.pyfunc.load_model(run_uri) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887b823b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile api/src/trainers/iris_rf_trainer.py\n",
    "# api/src/trainers/iris_rf_trainer.py\n",
    "from __future__ import annotations\n",
    "from .base import BaseTrainer, TrainResult\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import mlflow\n",
    "\n",
    "class IrisRandomForestTrainer(BaseTrainer):\n",
    "    name = \"iris_random_forest\"\n",
    "    model_type = \"classification\"\n",
    "\n",
    "    def default_hyperparams(self):\n",
    "        return {\n",
    "            \"n_estimators\": 300,\n",
    "            \"max_depth\": None,\n",
    "            \"random_state\": 42,\n",
    "        }\n",
    "\n",
    "    def train(self, **overrides) -> TrainResult:\n",
    "        hp = self.merge_hyperparams(overrides)\n",
    "        iris = load_iris(as_frame=True)\n",
    "        X, y = iris.data, iris.target\n",
    "        Xtr, Xte, ytr, yte = train_test_split(\n",
    "            X, y, test_size=0.25, stratify=y, random_state=hp[\"random_state\"]\n",
    "        )\n",
    "        rf = RandomForestClassifier(\n",
    "            n_estimators=hp[\"n_estimators\"],\n",
    "            max_depth=hp[\"max_depth\"],\n",
    "            random_state=hp[\"random_state\"],\n",
    "            n_jobs=-1,\n",
    "            class_weight=\"balanced\",\n",
    "        ).fit(Xtr, ytr)\n",
    "\n",
    "        preds = rf.predict(Xte)\n",
    "        metrics = {\n",
    "            \"accuracy\": accuracy_score(yte, preds),\n",
    "            \"f1_macro\": f1_score(yte, preds, average=\"macro\"),\n",
    "            \"precision_macro\": precision_score(yte, preds, average=\"macro\"),\n",
    "            \"recall_macro\": recall_score(yte, preds, average=\"macro\"),\n",
    "        }\n",
    "\n",
    "        class _Wrapper(mlflow.pyfunc.PythonModel):\n",
    "            def __init__(self, model, cols):\n",
    "                self.model = model\n",
    "                self.cols = cols\n",
    "            def predict(self, context, model_input, params=None):\n",
    "                import pandas as pd, numpy as np\n",
    "                df = model_input if isinstance(model_input, pd.DataFrame) else pd.DataFrame(model_input, columns=self.cols)\n",
    "                return self.model.predict_proba(df)\n",
    "\n",
    "        with mlflow.start_run(run_name=self.name) as run:\n",
    "            mlflow.log_params({k: v for k, v in hp.items()})\n",
    "            mlflow.log_metrics(metrics)\n",
    "            sig = mlflow.models.signature.infer_signature(X, rf.predict_proba(X))\n",
    "            mlflow.pyfunc.log_model(\n",
    "                artifact_path=\"model\",\n",
    "                python_model=_Wrapper(rf, list(X.columns)),\n",
    "                registered_model_name=self.name,\n",
    "                input_example=X.head(),\n",
    "                signature=sig,\n",
    "            )\n",
    "            return TrainResult(run_id=run.info.run_id, metrics=metrics) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b8c86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile api/src/registry/__init__.py\n",
    "# api/src/registry/__init__.py\n",
    "from .registry import register, all_names, get, load_from_entry_point\n",
    "from .types import TrainerSpec\n",
    "\n",
    "__all__ = [\"register\", \"all_names\", \"get\", \"load_from_entry_point\", \"TrainerSpec\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a2e895",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile api/src/registry/types.py\n",
    "# api/src/registry/types.py\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "from typing import Type, Dict, Any\n",
    "from ..trainers.base import BaseTrainer\n",
    "\n",
    "@dataclass\n",
    "class TrainerSpec:\n",
    "    name: str\n",
    "    cls: Type[BaseTrainer]\n",
    "    default_params: Dict[str, Any] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe06c7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile api/src/registry/registry.py\n",
    "# api/src/registry/registry.py\n",
    "from __future__ import annotations\n",
    "from importlib import import_module\n",
    "from typing import Dict, Iterable, Type\n",
    "from .types import TrainerSpec\n",
    "from ..trainers.base import BaseTrainer\n",
    "\n",
    "_REGISTRY: Dict[str, TrainerSpec] = {}\n",
    "\n",
    "def register(spec: TrainerSpec) -> None:\n",
    "    _REGISTRY[spec.name] = spec\n",
    "\n",
    "def all_names() -> Iterable[str]:\n",
    "    return _REGISTRY.keys()\n",
    "\n",
    "def get(name: str) -> TrainerSpec:\n",
    "    return _REGISTRY[name]\n",
    "\n",
    "def load_from_entry_point(dotted: str, name: str | None = None):\n",
    "    \"\"\"\n",
    "    Load 'pkg.module:ClassName' into registry.\n",
    "    \"\"\"\n",
    "    mod_path, cls_name = dotted.split(\":\")\n",
    "    mod = import_module(mod_path)\n",
    "    cls: Type[BaseTrainer] = getattr(mod, cls_name)\n",
    "    inst_name = name or getattr(cls, \"name\", cls_name.lower())\n",
    "    spec = TrainerSpec(name=inst_name, cls=cls, default_params=cls().default_hyperparams())\n",
    "    register(spec)\n",
    "    return spec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9cd57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile api/app/schemas/train.py\n",
    "from typing import Optional, Dict, Any\n",
    "from pydantic import BaseModel, Field\n",
    "from .bayes import BayesCancerParams\n",
    "\n",
    "class IrisTrainRequest(BaseModel):\n",
    "    \"\"\"\n",
    "    Kick off Iris model training.\n",
    "\n",
    "    â€¢ `model_type` â€“ 'rf' (Randomâ€‘Forest) | 'logreg'  \n",
    "    â€¢ `hyperparams` â€“ optional scikitâ€‘learn overrides, e.g. {\"n_estimators\": 500}  \n",
    "    â€¢ `async_training` â€“ true â‡’ returns job_id immediately\n",
    "    \"\"\"\n",
    "    model_type: str = Field(\n",
    "        default=\"rf\",\n",
    "        description=\"Which Iris trainer to run: 'rf' or 'logreg'\"\n",
    "    )\n",
    "    hyperparams: Optional[Dict[str, Any]] = Field(\n",
    "        default=None,\n",
    "        description=\"Optional hyperâ€‘parameter overrides\"\n",
    "    )\n",
    "    async_training: bool = Field(\n",
    "        default=False,\n",
    "        description=\"Run in background and return job ID\"\n",
    "    )\n",
    "\n",
    "class CancerTrainRequest(BaseModel):\n",
    "    \"\"\"\n",
    "    Train Breastâ€‘Cancer classifiers.\n",
    "\n",
    "    â€¢ `model_type` â€“ 'bayes' (hierâ€‘Bayes) | 'stub' (quick LogisticRegression)  \n",
    "    â€¢ `params` â€“ validated Bayesian hyperâ€‘parameters (only used when model_type='bayes')  \n",
    "    â€¢ `async_training` â€“ background flag\n",
    "    \"\"\"\n",
    "    model_type: str = Field(\n",
    "        default=\"bayes\",\n",
    "        description=\"Which cancer model to train: 'bayes' or 'stub'\"\n",
    "    )\n",
    "    params: Optional[BayesCancerParams] = Field(\n",
    "        default=None,\n",
    "        description=\"Bayesian hyperâ€‘parameters; ignored for stub model\"\n",
    "    )\n",
    "    async_training: bool = Field(\n",
    "        default=False,\n",
    "        description=\"Run in background and return job ID\"\n",
    "    )\n",
    "\n",
    "class BayesTrainRequest(BaseModel):\n",
    "    \"\"\"Request model for Bayesian cancer model training\"\"\"\n",
    "    params: Optional[BayesCancerParams] = Field(\n",
    "        default=None, \n",
    "        description=\"Bayesian hyperparameters. If None, uses defaults.\"\n",
    "    )\n",
    "    async_training: bool = Field(\n",
    "        default=False,\n",
    "        description=\"If True, returns job_id immediately. If False, waits for completion.\"\n",
    "    )\n",
    "\n",
    "class BayesTrainResponse(BaseModel):\n",
    "    \"\"\"Response model for Bayesian training\"\"\"\n",
    "    run_id: str = Field(description=\"MLflow run ID\")\n",
    "    job_id: Optional[str] = Field(default=None, description=\"Background job ID if async\")\n",
    "    status: str = Field(description=\"Training status: 'completed', 'queued', 'failed'\")\n",
    "    message: Optional[str] = Field(default=None, description=\"Status message or error\")\n",
    "\n",
    "class BayesConfigResponse(BaseModel):\n",
    "    \"\"\"Response model for Bayesian configuration endpoint\"\"\"\n",
    "    defaults: BayesCancerParams = Field(description=\"Default hyperparameters\")\n",
    "    bounds: dict = Field(description=\"Parameter bounds for UI controls\")\n",
    "    descriptions: dict = Field(description=\"Parameter descriptions for tooltips\")\n",
    "    runtime_estimate: dict = Field(description=\"Runtime estimation factors\")\n",
    "\n",
    "class BayesRunMetrics(BaseModel):\n",
    "    \"\"\"Response model for Bayesian run metrics\"\"\"\n",
    "    run_id: str\n",
    "    accuracy: float\n",
    "    rhat_max: Optional[float] = None\n",
    "    ess_bulk_min: Optional[float] = None\n",
    "    ess_tail_min: Optional[float] = None\n",
    "    waic: Optional[float] = None\n",
    "    loo: Optional[float] = None\n",
    "    status: str\n",
    "    warnings: list[str] = Field(default_factory=list) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcffb086",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile api/app/schemas/cancer.py\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "\n",
    "class CancerFeatures(BaseModel):\n",
    "    \"\"\"Breast cancer diagnostic features.\"\"\"\n",
    "    mean_radius: float = Field(..., description=\"Mean of distances from center to points on perimeter\")\n",
    "    mean_texture: float = Field(..., description=\"Standard deviation of gray-scale values\")\n",
    "    mean_perimeter: float = Field(..., description=\"Mean size of the core tumor\")\n",
    "    mean_area: float = Field(..., description=\"Mean area of the core tumor\")\n",
    "    mean_smoothness: float = Field(..., description=\"Mean of local variation in radius lengths\")\n",
    "    mean_compactness: float = Field(..., description=\"Mean of perimeter^2 / area - 1.0\")\n",
    "    mean_concavity: float = Field(..., description=\"Mean of severity of concave portions of the contour\")\n",
    "    mean_concave_points: float = Field(..., description=\"Mean for number of concave portions of the contour\")\n",
    "    mean_symmetry: float = Field(..., description=\"Mean symmetry\")\n",
    "    mean_fractal_dimension: float = Field(..., description=\"Mean for 'coastline approximation' - 1\")\n",
    "    \n",
    "    # SE features (standard error)\n",
    "    se_radius: float = Field(..., description=\"Standard error of radius\")\n",
    "    se_texture: float = Field(..., description=\"Standard error of texture\")\n",
    "    se_perimeter: float = Field(..., description=\"Standard error of perimeter\")\n",
    "    se_area: float = Field(..., description=\"Standard error of area\")\n",
    "    se_smoothness: float = Field(..., description=\"Standard error of smoothness\")\n",
    "    se_compactness: float = Field(..., description=\"Standard error of compactness\")\n",
    "    se_concavity: float = Field(..., description=\"Standard error of concavity\")\n",
    "    se_concave_points: float = Field(..., description=\"Standard error of concave points\")\n",
    "    se_symmetry: float = Field(..., description=\"Standard error of symmetry\")\n",
    "    se_fractal_dimension: float = Field(..., description=\"Standard error of fractal dimension\")\n",
    "    \n",
    "    # Worst features\n",
    "    worst_radius: float = Field(..., description=\"Worst radius\")\n",
    "    worst_texture: float = Field(..., description=\"Worst texture\")\n",
    "    worst_perimeter: float = Field(..., description=\"Worst perimeter\")\n",
    "    worst_area: float = Field(..., description=\"Worst area\")\n",
    "    worst_smoothness: float = Field(..., description=\"Worst smoothness\")\n",
    "    worst_compactness: float = Field(..., description=\"Worst compactness\")\n",
    "    worst_concavity: float = Field(..., description=\"Worst concavity\")\n",
    "    worst_concave_points: float = Field(..., description=\"Worst concave points\")\n",
    "    worst_symmetry: float = Field(..., description=\"Worst symmetry\")\n",
    "    worst_fractal_dimension: float = Field(..., description=\"Worst fractal dimension\")\n",
    "\n",
    "class CancerPredictRequest(BaseModel):\n",
    "    \"\"\"Cancer prediction request (allows 'rows' alias).\"\"\"\n",
    "    model_type: str = Field(\"bayes\", description=\"Model type: 'bayes', 'logreg', or 'rf'\")\n",
    "    samples: List[CancerFeatures] = Field(\n",
    "        ...,\n",
    "        description=\"Breast-cancer feature vectors\",\n",
    "        alias=\"rows\",\n",
    "    )\n",
    "    posterior_samples: Optional[int] = Field(\n",
    "        None, ge=10, le=10_000, description=\"Posterior draws for uncertainty\"\n",
    "    )\n",
    "\n",
    "    class Config:\n",
    "        populate_by_name = True\n",
    "        extra = \"forbid\"\n",
    "\n",
    "class CancerPredictResponse(BaseModel):\n",
    "    \"\"\"Cancer prediction response.\"\"\"\n",
    "    predictions: List[str] = Field(..., description=\"Predicted diagnosis (M=malignant, B=benign)\")\n",
    "    probabilities: List[float] = Field(..., description=\"Probability of malignancy\")\n",
    "    uncertainties: Optional[List[float]] = Field(None, description=\"Uncertainty estimates (if requested)\")\n",
    "    input_received: List[CancerFeatures] = Field(..., description=\"Echo of input features\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b76902f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile api/app/schemas/iris.py\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "\n",
    "class IrisFeatures(BaseModel):\n",
    "    \"\"\"Iris measurement features.\"\"\"\n",
    "    sepal_length: float = Field(..., description=\"Sepal length in cm\", ge=0, le=10)\n",
    "    sepal_width: float = Field(..., description=\"Sepal width in cm\", ge=0, le=10)\n",
    "    petal_length: float = Field(..., description=\"Petal length in cm\", ge=0, le=10)\n",
    "    petal_width: float = Field(..., description=\"Petal width in cm\", ge=0, le=10)\n",
    "\n",
    "class IrisPredictRequest(BaseModel):\n",
    "    \"\"\"Iris prediction request (accepts legacy 'rows' alias).\"\"\"\n",
    "    model_type: str = Field(\"rf\", description=\"Model type: 'rf' or 'logreg'\")\n",
    "    samples: List[IrisFeatures] = Field(\n",
    "        ...,\n",
    "        description=\"List of iris measurements\",\n",
    "        alias=\"rows\",\n",
    "    )\n",
    "\n",
    "    class Config:\n",
    "        populate_by_name = True\n",
    "        extra = \"forbid\"\n",
    "\n",
    "class IrisPredictResponse(BaseModel):\n",
    "    \"\"\"Iris prediction response.\"\"\"\n",
    "    predictions: List[str] = Field(..., description=\"Predicted iris species\")\n",
    "    probabilities: List[List[float]] = Field(..., description=\"Class probabilities\")\n",
    "    input_received: List[IrisFeatures] = Field(..., description=\"Echo of input features\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1eeb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile api/app/schemas/bayes.py\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import Optional\n",
    "\n",
    "class BayesCancerParams(BaseModel):\n",
    "    draws: int = Field(1000, ge=200, le=20_000, description=\"Posterior draws retained\")\n",
    "    tune: int = Field(1000, ge=200, le=20_000, description=\"Tuning (warmup) steps\")\n",
    "    target_accept: float = Field(0.95, ge=0.80, le=0.999, description=\"NUTS target acceptance\")\n",
    "    compute_waic: bool = Field(True, description=\"Attempt WAIC (may be slow)\")\n",
    "    compute_loo: bool = Field(False, description=\"Attempt LOO (slower); auto-off by default\")\n",
    "    max_rhat_warn: float = Field(1.01, ge=1.0, le=1.1)\n",
    "    min_ess_warn: int = Field(400, ge=50, le=5000)\n",
    "\n",
    "    @validator(\"tune\")\n",
    "    def tune_reasonable(cls, v, values):\n",
    "        if \"draws\" in values and v < 0.2 * values[\"draws\"]:\n",
    "            # gentle warning, not rejection\n",
    "            pass\n",
    "        return v\n",
    "\n",
    "    def to_kwargs(self):\n",
    "        return {\n",
    "            \"draws\": self.draws,\n",
    "            \"tune\": self.tune,\n",
    "            \"target_accept\": self.target_accept,\n",
    "        } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31bc40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile api/app/ml/__init__.py\n",
    "\"\"\"\n",
    "ML sub-package â€“ exposes built-in trainers so the service can import\n",
    "`app.ml.builtin_trainers` with an absolute import.\n",
    "\"\"\"\n",
    "\n",
    "from .builtin_trainers import (\n",
    "    train_iris_random_forest,\n",
    "    train_iris_logreg,\n",
    "    train_breast_cancer_bayes,\n",
    "    train_breast_cancer_stub,\n",
    ")\n",
    "\n",
    "__all__ = [\n",
    "    \"train_iris_random_forest\",\n",
    "    \"train_iris_logreg\",\n",
    "    \"train_breast_cancer_bayes\",\n",
    "    \"train_breast_cancer_stub\",\n",
    "] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20849e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile api/app/ml/utils.py\n",
    "# api/app/ml/utils.py\n",
    "\n",
    "def configure_pytensor_compiler(*_, **__):\n",
    "    \"\"\"\n",
    "    Stub kept for backwardâ€‘compatibility.\n",
    "\n",
    "    The project now uses the **JAX backend**, so PyTensor never calls a C\n",
    "    compiler.  This function therefore does nothing and always returns True.\n",
    "    \"\"\"\n",
    "    return True\n",
    "\n",
    "# â”€â”€â”€ LEGACY ALIAS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Some early-boot modules import \"find_compiler\", so we alias it here\n",
    "find_compiler = configure_pytensor_compiler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdcd78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile api/app/ml/builtin_trainers.py\n",
    "# api/ml/builtin_trainers.py\n",
    "\"\"\"\n",
    "Built-in trainers for Iris RF and Breast-Cancer Bayesian LogReg.\n",
    "Executed automatically by ModelService when a model is missing.\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from pathlib import Path\n",
    "import mlflow, mlflow.sklearn, mlflow.pyfunc\n",
    "from sklearn.datasets import load_iris, load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import pickle\n",
    "import warnings\n",
    "import subprocess\n",
    "import os\n",
    "import platform\n",
    "from app.core.config import settings\n",
    "\n",
    "# Conditional imports for heavy dependencies\n",
    "if os.getenv(\"UNIT_TESTING\") != \"1\" and os.getenv(\"SKIP_BACKGROUND_TRAINING\") != \"1\":\n",
    "    import pymc as pm\n",
    "    import arviz as az\n",
    "else:\n",
    "    pm = None\n",
    "    az = None\n",
    "\n",
    "# --- ADD THIS NEAR THE TOP (after imports) ----------------------------------\n",
    "def _ensure_experiment(name: str = \"ml_fullstack_models\") -> str:\n",
    "    \"\"\"\n",
    "    Guarantee that `name` exists and return its experiment_id.\n",
    "    Handles the MLflow race where set_experiment returns a dangling ID\n",
    "    if the experiment folder has not been written yet.\n",
    "    \"\"\"\n",
    "    client = mlflow.tracking.MlflowClient()\n",
    "    exp = client.get_experiment_by_name(name)\n",
    "    if exp is None:\n",
    "        exp_id = client.create_experiment(name)\n",
    "    else:\n",
    "        exp_id = exp.experiment_id\n",
    "    mlflow.set_experiment(name)          # marks it the active one\n",
    "    return exp_id\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Honour whatever Settings or the shell already provided; then\n",
    "# fall back if the host part cannot be resolved quickly.\n",
    "# ------------------------------------------------------------------\n",
    "from urllib.parse import urlparse\n",
    "import socket, time\n",
    "\n",
    "def _fast_resolve(uri: str) -> bool:\n",
    "    if uri.startswith(\"http\"):\n",
    "        host = urlparse(uri).hostname\n",
    "        try:\n",
    "            t0 = time.perf_counter()\n",
    "            socket.getaddrinfo(host, None, proto=socket.IPPROTO_TCP)\n",
    "            return (time.perf_counter() - t0) < 0.05\n",
    "        except socket.gaierror:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "# MLflow tracking URI is now resolved in ModelService.initialize()\n",
    "# Trainers assume MLflow is already configured and experiments exist\n",
    "logger.info(\"ðŸ“¦ Trainers ready - MLflow URI will be resolved at service startup\")\n",
    "\n",
    "MLFLOW_EXPERIMENT = \"ml_fullstack_models\"\n",
    "\n",
    "# Remove side-effectful MLflow calls at import time\n",
    "# Experiments will be created on-demand in each trainer function\n",
    "\n",
    "# --- psutil health probe ----------------------------------------------------\n",
    "def _psutil_healthy() -> bool:\n",
    "    \"\"\"\n",
    "    Return True if psutil imports cleanly *and* exposes a working Process() object.\n",
    "    We cache the result because repeated checks are cheap but noisy in logs.\n",
    "    \"\"\"\n",
    "    global _PSUTIL_HEALTH_CACHE\n",
    "    try:\n",
    "        return _PSUTIL_HEALTH_CACHE\n",
    "    except NameError:\n",
    "        pass\n",
    "\n",
    "    ok = False\n",
    "    try:\n",
    "        import psutil  # type: ignore\n",
    "        ok = hasattr(psutil, \"Process\")\n",
    "        if ok:\n",
    "            try:\n",
    "                _ = psutil.Process().pid  # touch native layer\n",
    "            except Exception:  # bad native ext\n",
    "                ok = False\n",
    "    except Exception:\n",
    "        ok = False\n",
    "\n",
    "    _PSUTIL_HEALTH_CACHE = ok\n",
    "    if not ok:\n",
    "        logger.warning(\"ðŸ©º psutil unhealthy â€“ disabling sklearn/joblib parallelism (n_jobs=1).\")\n",
    "    return ok\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "#  IRIS â€“ point-estimate Random-Forest (enhanced with better parameters)\n",
    "# -----------------------------------------------------------------------------\n",
    "def train_iris_random_forest(\n",
    "    n_estimators: int = 300,\n",
    "    max_depth: int | None = None,\n",
    "    random_state: int = 42,\n",
    ") -> str:\n",
    "    from sklearn.datasets import load_iris\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    import mlflow, mlflow.pyfunc\n",
    "\n",
    "    # 1ï¸âƒ£  ALWAYS ensure the experiment exists *inside* the function\n",
    "    _ensure_experiment(MLFLOW_EXPERIMENT)\n",
    "\n",
    "    iris = load_iris(as_frame=True)\n",
    "    X, y = iris.data, iris.target\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "        X, y, test_size=0.25, stratify=y, random_state=random_state\n",
    "    )\n",
    "\n",
    "    safe_jobs = -1 if _psutil_healthy() else 1\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        random_state=random_state,\n",
    "        n_jobs=safe_jobs,\n",
    "        class_weight=\"balanced\",\n",
    "    ).fit(X_tr, y_tr)\n",
    "\n",
    "    preds = rf.predict(X_te)\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_te, preds),\n",
    "        \"f1_macro\": f1_score(y_te, preds, average=\"macro\"),\n",
    "        \"precision_macro\": precision_score(y_te, preds, average=\"macro\"),\n",
    "        \"recall_macro\": recall_score(y_te, preds, average=\"macro\"),\n",
    "    }\n",
    "\n",
    "    class IrisRFWrapper(mlflow.pyfunc.PythonModel):\n",
    "        def __init__(self, model):\n",
    "            if hasattr(model, \"n_jobs\"):\n",
    "                model.n_jobs = 1\n",
    "            self.model = model\n",
    "            self._cols = list(X.columns)\n",
    "\n",
    "        def _df(self, arr):\n",
    "            import pandas as pd, numpy as np\n",
    "            if isinstance(arr, pd.DataFrame):\n",
    "                return arr\n",
    "            return pd.DataFrame(np.asarray(arr), columns=self._cols)\n",
    "\n",
    "        def predict(self, context, model_input, params=None):\n",
    "            X_ = self._df(model_input)\n",
    "            return self.model.predict_proba(X_)\n",
    "\n",
    "    with mlflow.start_run(run_name=\"iris_random_forest\") as run:\n",
    "        mlflow.log_metrics(metrics)\n",
    "        mlflow.log_params({\n",
    "            \"n_estimators\": n_estimators,\n",
    "            \"max_depth\": max_depth,\n",
    "            \"random_state\": random_state,\n",
    "            \"safe_n_jobs\": safe_jobs,\n",
    "        })\n",
    "\n",
    "        sig = mlflow.models.signature.infer_signature(X, rf.predict_proba(X))\n",
    "        mlflow.pyfunc.log_model(\n",
    "            artifact_path=\"model\",            # âœ… correct kw-arg\n",
    "            python_model=IrisRFWrapper(rf),\n",
    "            registered_model_name=\"iris_random_forest\",\n",
    "            input_example=X.head(),\n",
    "            signature=sig,\n",
    "        )\n",
    "        return run.info.run_id\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "#  IRIS â€“ logistic-regression trainer (NEW)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def train_iris_logreg(\n",
    "    C: float = 1.0,\n",
    "    max_iter: int = 400,\n",
    "    random_state: int = 42,\n",
    ") -> str:\n",
    "    from sklearn.datasets import load_iris\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    import mlflow, mlflow.pyfunc\n",
    "\n",
    "    # 1ï¸âƒ£  ALWAYS ensure the experiment exists *inside* the function\n",
    "    _ensure_experiment(MLFLOW_EXPERIMENT)\n",
    "\n",
    "    iris = load_iris(as_frame=True)\n",
    "    X, y = iris.data, iris.target\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "        X, y, test_size=0.25, stratify=y, random_state=random_state\n",
    "    )\n",
    "\n",
    "    safe_jobs = -1 if _psutil_healthy() else 1\n",
    "    clf = LogisticRegression(\n",
    "        C=C,\n",
    "        max_iter=max_iter,\n",
    "        multi_class=\"multinomial\",\n",
    "        solver=\"lbfgs\",\n",
    "        n_jobs=safe_jobs,\n",
    "        random_state=random_state,\n",
    "    ).fit(X_tr, y_tr)\n",
    "\n",
    "    preds = clf.predict(X_te)\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_te, preds),\n",
    "        \"f1_macro\": f1_score(y_te, preds, average=\"macro\"),\n",
    "        \"precision_macro\": precision_score(y_te, preds, average=\"macro\"),\n",
    "        \"recall_macro\": recall_score(y_te, preds, average=\"macro\"),\n",
    "    }\n",
    "\n",
    "    class IrisLogRegWrapper(mlflow.pyfunc.PythonModel):\n",
    "        def __init__(self, model):\n",
    "            if hasattr(model, \"n_jobs\"):\n",
    "                model.n_jobs = 1\n",
    "            self.model = model\n",
    "            self._cols = list(X.columns)\n",
    "\n",
    "        def _df(self, arr):\n",
    "            import pandas as pd, numpy as np\n",
    "            if isinstance(arr, pd.DataFrame):\n",
    "                return arr\n",
    "            return pd.DataFrame(np.asarray(arr), columns=self._cols)\n",
    "\n",
    "        def predict(self, context, model_input, params=None):\n",
    "            X_ = self._df(model_input)\n",
    "            return self.model.predict_proba(X_)\n",
    "\n",
    "    with mlflow.start_run(run_name=\"iris_logreg\") as run:\n",
    "        mlflow.log_metrics(metrics)\n",
    "        mlflow.log_params({\n",
    "            \"C\": C,\n",
    "            \"max_iter\": max_iter,\n",
    "            \"random_state\": random_state,\n",
    "            \"safe_n_jobs\": safe_jobs,\n",
    "        })\n",
    "\n",
    "        sig = mlflow.models.signature.infer_signature(X, clf.predict_proba(X))\n",
    "        mlflow.pyfunc.log_model(\n",
    "            artifact_path=\"model\",            # âœ… correct kw-arg\n",
    "            python_model=IrisLogRegWrapper(clf),\n",
    "            registered_model_name=\"iris_logreg\",\n",
    "            input_example=X.head(),\n",
    "            signature=sig,\n",
    "        )\n",
    "        return run.info.run_id\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "#  BREAST-CANCER STUB â€“ ultra-fast fallback model\n",
    "# -----------------------------------------------------------------------------\n",
    "def train_breast_cancer_stub(random_state: int = 42) -> str:\n",
    "    \"\"\"\n",
    "    Ultra-fast fallback binary LogisticRegression on the breast-cancer dataset.\n",
    "\n",
    "    Serializes safely on Windows by forcing `n_jobs=1` if psutil unhealthy,\n",
    "    and exports MLflow PythonModel w/ modern signature that returns P(malignant).\n",
    "    References: joblib parallelism + psutil; MLflow PythonModel signature. :contentReference[oaicite:21]{index=21}\n",
    "    \"\"\"\n",
    "    from sklearn.datasets import load_breast_cancer\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    import mlflow, tempfile, pickle, pandas as pd\n",
    "\n",
    "    # 1ï¸âƒ£  ALWAYS ensure the experiment exists *inside* the function\n",
    "    _ensure_experiment(MLFLOW_EXPERIMENT)\n",
    "\n",
    "    X, y = load_breast_cancer(return_X_y=True, as_frame=True)\n",
    "    Xtr, Xte, ytr, yte = train_test_split(\n",
    "        X, y, test_size=0.3, stratify=y, random_state=random_state\n",
    "    )\n",
    "\n",
    "    safe_jobs = -1 if _psutil_healthy() else 1\n",
    "\n",
    "    clf = LogisticRegression(\n",
    "        max_iter=200, n_jobs=safe_jobs, random_state=random_state\n",
    "    ).fit(Xtr, ytr)\n",
    "\n",
    "    class CancerStubWrapper(mlflow.pyfunc.PythonModel):\n",
    "        def __init__(self, model):\n",
    "            if hasattr(model, \"n_jobs\"):\n",
    "                try:\n",
    "                    model.n_jobs = 1\n",
    "                except Exception:\n",
    "                    pass\n",
    "            self.model = model\n",
    "            self._cols = list(X.columns)\n",
    "\n",
    "        def _df(self, arr):\n",
    "            import pandas as pd, numpy as np\n",
    "            if isinstance(arr, pd.DataFrame):\n",
    "                return arr\n",
    "            return pd.DataFrame(np.asarray(arr), columns=self._cols)\n",
    "\n",
    "        def predict(self, context, model_input, params=None):\n",
    "            proba = self.model.predict_proba(self._df(model_input))\n",
    "            return proba[:, 1]  # malignant probability\n",
    "\n",
    "        def predict_proba(self, X):\n",
    "            return self.model.predict_proba(self._df(X))\n",
    "\n",
    "    acc = accuracy_score(yte, clf.predict(Xte))\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as td, mlflow.start_run(run_name=\"breast_cancer_stub\") as run:\n",
    "        # Log config hash for reproducibility and drift detection\n",
    "        from app.core.config import settings as _s\n",
    "        import hashlib, json\n",
    "        _cfg_hash = hashlib.sha256(json.dumps(_s.model_dump(), sort_keys=True).encode()).hexdigest()\n",
    "        mlflow.log_param(\"train_config_hash\", _cfg_hash)\n",
    "\n",
    "        mlflow.log_metric(\"accuracy\", acc)\n",
    "        mlflow.log_param(\"safe_n_jobs\", safe_jobs)\n",
    "        wrapper = CancerStubWrapper(clf)\n",
    "        sig = mlflow.models.signature.infer_signature(X, wrapper.predict(None, X))\n",
    "        mlflow.pyfunc.log_model(\n",
    "            \"model\",\n",
    "            python_model=wrapper,\n",
    "            registered_model_name=\"breast_cancer_stub\",\n",
    "            input_example=X.head(),\n",
    "            signature=sig,\n",
    "        )\n",
    "        return run.info.run_id\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "#  BREAST-CANCER â€“ hierarchical Bayesian logistic regression\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def train_breast_cancer_bayes(\n",
    "    draws: int = 1000,\n",
    "    tune: int = 1000,\n",
    "    target_accept: float = 0.99,\n",
    "    params_obj=None,   # optional BayesCancerParams\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Hierarchical Bayesian logistic regression with varying intercepts.\n",
    "\n",
    "    Adds:\n",
    "      * Validated hyperparams via BayesCancerParams (if provided)\n",
    "      * Convergence diagnostics: R-hat, bulk/tail ESS\n",
    "      * Optional WAIC / LOO (guarded; can be disabled)\n",
    "      * Logged warnings if thresholds exceeded\n",
    "    \"\"\"\n",
    "    import pymc as pm\n",
    "    import pandas as pd, numpy as np\n",
    "    from sklearn.datasets import load_breast_cancer\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    import mlflow, tempfile, pickle\n",
    "    from pathlib import Path\n",
    "\n",
    "    # Schema override if provided\n",
    "    if params_obj is not None:\n",
    "        draws = params_obj.draws\n",
    "        tune = params_obj.tune\n",
    "        target_accept = params_obj.target_accept\n",
    "        compute_waic = params_obj.compute_waic\n",
    "        compute_loo = params_obj.compute_loo\n",
    "        max_rhat_warn = params_obj.max_rhat_warn\n",
    "        min_ess_warn = params_obj.min_ess_warn\n",
    "    else:\n",
    "        compute_waic = True\n",
    "        compute_loo = False\n",
    "        max_rhat_warn = 1.01\n",
    "        min_ess_warn = 400\n",
    "\n",
    "    logger.info(\n",
    "        \"BayesCancer: draws=%d tune=%d target_accept=%.3f waic=%s loo=%s\",\n",
    "        draws, tune, target_accept, compute_waic, compute_loo\n",
    "    )\n",
    "\n",
    "    _ensure_experiment(MLFLOW_EXPERIMENT)\n",
    "\n",
    "    X_df, y = load_breast_cancer(as_frame=True, return_X_y=True)\n",
    "    quint, edges = pd.qcut(X_df[\"mean texture\"], 5, labels=False, retbins=True)\n",
    "    g = np.asarray(quint, dtype=\"int64\")\n",
    "    scaler = StandardScaler().fit(X_df)\n",
    "    Xs = scaler.transform(X_df)\n",
    "\n",
    "    coords = {\"group\": np.arange(5)}\n",
    "    with pm.Model(coords=coords) as m:\n",
    "        Î± = pm.Normal(\"Î±\", 0.0, 1.0, dims=\"group\")\n",
    "        Î² = pm.Normal(\"Î²\", 0.0, 1.0, shape=Xs.shape[1])\n",
    "        logit = Î±[g] + pm.math.dot(Xs, Î²)\n",
    "        pm.Bernoulli(\"obs\", logit_p=logit, observed=y)\n",
    "        idata = pm.sample(\n",
    "            draws=draws,\n",
    "            tune=tune,\n",
    "            chains=4,\n",
    "            nuts_sampler=\"numpyro\",\n",
    "            target_accept=target_accept,\n",
    "            progressbar=False,\n",
    "        )\n",
    "\n",
    "    # Diagnostics\n",
    "    import arviz as az\n",
    "    rhat = az.rhat(idata).to_array().values.max()\n",
    "    ess_bulk = az.ess(idata, method=\"bulk\").to_array().values.min()\n",
    "    ess_tail = az.ess(idata, method=\"tail\").to_array().values.min()\n",
    "    waic_val = None\n",
    "    loo_val = None\n",
    "    try:\n",
    "        if compute_waic:\n",
    "            waic_val = float(az.waic(idata).waic)\n",
    "    except Exception as e:\n",
    "        logger.warning(\"WAIC computation failed: %s\", e)\n",
    "    try:\n",
    "        if compute_loo:\n",
    "            loo_val = float(az.loo(idata).loo)\n",
    "    except Exception as e:\n",
    "        logger.warning(\"LOO computation failed: %s\", e)\n",
    "\n",
    "    # Wrapper\n",
    "    class _HierBayesWrapper(mlflow.pyfunc.PythonModel):\n",
    "        def __init__(self, trace, sc, ed, cols):\n",
    "            self.trace, self.scaler, self.edges, self.cols = trace, sc, ed, cols\n",
    "\n",
    "        def _quint(self, df):\n",
    "            col = \"mean texture\"\n",
    "            if col not in df.columns and \"mean_texture\" in df.columns:\n",
    "                df = df.rename(columns={\"mean_texture\": col})\n",
    "            tex = df[col].to_numpy()\n",
    "            return np.clip(np.digitize(tex, self.edges, right=False), 0, 4)\n",
    "\n",
    "        def predict(self, context, model_input, params=None):\n",
    "            df = model_input if isinstance(model_input, pd.DataFrame) else pd.DataFrame(model_input, columns=self.cols)\n",
    "            xs = self.scaler.transform(df)\n",
    "            g = self._quint(df)\n",
    "            Î±g = self.trace.posterior[\"Î±\"].median((\"chain\", \"draw\")).values\n",
    "            Î² = self.trace.posterior[\"Î²\"].median((\"chain\", \"draw\")).values\n",
    "            log = Î±g[g] + np.dot(xs, Î²)\n",
    "            return 1.0 / (1.0 + np.exp(-log))\n",
    "\n",
    "    wrapper = _HierBayesWrapper(idata, scaler, edges[1:-1], X_df.columns.tolist())\n",
    "    preds = wrapper.predict(None, X_df)\n",
    "    acc = float(((preds > 0.5).astype(int) == y).mean())\n",
    "\n",
    "    # Threshold warnings\n",
    "    if rhat > max_rhat_warn:\n",
    "        logger.warning(\"R-hat exceeds threshold: %.4f > %.2f\", rhat, max_rhat_warn)\n",
    "    if ess_bulk < min_ess_warn:\n",
    "        logger.warning(\"Bulk ESS below threshold: %.1f < %d\", ess_bulk, min_ess_warn)\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as td, mlflow.start_run(run_name=\"breast_cancer_bayes\") as run:\n",
    "        from app.core.config import settings as _s\n",
    "        import hashlib, json\n",
    "        _cfg_hash = hashlib.sha256(json.dumps(_s.model_dump(), sort_keys=True).encode()).hexdigest()\n",
    "\n",
    "        # Metrics\n",
    "        mlflow.log_metric(\"accuracy\", acc)\n",
    "        mlflow.log_metric(\"rhat_max\", rhat)\n",
    "        mlflow.log_metric(\"ess_bulk_min\", ess_bulk)\n",
    "        mlflow.log_metric(\"ess_tail_min\", ess_tail)\n",
    "        if waic_val is not None:\n",
    "            mlflow.log_metric(\"waic\", waic_val)\n",
    "        if loo_val is not None:\n",
    "            mlflow.log_metric(\"loo\", loo_val)\n",
    "\n",
    "        # Params\n",
    "        mlflow.log_param(\"train_config_hash\", _cfg_hash)\n",
    "        mlflow.log_param(\"draws\", draws)\n",
    "        mlflow.log_param(\"tune\", tune)\n",
    "        mlflow.log_param(\"target_accept\", target_accept)\n",
    "        mlflow.log_param(\"compute_waic\", compute_waic)\n",
    "        mlflow.log_param(\"compute_loo\", compute_loo)\n",
    "\n",
    "        sc_path = Path(td) / \"scaler.pkl\"\n",
    "        pickle.dump(scaler, open(sc_path, \"wb\"))\n",
    "        mlflow.pyfunc.log_model(\n",
    "            \"model\",\n",
    "            python_model=wrapper,\n",
    "            artifacts={\"scaler\": str(sc_path)},\n",
    "            registered_model_name=\"breast_cancer_bayes\",\n",
    "            input_example=X_df.head(),\n",
    "            signature=mlflow.models.signature.infer_signature(X_df, wrapper.predict(None, X_df)),\n",
    "        )\n",
    "        return run.info.run_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549c2f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile api/app/services/ml/model_service.py\n",
    "\"\"\"\n",
    "Model service â€“ self-healing startup with background training.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import asyncio, logging, os, time, socket, shutil, subprocess, hashlib, json\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "import mlflow, pandas as pd, numpy as np\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.exceptions import MlflowException\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# IMPORTANT IMPORT NOTE\n",
    "# ------------------------------------------------------------------\n",
    "# This module lives in app/services/ml/.\n",
    "# To reach the sibling top-level package app/core/ we must step\n",
    "# *two* levels up (services/ml -> services -> app) before importing.\n",
    "# Rather than counting dots ('from ...core.config import settings'),\n",
    "# we choose *absolute imports* for clarity & stability across refactors.\n",
    "# See Python import system docs + Real Python discussion on why absolute\n",
    "# imports are preferred for larger projects.  :contentReference[oaicite:25]{index=25}\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "from app.core.config import settings\n",
    "from app.ml.builtin_trainers import (\n",
    "    train_iris_random_forest,\n",
    "    train_iris_logreg,  # NEW\n",
    "    train_breast_cancer_bayes,\n",
    "    train_breast_cancer_stub,\n",
    ")\n",
    "\n",
    "# NEW imports for registry integration\n",
    "from importlib import import_module\n",
    "try:\n",
    "    from src.registry import registry as dynamic_registry  # noqa\n",
    "except Exception:\n",
    "    dynamic_registry = None  # tolerant if path not yet packaged\n",
    "\n",
    "\n",
    "# from ..core.config import settings\n",
    "# from ..ml.builtin_trainers import (\n",
    "#     train_iris_random_forest,\n",
    "#     train_iris_logreg,  # NEW\n",
    "#     train_breast_cancer_bayes,\n",
    "#     train_breast_cancer_stub,\n",
    "# )\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- safe sklearn predict_proba helper ---------------------------------------\n",
    "def _safe_sklearn_proba(estimator, X, *, log_prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Call estimator.predict_proba(X) but recover from environments where\n",
    "    joblib/loky -> psutil introspection explodes (e.g., AttributeError: psutil.Process).\n",
    "\n",
    "    Strategy:\n",
    "    1. Try fast path.\n",
    "    2. On AttributeError mentioning psutil (or any RuntimeError from joblib),\n",
    "       set estimator.n_jobs = 1 if present and retry serially.\n",
    "    3. As a last resort, call estimator.predict(X) and synthesize 1-hot probs.\n",
    "\n",
    "    Returns a NumPy array of shape (n_samples, n_classes).\n",
    "    \"\"\"\n",
    "    import numpy as _np\n",
    "    from joblib import parallel_backend\n",
    "\n",
    "    # Make sure we have an array / DataFrame scikit can handle\n",
    "    X_ = X\n",
    "\n",
    "    # 1st attempt --------------------------------------------------------------\n",
    "    try:\n",
    "        return estimator.predict_proba(X_)\n",
    "    except Exception as e:  # broad â†’ we inspect below\n",
    "        msg = str(e)\n",
    "        bad_psutil = \"psutil\" in msg and \"Process\" in msg\n",
    "        if not bad_psutil:\n",
    "            logger.warning(\"%s predict_proba failed (%s) â€“ retry single-threaded\",\n",
    "                           log_prefix, e)\n",
    "\n",
    "        # 2nd attempt: force serial backend -----------------------------------\n",
    "        try:\n",
    "            if hasattr(estimator, \"n_jobs\"):\n",
    "                try:\n",
    "                    estimator.n_jobs = 1\n",
    "                except Exception:  # read-only attr\n",
    "                    pass\n",
    "            with parallel_backend(\"threading\", n_jobs=1):\n",
    "                return estimator.predict_proba(X_)\n",
    "        except Exception as e2:\n",
    "            logger.error(\"%s serial predict_proba failed (%s) â€“ fallback to classes\",\n",
    "                         log_prefix, e2)\n",
    "\n",
    "    # 3rd attempt: derive 1-hot from predict ----------------------------------\n",
    "    try:\n",
    "        preds = estimator.predict(X_)\n",
    "        preds = _np.asarray(preds, dtype=int)\n",
    "        n_classes = getattr(estimator, \"n_classes_\", preds.max() + 1)\n",
    "        probs = _np.zeros((preds.size, n_classes), dtype=float)\n",
    "        probs[_np.arange(preds.size), preds] = 1.0\n",
    "        return probs\n",
    "    except Exception as e3:\n",
    "        logger.exception(\"%s fallback predict also failed (%s)\", log_prefix, e3)\n",
    "        raise  # Let caller handle\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Cancer column mapping: Pydantic field names âžœ training column names\n",
    "# ---------------------------------------------------------------------------\n",
    "_CANCER_COLMAP: dict[str, str] = {\n",
    "    # Means\n",
    "    \"mean_radius\": \"mean radius\",\n",
    "    \"mean_texture\": \"mean texture\",\n",
    "    \"mean_perimeter\": \"mean perimeter\",\n",
    "    \"mean_area\": \"mean area\",\n",
    "    \"mean_smoothness\": \"mean smoothness\",\n",
    "    \"mean_compactness\": \"mean compactness\",\n",
    "    \"mean_concavity\": \"mean concavity\",\n",
    "    \"mean_concave_points\": \"mean concave points\",\n",
    "    \"mean_symmetry\": \"mean symmetry\",\n",
    "    \"mean_fractal_dimension\": \"mean fractal dimension\",\n",
    "    # SE\n",
    "    \"se_radius\": \"radius error\",\n",
    "    \"se_texture\": \"texture error\",\n",
    "    \"se_perimeter\": \"perimeter error\",\n",
    "    \"se_area\": \"area error\",\n",
    "    \"se_smoothness\": \"smoothness error\",\n",
    "    \"se_compactness\": \"compactness error\",\n",
    "    \"se_concavity\": \"concavity error\",\n",
    "    \"se_concave_points\": \"concave points error\",\n",
    "    \"se_symmetry\": \"symmetry error\",\n",
    "    \"se_fractal_dimension\": \"fractal dimension error\",\n",
    "    # Worst\n",
    "    \"worst_radius\": \"worst radius\",\n",
    "    \"worst_texture\": \"worst texture\",\n",
    "    \"worst_perimeter\": \"worst perimeter\",\n",
    "    \"worst_area\": \"worst area\",\n",
    "    \"worst_smoothness\": \"worst smoothness\",\n",
    "    \"worst_compactness\": \"worst compactness\",\n",
    "    \"worst_concavity\": \"worst concavity\",\n",
    "    \"worst_concave_points\": \"worst concave points\",\n",
    "    \"worst_symmetry\": \"worst symmetry\",\n",
    "    \"worst_fractal_dimension\": \"worst fractal dimension\",\n",
    "}\n",
    "\n",
    "def _rename_cancer_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ensure DataFrame columns match the training schema used by MLflow artefacts.\n",
    "    Unknown columns are left untouched so legacy models still work.\n",
    "    \"\"\"\n",
    "    return df.rename(columns=_CANCER_COLMAP)\n",
    "\n",
    "# Trainer mapping for self-healing\n",
    "TRAINERS = {\n",
    "    \"iris_random_forest\": train_iris_random_forest,\n",
    "    \"iris_logreg\":        train_iris_logreg,  # NEW\n",
    "    \"breast_cancer_bayes\": train_breast_cancer_bayes,\n",
    "    \"breast_cancer_stub\":  train_breast_cancer_stub,\n",
    "}\n",
    "\n",
    "class ModelService:\n",
    "    \"\"\"\n",
    "    Self-healing model service that loads existing models and schedules\n",
    "    background training for missing ones.\n",
    "    \"\"\"\n",
    "\n",
    "    _EXECUTOR = ThreadPoolExecutor(max_workers=2)\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self._unit_test_mode = settings.UNIT_TESTING\n",
    "        self.initialized = False\n",
    "\n",
    "        # ðŸš« Heavy clients only when NOT unit-testing\n",
    "        self.client = None if self._unit_test_mode else None  # Will be set in initialize()\n",
    "        self.mlflow_client = None\n",
    "\n",
    "        self.models: Dict[str, Any] = {}\n",
    "        # registry bootstrap flags\n",
    "        self._registry_loaded = False\n",
    "        self.status: Dict[str, str] = {\n",
    "            \"iris_random_forest\": \"missing\",\n",
    "            \"iris_logreg\":        \"missing\",  # NEW\n",
    "            \"breast_cancer_bayes\": \"missing\",\n",
    "            \"breast_cancer_stub\": \"missing\",\n",
    "        }\n",
    "\n",
    "    # --- Registry integration (increment 1) ---------------------------------\n",
    "    def _init_registry_once(self):\n",
    "        if self._registry_loaded:\n",
    "            return\n",
    "        if dynamic_registry is None:\n",
    "            logger.info(\"Registry package not available yet; skipping dynamic trainer loading.\")\n",
    "            self._registry_loaded = True\n",
    "            return\n",
    "        try:\n",
    "            # Hardcode first migrated trainer; later we will iterate YAML directory.\n",
    "            from src.registry.registry import load_from_entry_point  # type: ignore\n",
    "            load_from_entry_point(\"src.trainers.iris_rf_trainer:IrisRandomForestTrainer\")\n",
    "            self._registry_loaded = True\n",
    "            logger.info(\"Dynamic registry initialized with trainers: %s\",\n",
    "                        list(dynamic_registry.all_names()))\n",
    "        except Exception as e:\n",
    "            logger.warning(\"Failed to initialize registry: %s\", e)\n",
    "            self._registry_loaded = True  # prevent retry storm\n",
    "\n",
    "    def _get_trainer_or_none(self, name: str):\n",
    "        if not self._registry_loaded:\n",
    "            self._init_registry_once()\n",
    "        try:\n",
    "            from src.registry.registry import get as reg_get  # type: ignore\n",
    "            return reg_get(name)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    async def train_via_registry(self, name: str, overrides: Dict[str, Any] | None = None) -> Optional[str]:\n",
    "        spec = self._get_trainer_or_none(name)\n",
    "        if spec is None:\n",
    "            logger.info(\"No registry trainer for %s\", name)\n",
    "            return None\n",
    "        trainer = spec.cls()\n",
    "        # merge overrides\n",
    "        params = trainer.merge_hyperparams(overrides or {})\n",
    "        loop = asyncio.get_running_loop()\n",
    "        logger.info(\"Training %s via registry with params=%s\", name, params)\n",
    "        result = await loop.run_in_executor(self._EXECUTOR, lambda: trainer.train(**params))\n",
    "        # After training, force reload of production candidate (latest run fallback)\n",
    "        await self._try_load(name)\n",
    "        return result.run_id\n",
    "\n",
    "    def _resolve_tracking_uri(self) -> str:\n",
    "        \"\"\"\n",
    "        Resolve MLflow tracking URI with graceful fallback:\n",
    "          1. Explicit env var MLFLOW_TRACKING_URI\n",
    "          2. settings.MLFLOW_TRACKING_URI\n",
    "          3. local file store 'file:./mlruns_local'\n",
    "        DNS / connection problems downgrade to local file store.\n",
    "        \"\"\"\n",
    "        import socket, urllib.parse, mlflow\n",
    "        candidates = []\n",
    "        if os.getenv(\"MLFLOW_TRACKING_URI\"):\n",
    "            candidates.append((\"env\", os.getenv(\"MLFLOW_TRACKING_URI\")))\n",
    "        candidates.append((\"settings\", settings.MLFLOW_TRACKING_URI))\n",
    "        candidates.append((\"fallback\", \"file:./mlruns_local\"))\n",
    "\n",
    "        for origin, uri in candidates:\n",
    "            parsed = urllib.parse.urlparse(uri)\n",
    "            if parsed.scheme in (\"http\", \"https\"):\n",
    "                host = parsed.hostname\n",
    "                try:\n",
    "                    socket.getaddrinfo(host, parsed.port or 80)\n",
    "                    logger.info(\"MLflow URI ok (%s): %s\", origin, uri)\n",
    "                    return uri\n",
    "                except socket.gaierror as e:\n",
    "                    logger.warning(\"MLflow URI unresolved (%s=%s) -> %s\", origin, uri, e)\n",
    "            else:\n",
    "                # file store always acceptable\n",
    "                logger.info(\"MLflow file store selected (%s): %s\", origin, uri)\n",
    "                return uri\n",
    "\n",
    "        return \"file:./mlruns_local\"\n",
    "\n",
    "    async def initialize(self) -> None:\n",
    "        \"\"\"\n",
    "        Connect to MLflow â€“ fall back to local file store if the configured\n",
    "        tracking URI is unreachable *or* the client is missing critical methods\n",
    "        (e.g. when mlflow-skinny accidentally shadows the full package).\n",
    "        \"\"\"\n",
    "        if self.initialized:\n",
    "            return\n",
    "\n",
    "        # Log critical dependency versions for diagnostics\n",
    "        try:\n",
    "            import pytensor\n",
    "            logger.info(\"ðŸ“¦ PyTensor version: %s\", pytensor.__version__)\n",
    "        except ImportError:\n",
    "            logger.warning(\"âš ï¸  PyTensor not available\")\n",
    "        except Exception as e:\n",
    "            logger.warning(\"âš ï¸  Could not determine PyTensor version: %s\", e)\n",
    "\n",
    "        def _needs_fallback(client) -> bool:\n",
    "            # any missing attr is a strong signal we are on mlflow-skinny\n",
    "            return not callable(getattr(client, \"list_experiments\", None))\n",
    "\n",
    "        try:\n",
    "            resolved = self._resolve_tracking_uri()\n",
    "            mlflow.set_tracking_uri(resolved)\n",
    "            logger.info(\"Using tracking URI: %s\", resolved)\n",
    "            self.mlflow_client = MlflowClient(resolved)\n",
    "\n",
    "            if _needs_fallback(self.mlflow_client):\n",
    "                raise AttributeError(\"list_experiments not implemented â€“ skinny build detected\")\n",
    "\n",
    "            # minimal probe (cheap & always present)\n",
    "            self.mlflow_client.search_experiments(max_results=1)\n",
    "            logger.info(\"ðŸŸ¢  Connected to MLflow @ %s\", resolved)\n",
    "\n",
    "        except (MlflowException, socket.gaierror, AttributeError) as exc:\n",
    "            logger.warning(\"ðŸ”„  Falling back to local MLflow store â€“ %s\", exc)\n",
    "            mlflow.set_tracking_uri(\"file:./mlruns_local\")\n",
    "            self.mlflow_client = MlflowClient(\"file:./mlruns_local\")\n",
    "            logger.info(\"ðŸ“‚  Using local file store ./mlruns_local\")\n",
    "\n",
    "        await self._load_models()\n",
    "        self.initialized = True\n",
    "\n",
    "    async def _load_models(self) -> None:\n",
    "        \"\"\"Load existing models from MLflow.\"\"\"\n",
    "        for name in [\"iris_random_forest\", \"iris_logreg\",\n",
    "                     \"breast_cancer_bayes\", \"breast_cancer_stub\"]:\n",
    "            try:\n",
    "                await self._try_load(name)\n",
    "            except Exception as exc:\n",
    "                logger.error(\"âŒ  load %s failed: %s\", name, exc)\n",
    "\n",
    "    async def startup(self, auto_train: bool | None = None) -> None:\n",
    "        \"\"\"\n",
    "        Faster: serve stub immediately; heavy Bayesian job in background.\n",
    "        \"\"\"\n",
    "        if self._unit_test_mode:\n",
    "            logger.info(\"ðŸ”’ UNIT_TESTING=1 â€“ skipping model loading\")\n",
    "            return                      # ðŸ‘‰ nothing else runs\n",
    "\n",
    "        # Initialize MLflow connection first\n",
    "        await self.initialize()\n",
    "\n",
    "        if settings.SKIP_BACKGROUND_TRAINING and not settings.AUTO_TRAIN_MISSING:\n",
    "            logger.warning(\"â© Both training flags disabled â€“ models must already exist\")\n",
    "            # We still *try* to load existing artefacts so prod works\n",
    "            await self._try_load(\"iris_random_forest\")\n",
    "            await self._try_load(\"iris_logreg\")\n",
    "            await self._try_load(\"breast_cancer_bayes\")\n",
    "            return\n",
    "\n",
    "        auto = auto_train if auto_train is not None else settings.AUTO_TRAIN_MISSING\n",
    "        logger.info(\"ðŸ”„ Model-service startup (auto_train=%s)\", auto)\n",
    "\n",
    "        # Registry-aware load for migrated models\n",
    "        self._init_registry_once()\n",
    "\n",
    "        # Try dynamic load first for iris_random_forest\n",
    "        loaded_rf = await self._try_load(\"iris_random_forest\")\n",
    "        if not loaded_rf and auto:\n",
    "            # prefer registry path\n",
    "            run_id = await self.train_via_registry(\"iris_random_forest\")\n",
    "            if run_id:\n",
    "                await self._try_load(\"iris_random_forest\")\n",
    "\n",
    "        # Legacy deterministic model (to be migrated later)\n",
    "        if not await self._try_load(\"iris_logreg\") and auto:\n",
    "            logger.info(\"Training iris logistic-regression (legacy path)â€¦\")\n",
    "            await asyncio.get_running_loop().run_in_executor(\n",
    "                self._EXECUTOR, train_iris_logreg\n",
    "            )\n",
    "            await self._try_load(\"iris_logreg\")\n",
    "\n",
    "        # Bayesian path unchanged (will migrate later)\n",
    "        if not await self._try_load(\"breast_cancer_bayes\"):\n",
    "            if not await self._try_load(\"breast_cancer_stub\") and auto:\n",
    "                logger.info(\"Training stub cancer model â€¦\")\n",
    "                await asyncio.get_running_loop().run_in_executor(\n",
    "                    self._EXECUTOR, train_breast_cancer_stub\n",
    "                )\n",
    "                await self._try_load(\"breast_cancer_stub\")\n",
    "            if auto and not settings.SKIP_BACKGROUND_TRAINING:\n",
    "                logger.info(\"Scheduling Bayesian retrain in background\")\n",
    "                asyncio.create_task(\n",
    "                    self._train_and_reload(\"breast_cancer_bayes\", train_breast_cancer_bayes)\n",
    "                )\n",
    "\n",
    "    async def _try_load(self, name: str) -> bool:\n",
    "        \"\"\"Try to load a model and update status.\"\"\"\n",
    "        try:\n",
    "            model = await self._load_production_model(name)\n",
    "            if model:\n",
    "                self.models[name] = model\n",
    "                self.status[name] = \"loaded\"\n",
    "                logger.info(\"âœ… %s loaded\", name)\n",
    "                return True\n",
    "            self.status.setdefault(name, \"missing\")\n",
    "            return False\n",
    "        except Exception as exc:\n",
    "            logger.error(\"âŒ  load %s failed: %s\", name, exc)\n",
    "            self.status[name] = \"failed\"\n",
    "            self.status[f\"{name}_last_error\"] = str(exc)\n",
    "            return False\n",
    "\n",
    "    async def _train_and_reload(self, name: str, trainer) -> None:\n",
    "        \"\"\"Train a model in background and reload it, with verbose phase logs.\"\"\"\n",
    "        try:\n",
    "            t0 = time.perf_counter()\n",
    "            logger.info(\"ðŸ—ï¸  BEGIN training %s\", name)\n",
    "            self.status[name] = \"training\"\n",
    "\n",
    "            loop = asyncio.get_running_loop()\n",
    "            await loop.run_in_executor(self._EXECUTOR, trainer)\n",
    "\n",
    "            logger.info(\"ðŸ“¦ Training %s complete in %.1fs â€“ re-loading\", name,\n",
    "                        time.perf_counter() - t0)\n",
    "            model = await self._load_production_model(name)\n",
    "            if not model:\n",
    "                raise RuntimeError(f\"{name} trained but could not be re-loaded\")\n",
    "\n",
    "            self.models[name] = model\n",
    "            self.status[name] = \"loaded\"\n",
    "\n",
    "            # Trigger retention cleanâ€‘up in background\n",
    "            loop = asyncio.get_running_loop()\n",
    "            loop.run_in_executor(self._EXECUTOR,\n",
    "                                 lambda: asyncio.run(self._cleanup_runs(name)))\n",
    "            logger.info(\"âœ… %s trained & loaded\", name)\n",
    "\n",
    "        except Exception as exc:\n",
    "            self.status[name] = \"failed\"\n",
    "            logger.error(\"âŒ %s failed: %s\", name, exc, exc_info=True)  # â† keeps trace\n",
    "            # NEW: persist last_error for UI / debug endpoint\n",
    "            self.status[f\"{name}_last_error\"] = str(exc)\n",
    "\n",
    "# --- DROP-IN REPLACEMENT ----------------------------------------------------\n",
    "    async def _load_production_model(self, name: str):\n",
    "        \"\"\"\n",
    "        Load the canonical production model with alias support *and* perform an\n",
    "        **environment audit** of the recorded vs. runtime dependencies.\n",
    "\n",
    "        We DO NOT install anything automatically.  Instead we:\n",
    "            â€¢ attempt to load in the usual fallback order (@prod â†’ @staging â†’ Production stage â†’ latest run)\n",
    "            â€¢ after a *successful* load, call `_audit_model_env(uri, name)` to diff\n",
    "              the model's logged environment spec against the current runtime\n",
    "              (importlib.metadata) and record mismatches in `self.status`.\n",
    "\n",
    "        The audit is *diagnostic* unless an optional enforcement policy is enabled\n",
    "        via env/config (MODEL_ENV_ENFORCEMENT = warn|fail|retrain).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Loaded MLflow model instance *or* None if nothing could be loaded, or\n",
    "        load was refused under \"fail\" policy for env mismatch.\n",
    "        \"\"\"\n",
    "        import sys\n",
    "        import mlflow\n",
    "        from mlflow.tracking.artifact_utils import _download_artifact_from_uri\n",
    "        from packaging.version import Version, InvalidVersion\n",
    "        import importlib.metadata as im\n",
    "        import json\n",
    "        import os\n",
    "\n",
    "        # Use MLOps configuration for enforcement policy\n",
    "        policy = settings.MODEL_AUDIT_ENFORCEMENT.lower()\n",
    "\n",
    "        def _warn_model_env(uri: str) -> None:\n",
    "            # unchanged bestâ€‘effort header check (Python version)\n",
    "            try:\n",
    "                local_dir = _download_artifact_from_uri(uri)\n",
    "                mlmodel_path = Path(local_dir) / \"MLmodel\"\n",
    "                if not mlmodel_path.is_file():\n",
    "                    return\n",
    "                import yaml\n",
    "                meta = yaml.safe_load(mlmodel_path.read_text())\n",
    "                py_model_ver = (\n",
    "                    meta.get(\"python_env\", {}).get(\"python\")\n",
    "                    or meta.get(\"flavors\", {})\n",
    "                        .get(\"python_function\", {})\n",
    "                        .get(\"loader_module_python_version\")\n",
    "                )\n",
    "                runtime_py = f\"{sys.version_info.major}.{sys.version_info.minor}\"\n",
    "                if py_model_ver and not py_model_ver.startswith(runtime_py):\n",
    "                    logger.warning(\n",
    "                        \"âš ï¸ %s logged under Python %s but runtime is %s; \"\n",
    "                        \"deserialization may fail. Consider retraining.\",\n",
    "                        name, py_model_ver, runtime_py\n",
    "                    )\n",
    "            except Exception as e:  # best-effort\n",
    "                logger.debug(\"env check failed for %s (%s)\", uri, e)\n",
    "\n",
    "        def _audit_model_env(uri: str, model_name: str) -> dict:\n",
    "            \"\"\"\n",
    "            Return a dict: {pkg: {'required': spec, 'current': ver, 'match': bool, 'severity': str}}\n",
    "            Only logs the pipâ€‘install command if there are mismatches.\n",
    "            \"\"\"\n",
    "            import logging\n",
    "            from pathlib import Path\n",
    "            from packaging.version import Version, InvalidVersion\n",
    "            import importlib.metadata as im\n",
    "            from mlflow.pyfunc import get_model_dependencies\n",
    "\n",
    "            audit: dict[str, dict] = {}\n",
    "\n",
    "            # 1ï¸âƒ£ Suppress MLflow's own INFO log for pip install\n",
    "            pyfunc_logger = logging.getLogger(\"mlflow.pyfunc\")\n",
    "            old_level = pyfunc_logger.level\n",
    "            pyfunc_logger.setLevel(logging.WARNING)\n",
    "            try:\n",
    "                try:\n",
    "                    deps_path = get_model_dependencies(uri)\n",
    "                except Exception:\n",
    "                    deps_path = None\n",
    "            finally:\n",
    "                pyfunc_logger.setLevel(old_level)\n",
    "\n",
    "            # 2ï¸âƒ£ Read the pip requirements.txt\n",
    "            req_lines: list[str] = []\n",
    "            if deps_path and Path(deps_path).is_file():\n",
    "                for ln in Path(deps_path).read_text().splitlines():\n",
    "                    ln = ln.strip()\n",
    "                    if not ln or ln.startswith(\"#\"):\n",
    "                        continue\n",
    "                    req_lines.append(ln)\n",
    "\n",
    "            # 3ï¸âƒ£ Build the audit by comparing to runtime versions\n",
    "            for spec in req_lines:\n",
    "                pkg = spec.split(\"@\", 1)[0].split(\";\", 1)[0].strip()\n",
    "                pkg_lc = pkg.lower().replace(\"_\", \"-\")\n",
    "\n",
    "                req_ver = None\n",
    "                if \"==\" in spec:\n",
    "                    req_ver = spec.split(\"==\", 1)[1].strip()\n",
    "                elif \">=\" in spec:\n",
    "                    req_ver = spec.split(\">=\", 1)[1].strip()\n",
    "\n",
    "                cur_ver = None\n",
    "                try:\n",
    "                    cur_ver = im.version(pkg_lc)\n",
    "                except Exception:\n",
    "                    cur_ver = None\n",
    "\n",
    "                match = True\n",
    "                sev = \"OK\"\n",
    "                if req_ver:\n",
    "                    try:\n",
    "                        v_req = Version(req_ver)\n",
    "                        if cur_ver:\n",
    "                            v_cur = Version(cur_ver)\n",
    "                            if v_cur.major != v_req.major:\n",
    "                                sev, match = \"MAJOR_DRT\", False\n",
    "                            elif v_cur != v_req:\n",
    "                                sev, match = \"MINOR_DRT\", False\n",
    "                        else:\n",
    "                            sev, match = \"MISSING\", False\n",
    "                    except InvalidVersion:\n",
    "                        pass\n",
    "                elif cur_ver is None:\n",
    "                    sev, match = \"MISSING\", False\n",
    "\n",
    "                audit[pkg_lc] = {\n",
    "                    \"required\": req_ver,\n",
    "                    \"current\": cur_ver,\n",
    "                    \"match\": match,\n",
    "                    \"severity\": sev,\n",
    "                }\n",
    "\n",
    "            # 4ï¸âƒ£ Record audit in service status\n",
    "            self.status[f\"{model_name}_dep_audit\"] = audit\n",
    "\n",
    "            # 5ï¸âƒ£ Only show pipâ€‘install hint if there *are* mismatches\n",
    "            if deps_path and any(not rec[\"match\"] for rec in audit.values()):\n",
    "                pyfunc_logger.info(\n",
    "                    \"To install the dependencies that were used to train the model, \"\n",
    "                    \"run the following command: 'pip install -r %s'\",\n",
    "                    deps_path,\n",
    "                )\n",
    "\n",
    "            # 6ï¸âƒ£ MLOps policy enforcement based on environment\n",
    "            if policy in (\"fail\", \"retrain\"):\n",
    "                critical = (\"numpy\", \"scipy\", \"scikit-learn\", \"psutil\")\n",
    "                majors = [\n",
    "                    pkg\n",
    "                    for pkg, rec in audit.items()\n",
    "                    if pkg in critical and rec[\"severity\"] == \"MAJOR_DRT\"\n",
    "                ]\n",
    "                if majors:\n",
    "                    msg = f\"Critical env drift for {model_name}: {majors}\"\n",
    "                    logger.error(msg)\n",
    "                    if policy == \"fail\":\n",
    "                        self.status[f\"{model_name}_last_error\"] = msg\n",
    "                        return {\"_REFUSE_LOAD\": True}\n",
    "                    elif policy == \"retrain\":\n",
    "                        logger.warning(\n",
    "                            \"Scheduling background retrain for %s due to env drift\",\n",
    "                            model_name,\n",
    "                        )\n",
    "                        asyncio.create_task(\n",
    "                            self._train_and_reload(model_name, TRAINERS[model_name])\n",
    "                        )\n",
    "\n",
    "            return audit\n",
    "\n",
    "        client = self.mlflow_client\n",
    "\n",
    "        # MLOps-aware loading order based on environment\n",
    "        env_canon = settings.ENVIRONMENT_CANONICAL\n",
    "        if env_canon == \"production\":\n",
    "            # Production: strict order - only Production stage or @prod alias\n",
    "            attempts = [\n",
    "                (\"@prod\", f\"models:/{name}@prod\"),\n",
    "                (\"Production stage\", None),  # handle below\n",
    "            ]\n",
    "        elif env_canon == \"staging\":\n",
    "            # Staging: allow staging versions for testing\n",
    "            attempts = [\n",
    "                (\"@staging\", f\"models:/{name}@staging\"),\n",
    "                (\"@prod\", f\"models:/{name}@prod\"),\n",
    "                (\"Production stage\", None),  # handle below\n",
    "            ]\n",
    "        else:\n",
    "            # Development: most permissive\n",
    "            attempts = [\n",
    "                (\"@prod\", f\"models:/{name}@prod\"),\n",
    "                (\"@staging\", f\"models:/{name}@staging\"),\n",
    "                (\"Production stage\", None),  # handle below\n",
    "                (\"latest run\", None),        # handle below\n",
    "            ]\n",
    "\n",
    "        # 1ï¸âƒ£ Try aliases first ------------------------------------------------------\n",
    "        for alias_name, uri in attempts[:2]:  # Only try aliases\n",
    "            if uri is None:\n",
    "                continue\n",
    "            try:\n",
    "                _warn_model_env(uri)\n",
    "                logger.info(\"â†ªï¸Ž  Loading %s from alias %s\", name, alias_name)\n",
    "                mdl = mlflow.pyfunc.load_model(uri)\n",
    "                audit = _audit_model_env(uri, name)\n",
    "                if audit.get(\"_REFUSE_LOAD\"):\n",
    "                    logger.warning(\"Refusing %s from %s under policy; continuing fallbacks\", name, alias_name)\n",
    "                else:\n",
    "                    # --- config hash drift check -------------------------------------------------\n",
    "                    try:\n",
    "                        # Candidate may be run-based or version-based; we try to extract run_id param from MLflow model flavor metadata.\n",
    "                        # Fallback: skip silently.\n",
    "                        from mlflow import get_tracking_uri\n",
    "                        tracking_client = self.mlflow_client\n",
    "                        # try to read run params if we have run context\n",
    "                        # NOTE: _download_artifact_from_uri gave us `uri`; if it's a runs:/ URI we can parse run_id\n",
    "                        if uri.startswith(\"runs:/\"):\n",
    "                            run_id = uri.split(\"/\", 2)[1]\n",
    "                            run = tracking_client.get_run(run_id)\n",
    "                            train_hash = run.data.params.get(\"train_config_hash\")\n",
    "                            if train_hash:\n",
    "                                from app.core.config import settings as _s\n",
    "                                cur_hash = hashlib.sha256(json.dumps(_s.model_dump(), sort_keys=True).encode()).hexdigest()\n",
    "                                if train_hash != cur_hash:\n",
    "                                    logger.warning(\n",
    "                                        \"âš ï¸ Config drift for %s: train_hash=%s current=%s\",\n",
    "                                        model_name, train_hash[:8], cur_hash[:8]\n",
    "                                    )\n",
    "                    except Exception as _e_hash:  # best-effort\n",
    "                        logger.debug(\"Config drift check skipped: %s\", _e_hash)\n",
    "                    return mdl\n",
    "            except Exception as e:\n",
    "                logger.debug(\"Alias %s not available for %s: %s\", alias_name, name, e)\n",
    "\n",
    "        # 2ï¸âƒ£ Try Production stage ---------------------------------------------------\n",
    "        try:\n",
    "            versions = client.search_model_versions(f\"name='{name}' AND stage='Production'\")\n",
    "            if versions:\n",
    "                version = versions[0].version\n",
    "                uri = f\"models:/{name}/{version}\"\n",
    "                _warn_model_env(uri)\n",
    "                logger.info(\"â†ªï¸Ž  Loading %s from registry: Production v%s\", name, version)\n",
    "                mdl = mlflow.pyfunc.load_model(uri)\n",
    "                audit = _audit_model_env(uri, name)\n",
    "                if audit.get(\"_REFUSE_LOAD\"):\n",
    "                    logger.warning(\"Refusing %s Production v%s under policy; continuing fallbacks\", name, version)\n",
    "                else:\n",
    "                    return mdl\n",
    "        except Exception as e:\n",
    "            logger.debug(\"Production stage not available for %s: %s\", name, e)\n",
    "\n",
    "        # 3ï¸âƒ£ Try Staging stage (only in dev/staging) ------------------------------\n",
    "        if settings.ENVIRONMENT != \"production\":\n",
    "            try:\n",
    "                versions = client.search_model_versions(f\"name='{name}' AND stage='Staging'\")\n",
    "                if versions:\n",
    "                    version = versions[0].version\n",
    "                    uri = f\"models:/{name}/{version}\"\n",
    "                    _warn_model_env(uri)\n",
    "                    logger.info(\"â†ªï¸Ž  Loading %s from registry: Staging v%s\", name, version)\n",
    "                    mdl = mlflow.pyfunc.load_model(uri)\n",
    "                    audit = _audit_model_env(uri, name)\n",
    "                    if audit.get(\"_REFUSE_LOAD\"):\n",
    "                        logger.warning(\"Refusing %s Staging v%s under policy; continuing fallbacks\", name, version)\n",
    "                    else:\n",
    "                        return mdl\n",
    "            except Exception as e:\n",
    "                logger.debug(\"Staging stage not available for %s: %s\", name, e)\n",
    "\n",
    "        # 4ï¸âƒ£  (Possible) Fallback to latest run â€“ now allowed in prod too\n",
    "        allow_run_fallback = (\n",
    "            settings.ENVIRONMENT_CANONICAL != \"production\"\n",
    "            or settings.ALLOW_PROD_RUN_FALLBACK\n",
    "        )\n",
    "        if allow_run_fallback:\n",
    "            try:\n",
    "                runs = []\n",
    "                for exp in client.search_experiments():\n",
    "                    runs.extend(client.search_runs(\n",
    "                        [exp.experiment_id],\n",
    "                        f\"tags.mlflow.runName = '{name}'\",\n",
    "                        order_by=[\"attributes.start_time DESC\"],\n",
    "                        max_results=1))\n",
    "                if runs:\n",
    "                    uri = f\"runs:/{runs[0].info.run_id}/model\"\n",
    "                    logger.warning(\n",
    "                        \"âš ï¸  %s: alias/stage missing â€“ loading *latest run* (%s) \"\n",
    "                        \"because ALLOW_PROD_RUN_FALLBACK=%d\",\n",
    "                        name, runs[0].info.run_id, allow_run_fallback,\n",
    "                    )\n",
    "                    _warn_model_env(uri)\n",
    "                    mdl = mlflow.pyfunc.load_model(uri)\n",
    "                    audit = _audit_model_env(uri, name)\n",
    "                    if audit.get(\"_REFUSE_LOAD\"):\n",
    "                        logger.warning(\"Refusing %s latest run under policy\", name)\n",
    "                    else:\n",
    "                        return mdl\n",
    "            except Exception as e:\n",
    "                logger.debug(\"Latestâ€‘run fallback failed for %s: %s\", name, e)\n",
    "\n",
    "        logger.error(\"âŒ No suitable model found for %s after all fallbacks\", name)\n",
    "        return None\n",
    "# --- END DROP-IN REPLACEMENT -------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "    async def evaluate_model_quality(\n",
    "        self, \n",
    "        model_name: str, \n",
    "        candidate_run_id: str,\n",
    "        test_data_path: Optional[str] = None\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Evaluate a candidate model against production baseline.\n",
    "\n",
    "        This implements quality gates for MLOps:\n",
    "        1. Load production model (if exists)\n",
    "        2. Load candidate model from run_id\n",
    "        3. Evaluate both on test set\n",
    "        4. Return comparison metrics\n",
    "\n",
    "        Args:\n",
    "            model_name: Name of the model to evaluate\n",
    "            candidate_run_id: MLflow run ID of candidate model\n",
    "            test_data_path: Optional path to test data (uses built-in if None)\n",
    "\n",
    "        Returns:\n",
    "            Dict with evaluation results and promotion decision\n",
    "        \"\"\"\n",
    "        from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "        import pandas as pd\n",
    "\n",
    "        logger.info(\"ðŸ” Evaluating quality gate for %s (candidate: %s)\", model_name, candidate_run_id)\n",
    "\n",
    "        # Load test data\n",
    "        if model_name.startswith(\"iris\"):\n",
    "            from sklearn.datasets import load_iris\n",
    "            from sklearn.model_selection import train_test_split\n",
    "\n",
    "            iris = load_iris(as_frame=True)\n",
    "            X, y = iris.data, iris.target\n",
    "            _, X_test, _, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)\n",
    "\n",
    "        elif model_name.startswith(\"breast_cancer\"):\n",
    "            from sklearn.datasets import load_breast_cancer\n",
    "            from sklearn.model_selection import train_test_split\n",
    "\n",
    "            X, y = load_breast_cancer(return_X_y=True, as_frame=True)\n",
    "            _, X_test, _, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "            X_test = _rename_cancer_columns(X_test)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model type: {model_name}\")\n",
    "\n",
    "        # Load candidate model\n",
    "        try:\n",
    "            candidate_uri = f\"runs:/{candidate_run_id}/model\"\n",
    "            candidate_model = mlflow.pyfunc.load_model(candidate_uri)\n",
    "            logger.info(\"âœ… Loaded candidate model from %s\", candidate_run_id)\n",
    "        except Exception as e:\n",
    "            logger.error(\"âŒ Failed to load candidate model: %s\", e)\n",
    "            return {\n",
    "                \"promoted\": False,\n",
    "                \"error\": f\"Failed to load candidate model: {e}\",\n",
    "                \"candidate_metrics\": None,\n",
    "                \"production_metrics\": None\n",
    "            }\n",
    "\n",
    "        # Evaluate candidate\n",
    "        try:\n",
    "            if model_name.startswith(\"iris\"):\n",
    "                y_pred = candidate_model.predict(X_test)\n",
    "                if len(y_pred.shape) == 2:  # probabilities\n",
    "                    y_pred = y_pred.argmax(axis=1)\n",
    "            else:  # cancer model\n",
    "                y_pred_proba = candidate_model.predict(X_test)\n",
    "                y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "            candidate_metrics = {\n",
    "                \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "                \"f1_macro\": f1_score(y_test, y_pred, average=\"macro\"),\n",
    "                \"precision_macro\": precision_score(y_test, y_pred, average=\"macro\"),\n",
    "                \"recall_macro\": recall_score(y_test, y_pred, average=\"macro\")\n",
    "            }\n",
    "            logger.info(\"ðŸ“Š Candidate metrics: %s\", candidate_metrics)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(\"âŒ Failed to evaluate candidate: %s\", e)\n",
    "            return {\n",
    "                \"promoted\": False,\n",
    "                \"error\": f\"Failed to evaluate candidate: {e}\",\n",
    "                \"candidate_metrics\": None,\n",
    "                \"production_metrics\": None\n",
    "            }\n",
    "\n",
    "        # Try to load production model for comparison\n",
    "        production_metrics = None\n",
    "        try:\n",
    "            prod_model = await self._load_production_model(model_name)\n",
    "            if prod_model:\n",
    "                if model_name.startswith(\"iris\"):\n",
    "                    y_pred_prod = prod_model.predict(X_test)\n",
    "                    if len(y_pred_prod.shape) == 2:  # probabilities\n",
    "                        y_pred_prod = y_pred_prod.argmax(axis=1)\n",
    "                else:  # cancer model\n",
    "                    y_pred_proba_prod = prod_model.predict(X_test)\n",
    "                    y_pred_prod = (y_pred_proba_prod > 0.5).astype(int)\n",
    "\n",
    "                production_metrics = {\n",
    "                    \"accuracy\": accuracy_score(y_test, y_pred_prod),\n",
    "                    \"f1_macro\": f1_score(y_test, y_pred_prod, average=\"macro\"),\n",
    "                    \"precision_macro\": precision_score(y_test, y_pred_prod, average=\"macro\"),\n",
    "                    \"recall_macro\": recall_score(y_test, y_pred_prod, average=\"macro\")\n",
    "                }\n",
    "                logger.info(\"ðŸ“Š Production metrics: %s\", production_metrics)\n",
    "            else:\n",
    "                logger.info(\"ðŸ“Š No production model found for comparison\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(\"âš ï¸  Could not evaluate production model: %s\", e)\n",
    "\n",
    "                # Quality gate decision\n",
    "        promoted = False\n",
    "        reason = \"\"\n",
    "\n",
    "        if production_metrics:\n",
    "            # Compare against production baseline\n",
    "            acc_improvement = candidate_metrics[\"accuracy\"] - production_metrics[\"accuracy\"]\n",
    "            f1_improvement = candidate_metrics[\"f1_macro\"] - production_metrics[\"f1_macro\"]\n",
    "\n",
    "            # Quality gate: must maintain or improve performance\n",
    "            if acc_improvement >= -0.01 and f1_improvement >= -0.01:  # Allow 1% degradation\n",
    "                promoted = True\n",
    "                reason = f\"Performance maintained (acc: {acc_improvement:+.3f}, f1: {f1_improvement:+.3f})\"\n",
    "            else:\n",
    "                reason = f\"Performance degraded (acc: {acc_improvement:+.3f}, f1: {f1_improvement:+.3f})\"\n",
    "        else:\n",
    "            # No production baseline - use absolute thresholds from settings\n",
    "            if candidate_metrics[\"accuracy\"] >= settings.QUALITY_GATE_ACCURACY_THRESHOLD \\\n",
    "               and candidate_metrics[\"f1_macro\"] >= settings.QUALITY_GATE_F1_THRESHOLD:\n",
    "                promoted = True\n",
    "                reason = f\"Meets minimum thresholds (acc: {candidate_metrics['accuracy']:.3f} >= {settings.QUALITY_GATE_ACCURACY_THRESHOLD}, f1: {candidate_metrics['f1_macro']:.3f} >= {settings.QUALITY_GATE_F1_THRESHOLD})\"\n",
    "            else:\n",
    "                reason = f\"Below minimum thresholds (acc: {candidate_metrics['accuracy']:.3f} < {settings.QUALITY_GATE_ACCURACY_THRESHOLD} or f1: {candidate_metrics['f1_macro']:.3f} < {settings.QUALITY_GATE_F1_THRESHOLD})\"\n",
    "\n",
    "        result = {\n",
    "            \"promoted\": promoted,\n",
    "            \"reason\": reason,\n",
    "            \"candidate_metrics\": candidate_metrics,\n",
    "            \"production_metrics\": production_metrics,\n",
    "            \"candidate_run_id\": candidate_run_id,\n",
    "            \"model_name\": model_name\n",
    "        }\n",
    "\n",
    "        # log evaluation metadata back to MLflow\n",
    "        try:\n",
    "            client = self.mlflow_client\n",
    "            client.set_tag(candidate_run_id, f\"quality_gate:{settings.ENVIRONMENT_CANONICAL}\",\n",
    "                           \"PASSED\" if promoted else \"FAILED\")\n",
    "            client.set_tag(candidate_run_id, \"quality_gate_reason\", reason)\n",
    "        except Exception as e:\n",
    "            logger.debug(\"Failed to set MLflow quality gate tags: %s\", e)\n",
    "\n",
    "        logger.info(\"ðŸŽ¯ Quality gate result: %s - %s\", \"PASSED\" if promoted else \"FAILED\", reason)\n",
    "        return result\n",
    "\n",
    "    async def promote_model_to_staging(\n",
    "        self, \n",
    "        model_name: str, \n",
    "        run_id: str\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Promote a model to staging after quality gate passes.\n",
    "\n",
    "        This is the core MLOps promotion logic:\n",
    "        1. Evaluate model quality\n",
    "        2. If passed, register as staging version\n",
    "        3. Set @staging alias\n",
    "\n",
    "        Args:\n",
    "            model_name: Name of the model to promote\n",
    "            run_id: MLflow run ID of the candidate model\n",
    "\n",
    "        Returns:\n",
    "            Dict with promotion result\n",
    "        \"\"\"\n",
    "        logger.info(\"ðŸš€ Starting promotion process for %s (run: %s)\", model_name, run_id)\n",
    "\n",
    "        # Evaluate quality gate\n",
    "        eval_result = await self.evaluate_model_quality(model_name, run_id)\n",
    "\n",
    "        if not eval_result[\"promoted\"]:\n",
    "            logger.warning(\"âŒ Quality gate failed for %s: %s\", model_name, eval_result.get(\"reason\", \"Unknown\"))\n",
    "            return {\n",
    "                \"promoted\": False,\n",
    "                \"error\": eval_result.get(\"error\", eval_result.get(\"reason\", \"Quality gate failed\")),\n",
    "                \"evaluation\": eval_result\n",
    "            }\n",
    "\n",
    "        # Promote to staging\n",
    "        try:\n",
    "            client = self.mlflow_client\n",
    "            candidate_uri = f\"runs:/{run_id}/model\"\n",
    "\n",
    "            # Create new model version\n",
    "            mv = client.create_model_version(\n",
    "                name=model_name,\n",
    "                source=candidate_uri,\n",
    "                run_id=run_id\n",
    "            )\n",
    "\n",
    "            # Transition to Staging\n",
    "            client.transition_model_version_stage(\n",
    "                name=model_name,\n",
    "                version=mv.version,\n",
    "                stage=\"Staging\"\n",
    "            )\n",
    "\n",
    "            # Set @staging alias\n",
    "            client.set_registered_model_alias(\n",
    "                name=model_name,\n",
    "                alias=\"staging\",\n",
    "                version=mv.version\n",
    "            )\n",
    "\n",
    "            logger.info(\"âœ… Successfully promoted %s to staging (version %s)\", model_name, mv.version)\n",
    "\n",
    "            return {\n",
    "                \"promoted\": True,\n",
    "                \"version\": mv.version,\n",
    "                \"stage\": \"Staging\",\n",
    "                \"alias\": \"staging\",\n",
    "                \"evaluation\": eval_result\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            logger.error(\"âŒ Failed to promote %s to staging: %s\", model_name, error_msg)\n",
    "            return {\n",
    "                \"promoted\": False,\n",
    "                \"error\": f\"Promotion failed: {error_msg}\",\n",
    "                \"evaluation\": eval_result\n",
    "            }\n",
    "\n",
    "    async def promote_model_to_production(\n",
    "        self, \n",
    "        model_name: str,\n",
    "        version: Optional[int] = None,\n",
    "        approved_by: Optional[str] = None\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Promote a staging model to production.\n",
    "\n",
    "        This can be called manually or automatically:\n",
    "        1. If version specified, promote that specific version\n",
    "        2. Otherwise, promote the current @staging alias\n",
    "        3. Set @prod alias for atomic promotion\n",
    "\n",
    "        Args:\n",
    "            model_name: Name of the model to promote\n",
    "            version: Specific version to promote (optional)\n",
    "\n",
    "        Returns:\n",
    "            Dict with promotion result\n",
    "        \"\"\"\n",
    "        logger.info(\"ðŸš€ Promoting %s to production (version: %s)\", model_name, version or \"staging\")\n",
    "\n",
    "        # Enforce human approval in production if required\n",
    "        if settings.REQUIRE_MODEL_APPROVAL and settings.ENVIRONMENT_CANONICAL == \"production\":\n",
    "            if not approved_by:\n",
    "                return {\n",
    "                    \"promoted\": False,\n",
    "                    \"error\": \"Approval required: pass approved_by=<user> when promoting to production.\",\n",
    "                }\n",
    "\n",
    "        try:\n",
    "            client = self.mlflow_client\n",
    "\n",
    "            if version is None:\n",
    "                # Get the current staging version\n",
    "                staging_versions = client.search_model_versions(\n",
    "                    f\"name='{model_name}' AND stage='Staging'\"\n",
    "                )\n",
    "                if not staging_versions:\n",
    "                    return {\n",
    "                        \"promoted\": False,\n",
    "                        \"error\": f\"No staging version found for {model_name}\"\n",
    "                    }\n",
    "                version = staging_versions[0].version\n",
    "\n",
    "            # Transition to Production\n",
    "            client.transition_model_version_stage(\n",
    "                name=model_name,\n",
    "                version=version,\n",
    "                stage=\"Production\"\n",
    "            )\n",
    "\n",
    "            # Set @prod alias for atomic promotion\n",
    "            client.set_registered_model_alias(\n",
    "                name=model_name,\n",
    "                alias=\"prod\",\n",
    "                version=version\n",
    "            )\n",
    "\n",
    "            # record approval metadata as tags\n",
    "            try:\n",
    "                client.set_model_version_tag(model_name, version, \"approved_by\", str(approved_by or \"n/a\"))\n",
    "                client.set_model_version_tag(model_name, version, \"approved_env\", str(settings.ENVIRONMENT_CANONICAL))\n",
    "            except Exception as e:\n",
    "                logger.debug(\"Could not tag model version approval: %s\", e)\n",
    "\n",
    "            logger.info(\"âœ… Successfully promoted %s to production (version %s)\", model_name, version)\n",
    "\n",
    "            return {\n",
    "                \"promoted\": True,\n",
    "                \"version\": version,\n",
    "                \"stage\": \"Production\",\n",
    "                \"alias\": \"prod\"\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            logger.error(\"âŒ Failed to promote %s to production: %s\", model_name, error_msg)\n",
    "            return {\n",
    "                \"promoted\": False,\n",
    "                \"error\": f\"Production promotion failed: {error_msg}\"\n",
    "            }\n",
    "\n",
    "    async def promote_model_to_stage(\n",
    "        self, \n",
    "        model_name: str,\n",
    "        target_stage: str,\n",
    "        version: Optional[int] = None,\n",
    "        approved_by: Optional[str] = None\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Promote a model to a specific stage (staging or production).\n",
    "\n",
    "        Args:\n",
    "            model_name: Name of the model to promote\n",
    "            target_stage: Target stage ('Staging' or 'Production')\n",
    "            version: Specific version to promote (optional)\n",
    "            approved_by: User who approved the promotion (optional)\n",
    "\n",
    "        Returns:\n",
    "            Dict with promotion result\n",
    "        \"\"\"\n",
    "        logger.info(\"ðŸš€ Promoting %s to %s (version: %s)\", model_name, target_stage, version or \"latest\")\n",
    "\n",
    "        # Validate target stage\n",
    "        if target_stage not in [\"Staging\", \"Production\"]:\n",
    "            return {\n",
    "                \"promoted\": False,\n",
    "                \"error\": f\"Invalid target stage: {target_stage}. Must be 'Staging' or 'Production'\"\n",
    "            }\n",
    "\n",
    "        # Enforce human approval in production if required\n",
    "        if target_stage == \"Production\" and settings.REQUIRE_MODEL_APPROVAL and settings.ENVIRONMENT_CANONICAL == \"production\":\n",
    "            if not approved_by:\n",
    "                return {\n",
    "                    \"promoted\": False,\n",
    "                    \"error\": \"Approval required: pass approved_by=<user> when promoting to production.\",\n",
    "                }\n",
    "\n",
    "        try:\n",
    "            client = self.mlflow_client\n",
    "\n",
    "            if version is None:\n",
    "                # Get the latest version\n",
    "                versions = client.search_model_versions(f\"name='{model_name}'\")\n",
    "                if not versions:\n",
    "                    return {\n",
    "                        \"promoted\": False,\n",
    "                        \"error\": f\"No versions found for {model_name}\"\n",
    "                    }\n",
    "                version = versions[0].version\n",
    "\n",
    "            # Set appropriate alias (modern approach - skip deprecated stage transitions)\n",
    "            alias = \"prod\" if target_stage == \"Production\" else \"staging\"\n",
    "            client.set_registered_model_alias(\n",
    "                name=model_name,\n",
    "                alias=alias,\n",
    "                version=version\n",
    "            )\n",
    "\n",
    "            # record approval metadata as tags\n",
    "            try:\n",
    "                client.set_model_version_tag(model_name, version, \"approved_by\", approved_by or \"n/a\")\n",
    "                client.set_model_version_tag(model_name, version, \"approved_env\", settings.ENVIRONMENT_CANONICAL)\n",
    "            except Exception as e:\n",
    "                logger.debug(\"Could not tag model version approval: %s\", e)\n",
    "\n",
    "            logger.info(\"âœ… Successfully promoted %s to %s (version %s)\", model_name, target_stage, version)\n",
    "\n",
    "            return {\n",
    "                \"promoted\": True,\n",
    "                \"version\": version,\n",
    "                \"stage\": target_stage,\n",
    "                \"alias\": alias\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            logger.error(\"âŒ Failed to promote %s to %s: %s\", model_name, target_stage, error_msg)\n",
    "            return {\n",
    "                \"promoted\": False,\n",
    "                \"error\": f\"{target_stage} promotion failed: {error_msg}\"\n",
    "            }\n",
    "\n",
    "    async def get_model_metrics(self, model_name: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve metrics for all versions of a registered model.\n",
    "\n",
    "        This enables MLOps comparison between different model versions\n",
    "        for quality gate decisions and promotion workflows.\n",
    "\n",
    "        Args:\n",
    "            model_name: Name of the registered model\n",
    "\n",
    "        Returns:\n",
    "            List of dicts with version info and metrics for each model version\n",
    "        \"\"\"\n",
    "        client = self.mlflow_client\n",
    "        versions = client.search_model_versions(f\"name='{model_name}'\")\n",
    "        results = []\n",
    "\n",
    "        for v in versions:\n",
    "            run_id = v.run_id\n",
    "            try:\n",
    "                run = client.get_run(run_id)\n",
    "                # Convert MLflow Metric objects to plain Python values\n",
    "                metrics = {}\n",
    "                for key, metric in run.data.metrics.items():\n",
    "                    metrics[key] = float(metric.value) if hasattr(metric, 'value') else float(metric)\n",
    "                # Add metadata for better MLOps context\n",
    "                tags = run.data.tags\n",
    "                creation_timestamp = v.creation_timestamp\n",
    "                last_updated_timestamp = v.last_updated_timestamp\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not fetch metrics for {model_name} v{v.version}: {e}\")\n",
    "                metrics = {}\n",
    "                tags = {}\n",
    "                creation_timestamp = None\n",
    "                last_updated_timestamp = None\n",
    "\n",
    "            results.append({\n",
    "                \"version\": int(v.version),\n",
    "                \"stage\": v.current_stage,\n",
    "                \"run_id\": run_id,\n",
    "                \"metrics\": metrics,\n",
    "                \"tags\": tags,\n",
    "                \"creation_timestamp\": creation_timestamp,\n",
    "                \"last_updated_timestamp\": last_updated_timestamp,\n",
    "                \"description\": v.description or \"\"\n",
    "            })\n",
    "\n",
    "        # Sort by version number for consistent ordering\n",
    "        results.sort(key=lambda x: x[\"version\"])\n",
    "        return results\n",
    "\n",
    "    async def compare_model_versions(\n",
    "        self, \n",
    "        model_name: str, \n",
    "        version_a: int, \n",
    "        version_b: int\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Compare two specific model versions for MLOps decision making.\n",
    "\n",
    "        Args:\n",
    "            model_name: Name of the registered model\n",
    "            version_a: First version to compare\n",
    "            version_b: Second version to compare\n",
    "\n",
    "        Returns:\n",
    "            Dict with comparison results and recommendation\n",
    "        \"\"\"\n",
    "        client = self.mlflow_client\n",
    "\n",
    "        # Get both versions\n",
    "        try:\n",
    "            version_a_info = client.get_model_version(model_name, version_a)\n",
    "            version_b_info = client.get_model_version(model_name, version_b)\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"error\": f\"Could not fetch model versions: {e}\",\n",
    "                \"comparison\": None\n",
    "            }\n",
    "\n",
    "        # Get metrics for both versions\n",
    "        metrics_a = await self._get_version_metrics(version_a_info.run_id)\n",
    "        metrics_b = await self._get_version_metrics(version_b_info.run_id)\n",
    "\n",
    "        # Compare key metrics\n",
    "        comparison = {}\n",
    "        for metric in [\"accuracy\", \"f1_macro\", \"precision_macro\", \"recall_macro\"]:\n",
    "            if metric in metrics_a and metric in metrics_b:\n",
    "                val_a = metrics_a[metric]\n",
    "                val_b = metrics_b[metric]\n",
    "                diff = val_b - val_a\n",
    "                comparison[metric] = {\n",
    "                    \"version_a\": val_a,\n",
    "                    \"version_b\": val_b,\n",
    "                    \"difference\": diff,\n",
    "                    \"improvement\": diff > 0\n",
    "                }\n",
    "\n",
    "        # Determine recommendation\n",
    "        improvements = sum(1 for comp in comparison.values() if comp[\"improvement\"])\n",
    "        total_metrics = len(comparison)\n",
    "\n",
    "        if total_metrics == 0:\n",
    "            recommendation = \"insufficient_data\"\n",
    "        elif improvements == total_metrics:\n",
    "            recommendation = \"promote_version_b\"\n",
    "        elif improvements == 0:\n",
    "            recommendation = \"keep_version_a\"\n",
    "        else:\n",
    "            recommendation = \"mixed_results\"\n",
    "\n",
    "        return {\n",
    "            \"model_name\": model_name,\n",
    "            \"version_a\": {\n",
    "                \"version\": version_a,\n",
    "                \"stage\": version_a_info.current_stage,\n",
    "                \"run_id\": version_a_info.run_id,\n",
    "                \"metrics\": metrics_a\n",
    "            },\n",
    "            \"version_b\": {\n",
    "                \"version\": version_b,\n",
    "                \"stage\": version_b_info.current_stage,\n",
    "                \"run_id\": version_b_info.run_id,\n",
    "                \"metrics\": metrics_b\n",
    "            },\n",
    "            \"comparison\": comparison,\n",
    "            \"recommendation\": recommendation,\n",
    "            \"summary\": f\"Version B improves {improvements}/{total_metrics} metrics\"\n",
    "        }\n",
    "\n",
    "    async def _get_version_metrics(self, run_id: str) -> Dict[str, float]:\n",
    "        \"\"\"Helper to get metrics for a specific run.\"\"\"\n",
    "        try:\n",
    "            run = self.mlflow_client.get_run(run_id)\n",
    "            # Convert MLflow Metric objects to plain Python values\n",
    "            metrics = {}\n",
    "            for key, metric in run.data.metrics.items():\n",
    "                metrics[key] = float(metric.value) if hasattr(metric, 'value') else float(metric)\n",
    "            return metrics\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not fetch metrics for run {run_id}: {e}\")\n",
    "            return {}\n",
    "\n",
    "    # Manual training endpoints (for UI)\n",
    "    async def train_iris(self, model_type: str = \"rf\") -> None:\n",
    "        \"\"\"\n",
    "        Train either the Random Forest or the Logistic Regression\n",
    "        on the Iris dataset, per the caller's choice.\n",
    "        \"\"\"\n",
    "        if model_type == \"rf\":\n",
    "            name, trainer = \"iris_random_forest\", TRAINERS[\"iris_random_forest\"]\n",
    "        else:  # \"logreg\"\n",
    "            name, trainer = \"iris_logreg\", TRAINERS[\"iris_logreg\"]\n",
    "\n",
    "        # reuse your existing helper\n",
    "        await self._train_and_reload(name, trainer)\n",
    "\n",
    "    async def train_cancer(self, model_type: str = \"bayes\") -> None:\n",
    "        \"\"\"\n",
    "        Train either the Bayesian (PyMC) or stub (LogReg)\n",
    "        on the Breast Cancer dataset, per the caller's choice.\n",
    "        \"\"\"\n",
    "        if model_type == \"bayes\":\n",
    "            name, trainer = \"breast_cancer_bayes\", TRAINERS[\"breast_cancer_bayes\"]\n",
    "        else:  # \"stub\"\n",
    "            name, trainer = \"breast_cancer_stub\", TRAINERS[\"breast_cancer_stub\"]\n",
    "\n",
    "        await self._train_and_reload(name, trainer)\n",
    "\n",
    "    async def train_bayes_cancer_with_params(self, params=None) -> str:\n",
    "        \"\"\"\n",
    "        Train Bayesian cancer model with validated parameters.\n",
    "        Returns the MLflow run ID.\n",
    "        \"\"\"\n",
    "        from app.ml.builtin_trainers import train_breast_cancer_bayes\n",
    "        \n",
    "        # Run training with parameters\n",
    "        run_id = train_breast_cancer_bayes(params_obj=params)\n",
    "        \n",
    "        # Reload the model after training\n",
    "        await self._try_load(\"breast_cancer_bayes\")\n",
    "        \n",
    "        return run_id\n",
    "\n",
    "    # Predict methods (unchanged from your previous version)\n",
    "    async def predict_iris(\n",
    "        self,\n",
    "        features: List[Dict[str, float]],\n",
    "        model_type: str = \"rf\",\n",
    "    ) -> Tuple[List[str], List[List[float]]]:\n",
    "        \"\"\"\n",
    "        Predict Iris species from measurements.\n",
    "\n",
    "        Hardens input normalization & always uses a serial, psutilâ€‘safe path\n",
    "        to compute class probabilities to avoid joblib/loky crashes when\n",
    "        psutil is broken. Also ensures feature names are preserved to silence\n",
    "        scikitâ€‘learn's 'X does not have valid feature names' warning. :contentReference[oaicite:23]{index=23}\n",
    "        \"\"\"\n",
    "        if model_type not in (\"rf\", \"logreg\"):\n",
    "            raise ValueError(\"model_type must be 'rf' or 'logreg'\")\n",
    "\n",
    "        model_name = \"iris_random_forest\" if model_type == \"rf\" else \"iris_logreg\"\n",
    "        model = self.models.get(model_name)\n",
    "        if not model:\n",
    "            raise RuntimeError(f\"{model_name} not loaded\")\n",
    "\n",
    "        # construct DF w/ training column names in correct order\n",
    "        X_df = pd.DataFrame(\n",
    "            [{\n",
    "                \"sepal length (cm)\":  f[\"sepal_length\"],\n",
    "                \"sepal width (cm)\":   f[\"sepal_width\"],\n",
    "                \"petal length (cm)\":  f[\"petal_length\"],\n",
    "                \"petal width (cm)\":   f[\"petal_width\"],\n",
    "            } for f in features]\n",
    "        )\n",
    "        logger.debug(\"predict_iris(%s) columns=%s\", model_name, X_df.columns.tolist())\n",
    "\n",
    "        # ALWAYS unwrap and call safe helper (skip top-level pyfunc)\n",
    "        base = model\n",
    "        try:\n",
    "            py_model = model.unwrap_python_model()  # mlflow â‰¥2\n",
    "            if hasattr(py_model, \"model\"):\n",
    "                base = py_model.model\n",
    "            else:\n",
    "                base = py_model\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        probs = _safe_sklearn_proba(base, X_df, log_prefix=model_name)\n",
    "\n",
    "        # convert to names\n",
    "        import numpy as _np\n",
    "        probs = _np.asarray(probs, dtype=float)\n",
    "        if probs.ndim == 1:  # defensive\n",
    "            # promote to 3-class; treat as class-0 vs rest\n",
    "            z = _np.zeros((probs.size, 3), dtype=float)\n",
    "            z[:, 0] = probs\n",
    "            z[:, 1:] = (1 - probs) / 2\n",
    "            probs = z\n",
    "        preds = probs.argmax(axis=1)\n",
    "        class_names = [\"setosa\", \"versicolor\", \"virginica\"]\n",
    "        pred_names = [class_names[int(i)] for i in preds]\n",
    "        return pred_names, probs.tolist()\n",
    "\n",
    "\n",
    "    async def predict_cancer(\n",
    "        self,\n",
    "        features: List[Dict[str, float]],\n",
    "        model_type: str = \"bayes\",\n",
    "        posterior_samples: Optional[int] = None,\n",
    "    ) -> Tuple[List[str], List[float], Optional[List[Tuple[float, float]]]]:\n",
    "        \"\"\"\n",
    "        Predict breast cancer diagnosis.\n",
    "\n",
    "        For stub (sklearn) path we unwrap & call psutilâ€‘safe helper to avoid\n",
    "        loky/psutil crashes; for bayes path we call model.predict() directly.\n",
    "        MLflow PythonModel wrappers now expose modern signature. :contentReference[oaicite:24]{index=24}\n",
    "        \"\"\"\n",
    "        if model_type == \"bayes\":\n",
    "            model = self.models.get(\"breast_cancer_bayes\") or self.models.get(\"breast_cancer_stub\")\n",
    "            using_bayes = \"breast_cancer_bayes\" in self.models and model is self.models[\"breast_cancer_bayes\"]\n",
    "        elif model_type == \"stub\":\n",
    "            model = self.models.get(\"breast_cancer_stub\")\n",
    "            using_bayes = False\n",
    "        else:\n",
    "            raise ValueError(\"model_type must be 'bayes' or 'stub'\")\n",
    "        if not model:\n",
    "            raise RuntimeError(\"No cancer model available\")\n",
    "\n",
    "        X_df_raw = pd.DataFrame(features)\n",
    "        X_df = _rename_cancer_columns(X_df_raw)\n",
    "\n",
    "        if using_bayes and hasattr(model, \"predict\"):\n",
    "            probs = model.predict(X_df)\n",
    "        else:\n",
    "            # unwrap & safe path\n",
    "            base = model\n",
    "            try:\n",
    "                py_model = model.unwrap_python_model()\n",
    "                base = getattr(py_model, \"model\", py_model)\n",
    "            except Exception:\n",
    "                pass\n",
    "            probs_full = _safe_sklearn_proba(base, X_df, log_prefix=\"breast_cancer_stub\")\n",
    "            probs = probs_full[:, 1] if probs_full.ndim == 2 else probs_full\n",
    "\n",
    "        labels = [\"malignant\" if p > 0.5 else \"benign\" for p in probs]\n",
    "\n",
    "        ci = None\n",
    "        if posterior_samples and using_bayes:\n",
    "            try:\n",
    "                # Access the underlying python model to get the trace\n",
    "                python_model = model.unwrap_python_model()\n",
    "\n",
    "                # Access posterior samples for uncertainty quantification\n",
    "                draws = python_model.trace.posterior\n",
    "                Î±g = draws[\"Î±\"].stack(samples=(\"chain\", \"draw\"))\n",
    "                Î² = draws[\"Î²\"].stack(samples=(\"chain\", \"draw\"))\n",
    "\n",
    "                # Get group indices and standardized features\n",
    "                g = python_model._quint(X_df)\n",
    "                Xs = python_model.scaler.transform(X_df)\n",
    "\n",
    "                # Compute posterior predictive samples\n",
    "                logits = Î±g.values[:, g] + np.dot(Î².values.T, Xs.T)      # shape (S, N)\n",
    "                pp = 1 / (1 + np.exp(-logits))\n",
    "\n",
    "                # Compute 95% credible intervals\n",
    "                lo, hi = np.percentile(pp, [2.5, 97.5], axis=0)\n",
    "                ci = list(zip(lo.tolist(), hi.tolist()))\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to compute uncertainty intervals: {e}\")\n",
    "                ci = None\n",
    "\n",
    "        return labels, probs.tolist(), ci\n",
    "\n",
    "    async def _cleanup_runs(self, model_name: str) -> None:\n",
    "        \"\"\"\n",
    "        Keep the **newest N runs** for `model_name` and drop the rest, then\n",
    "        optionally invoke `mlflow gc` to purge artifact folders.\n",
    "\n",
    "        Runs marked *deleted* are still present on disk until GC executes,\n",
    "        so we always run GC when `settings.MLFLOW_GC_AFTER_TRAIN` is True.\n",
    "        \"\"\"\n",
    "        keep = max(settings.RETAIN_RUNS_PER_MODEL, 0)\n",
    "        try:\n",
    "            # 1ï¸âƒ£ fetch runs newestâ†’oldest\n",
    "            runs = self.mlflow_client.search_runs(\n",
    "                experiment_ids=[exp.experiment_id for exp in self.mlflow_client.search_experiments()],\n",
    "                filter_string=f\"tags.mlflow.runName = '{model_name}'\",\n",
    "                order_by=[\"attributes.start_time DESC\"],\n",
    "            )\n",
    "            if len(runs) <= keep:\n",
    "                logger.debug(\"No pruning needed for %s (runs=%d, keep=%d)\",\n",
    "                             model_name, len(runs), keep)\n",
    "                return\n",
    "\n",
    "            to_delete = runs[keep:]\n",
    "            for r in to_delete:\n",
    "                self.mlflow_client.delete_run(r.info.run_id)\n",
    "            logger.info(\"ðŸ—‘ï¸  Pruned %d old %s runs; kept %d\",\n",
    "                        len(to_delete), model_name, keep)\n",
    "\n",
    "            # 2ï¸âƒ£ garbageâ€‘collect artifacts\n",
    "            if settings.MLFLOW_GC_AFTER_TRAIN:\n",
    "                uri = mlflow.get_tracking_uri().removeprefix(\"file:\")\n",
    "                before = shutil.disk_usage(uri).used\n",
    "                subprocess.run(\n",
    "                    [\"mlflow\", \"gc\",\n",
    "                     \"--backend-store-uri\", uri,\n",
    "                     \"--artifact-store\", uri],\n",
    "                    check=True,\n",
    "                    stdout=subprocess.PIPE,\n",
    "                    stderr=subprocess.PIPE,\n",
    "                )\n",
    "                after = shutil.disk_usage(uri).used\n",
    "                logger.info(\"ðŸ§¹ mlflow gc completed (%.2f MB â†’ %.2f MB)\",\n",
    "                            before/1e6, after/1e6)\n",
    "\n",
    "        except Exception as exc:\n",
    "            logger.warning(\"Cleanup for %s failed: %s\", model_name, exc)\n",
    "\n",
    "    async def vacuum_store(self) -> None:\n",
    "        \"\"\"Force a *storeâ€‘wide* `mlflow gc` (use from cron jobs).\"\"\"\n",
    "        try:\n",
    "            uri = mlflow.get_tracking_uri().removeprefix(\"file:\")\n",
    "            before = shutil.disk_usage(uri).used\n",
    "            subprocess.run(\n",
    "                [\"mlflow\", \"gc\",\n",
    "                 \"--backend-store-uri\", uri,\n",
    "                 \"--artifact-store\", uri],\n",
    "                check=True,\n",
    "                stdout=subprocess.PIPE,\n",
    "                stderr=subprocess.PIPE,\n",
    "            )\n",
    "            after = shutil.disk_usage(uri).used\n",
    "            logger.info(\"ðŸ§¹ Store-wide vacuum completed (%.2f MB â†’ %.2f MB)\",\n",
    "                        before/1e6, after/1e6)\n",
    "        except Exception as exc:\n",
    "            logger.warning(\"Store vacuum failed: %s\", exc)\n",
    "\n",
    "\n",
    "# Global singleton\n",
    "model_service = ModelService()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d83e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile api/app/main.py\n",
    "import logging\n",
    "import os\n",
    "import asyncio\n",
    "import json\n",
    "from fastapi import FastAPI, Request, Depends, BackgroundTasks, status, HTTPException\n",
    "from fastapi.security import OAuth2PasswordRequestForm\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.responses import JSONResponse\n",
    "from sqlalchemy.ext.asyncio import AsyncSession\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "import time\n",
    "from typing import Optional\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# â”€â”€ NEW: Fix ML backend configuration before any JAX imports â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from .utils.env_sanitizer import fix_ml_backends\n",
    "fix_ml_backends()\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# â”€â”€ NEW: Rate limiting imports â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from fastapi_limiter import FastAPILimiter\n",
    "from redis import asyncio as redis\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# â”€â”€ NEW: Concurrency limiting imports â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from .middleware.concurrency import ConcurrencyLimiter\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "from .db import lifespan, get_db, get_app_ready\n",
    "from .security import create_access_token, get_current_user, verify_password\n",
    "from .crud import get_user_by_username\n",
    "from .schemas.iris import IrisPredictRequest, IrisPredictResponse, IrisFeatures\n",
    "from .schemas.cancer import CancerPredictRequest, CancerPredictResponse, CancerFeatures\n",
    "from .schemas.train import IrisTrainRequest, CancerTrainRequest, BayesTrainRequest, BayesTrainResponse, BayesConfigResponse, BayesRunMetrics\n",
    "from .services.ml.model_service import model_service\n",
    "from .core.config import settings\n",
    "from .deps.limits import default_limit, heavy_limit, login_limit, training_limit, light_limit\n",
    "from .security import LoginPayload, get_credentials\n",
    "\n",
    "# â”€â”€ NEW: guarantee log directory exists â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# â”€â”€ NEW: Redis cache client for prediction caching â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Use the same Redis URL logic as in db.py for consistency\n",
    "if settings.CACHE_ENABLED:\n",
    "    env_url = os.getenv(\"REDIS_URL\")\n",
    "    if settings.ENVIRONMENT_CANONICAL == \"production\" and env_url:\n",
    "        redis_url = env_url\n",
    "    else:\n",
    "        redis_url = settings.REDIS_URL\n",
    "\n",
    "    cache = redis.from_url(\n",
    "        redis_url,\n",
    "        encoding=\"utf-8\",\n",
    "        decode_responses=True,\n",
    "    )\n",
    "    logger.info(\"ðŸ“¦ Prediction caching enabled (Redis %s)\", redis_url)\n",
    "else:\n",
    "    cache = None\n",
    "    logger.info(\"ðŸ“¦ Prediction caching disabled by config\")\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# Pydantic models\n",
    "class Payload(BaseModel):\n",
    "    count: int\n",
    "\n",
    "class PredictionRequest(BaseModel):\n",
    "    data: Payload\n",
    "\n",
    "class PredictionResponse(BaseModel):\n",
    "    prediction: str\n",
    "    confidence: float\n",
    "    input_received: Payload  # Echo back the input for verification\n",
    "\n",
    "class Token(BaseModel):\n",
    "    access_token: str\n",
    "    token_type: str\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"FastAPI + React ML App\",\n",
    "    version=\"1.0.0\",\n",
    "    docs_url=\"/api/v1/docs\",\n",
    "    redoc_url=\"/api/v1/redoc\",\n",
    "    openapi_url=\"/api/v1/openapi.json\",\n",
    "    swagger_ui_parameters={\"persistAuthorization\": True},\n",
    "    lifespan=lifespan,  # register startup/shutdown events\n",
    ")\n",
    "\n",
    "# â”€â”€ Rate limiting is now initialized in lifespan() â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# Configure CORS with environment-based origins\n",
    "origins_env = settings.ALLOWED_ORIGINS\n",
    "origins: list[str] = [o.strip() for o in origins_env.split(\",\")] if origins_env != \"*\" else [\"*\"]\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],  # In production, replace with specific origins\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# â”€â”€ NEW: Add concurrency limiting middleware â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "app.add_middleware(ConcurrencyLimiter, max_concurrent=4)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "@app.middleware(\"http\")\n",
    "async def add_process_time_header(request: Request, call_next):\n",
    "    \"\"\"Measure request time and add X-Process-Time header.\"\"\"\n",
    "    start = time.perf_counter()\n",
    "    response = await call_next(request)\n",
    "    elapsed = time.perf_counter() - start\n",
    "    response.headers[\"X-Process-Time\"] = f\"{elapsed:.4f}\"\n",
    "    return response\n",
    "\n",
    "# Health check endpoint\n",
    "@app.get(\"/api/v1/health\")\n",
    "async def health_check():\n",
    "    \"\"\"Basic health check - always returns 200 if server is running.\"\"\"\n",
    "    return {\"status\": \"healthy\", \"timestamp\": time.time()}\n",
    "\n",
    "@app.get(\"/api/v1/hello\")\n",
    "async def hello(current_user: str = Depends(get_current_user)):\n",
    "    \"\"\"Simple endpoint for token validation.\"\"\"\n",
    "    return {\"message\": f\"Hello {current_user}!\", \"status\": \"authenticated\"}\n",
    "\n",
    "@app.get(\"/api/v1/ready\")\n",
    "async def ready():\n",
    "    \"\"\"Basic readiness check.\"\"\"\n",
    "    return {\"ready\": get_app_ready()}\n",
    "\n",
    "@app.get(\"/api/v1/ready/frontend\")\n",
    "async def ready_frontend() -> dict:\n",
    "    \"\"\"\n",
    "    Frontend-safe readiness payload.\n",
    "    Returns only small, stable fields the React SPA depends on.\n",
    "    This avoids the large nested dependency audit data that was causing frontend crashes.\n",
    "    \"\"\"\n",
    "    ready_for_login = get_app_ready()\n",
    "    loaded = set(model_service.models.keys())\n",
    "    return {\n",
    "        \"ready\": ready_for_login,\n",
    "        \"models\": {\n",
    "            \"iris\": \"iris_random_forest\" in loaded or \"iris_logreg\" in loaded,\n",
    "            \"cancer\": \"breast_cancer_bayes\" in loaded or \"breast_cancer_stub\" in loaded,\n",
    "        },\n",
    "        \"has_bayes\": \"breast_cancer_bayes\" in loaded,\n",
    "        \"has_stub\": \"breast_cancer_stub\" in loaded,\n",
    "        \"all_models_loaded\": all(\n",
    "            model in loaded \n",
    "            for model in [\"iris_random_forest\", \"breast_cancer_bayes\"]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "@app.post(\"/api/v1/token\", response_model=Token, dependencies=[Depends(login_limit)])\n",
    "async def login(\n",
    "    creds: LoginPayload = Depends(get_credentials),\n",
    "    db: AsyncSession = Depends(get_db),\n",
    "):\n",
    "    \"\"\"\n",
    "    Issue a JWT. Accepts **either**\n",
    "    â€¢ JSON {\"username\": \"...\", \"password\": \"...\"}  *or*\n",
    "    â€¢ classic xâ€‘wwwâ€‘formâ€‘urlencoded.\n",
    "    \"\"\"\n",
    "    # 1ï¸âƒ£ readiness gate\n",
    "    if not get_app_ready():\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,\n",
    "            detail=\"Backend still loading models. Try again in a moment.\",\n",
    "            headers={\"Retryâ€‘After\": \"10\"},\n",
    "        )\n",
    "\n",
    "    # 2ï¸âƒ£ verify credentials\n",
    "    user = await get_user_by_username(db, creds.username)\n",
    "    if not user or not verify_password(creds.password, user.hashed_password):\n",
    "        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED,\n",
    "                            detail=\"Invalid credentials\")\n",
    "\n",
    "    # 3ï¸âƒ£ issue token\n",
    "    token = create_access_token(subject=user.username)\n",
    "    return Token(access_token=token, token_type=\"bearer\")\n",
    "\n",
    "# --- PATCH: ready_full -------------------------------------------------------\n",
    "@app.get(\"/api/v1/ready/full\")\n",
    "async def ready_full(debug: Optional[bool] = False) -> dict:\n",
    "    \"\"\"\n",
    "    Extended readiness probe with environment drift summary.\n",
    "\n",
    "    Query param:\n",
    "      debug=1  -> include filtered_status map for troubleshooting.\n",
    "    \"\"\"\n",
    "    ready_for_login = get_app_ready()\n",
    "    expected = {\"iris_random_forest\", \"breast_cancer_bayes\"}  # minimal contract\n",
    "    loaded = set(model_service.models.keys())\n",
    "\n",
    "    # ----- helpers -----------------------------------------------------------\n",
    "    def _is_meta(k: str) -> bool:\n",
    "        return k.endswith(\"_dep_audit\") or k.endswith(\"_last_error\")\n",
    "\n",
    "    def _model_status_items():\n",
    "        for k, v in model_service.status.items():\n",
    "            if _is_meta(k):\n",
    "                continue\n",
    "            yield k, v\n",
    "\n",
    "    # ----- env drift summary -------------------------------------------------\n",
    "    drift = {}\n",
    "    for m in (\"iris_random_forest\", \"iris_logreg\", \"breast_cancer_bayes\", \"breast_cancer_stub\"):\n",
    "        audit = model_service.status.get(f\"{m}_dep_audit\", {})\n",
    "        critical = any(\n",
    "            (pkg in (\"numpy\", \"scipy\", \"scikit-learn\", \"psutil\")) and rec.get(\"severity\") == \"MAJOR_DRT\"\n",
    "            for pkg, rec in audit.items()\n",
    "        )\n",
    "        drift[m] = {\"critical_drift\": critical, \"details\": audit}\n",
    "\n",
    "    # ----- core fields -------------------------------------------------------\n",
    "    filtered_status = dict(_model_status_items())\n",
    "    all_models_loaded = all(v == \"loaded\" for v in filtered_status.values())\n",
    "    training = [k for k, v in filtered_status.items() if v == \"training\"]\n",
    "\n",
    "    response = {\n",
    "        \"ready\": ready_for_login,\n",
    "        \"model_status\": model_service.status,  # raw (includes meta)\n",
    "        \"env_drift\": drift,\n",
    "        \"all_models_loaded\": all_models_loaded,\n",
    "        \"models\": {m: (m in loaded) for m in expected},\n",
    "        \"training\": training,\n",
    "    }\n",
    "\n",
    "    if debug:\n",
    "        response[\"status_filtered\"] = filtered_status\n",
    "        response[\"status_counts\"] = {\n",
    "            \"raw\": len(model_service.status),\n",
    "            \"filtered\": len(filtered_status),\n",
    "        }\n",
    "\n",
    "    # Log response size for debugging\n",
    "    if debug:\n",
    "        import json\n",
    "        response_size = len(json.dumps(response))\n",
    "        logger.info(\"READY_FULL debug: payload size=%d bytes\", response_size)\n",
    "\n",
    "    logger.debug(\"READY endpoint â€“ _app_ready=%s\", ready_for_login)\n",
    "    return response\n",
    "# --- END PATCH ---------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "# â”€â”€ Alias routes (no auth, not shown in OpenAPI) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "@app.get(\"/ready/full\", include_in_schema=False)\n",
    "async def ready_full_alias():\n",
    "    \"\"\"Alias for front-end calls that miss the /api/v1 prefix.\"\"\"\n",
    "    return await ready_full()\n",
    "\n",
    "@app.get(\"/health\", include_in_schema=False)\n",
    "async def health_alias():\n",
    "    \"\"\"Alias for plain /health (SPA hits it before it knows the prefix).\"\"\"\n",
    "    return await health_check()\n",
    "\n",
    "@app.post(\"/token\", include_in_schema=False)\n",
    "async def login_alias(request: Request):\n",
    "    \"\"\"\n",
    "    Alias: accept /token like /api/v1/token.\n",
    "    Keeps the OAuth2PasswordRequestForm semantics without exposing clutter in docs.\n",
    "    \"\"\"\n",
    "    from fastapi import Form\n",
    "\n",
    "    # Parse form data manually to match OAuth2PasswordRequestForm behavior\n",
    "    form_data = await request.form()\n",
    "    username = form_data.get(\"username\")\n",
    "    password = form_data.get(\"password\")\n",
    "\n",
    "    if not username or not password:\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n",
    "            detail=\"username and password are required\"\n",
    "        )\n",
    "\n",
    "    # Create a mock OAuth2PasswordRequestForm object\n",
    "    class MockForm:\n",
    "        def __init__(self, username, password):\n",
    "            self.username = username\n",
    "            self.password = password\n",
    "\n",
    "    mock_form = MockForm(username, password)\n",
    "\n",
    "    # Reuse the existing login logic\n",
    "    db = await get_db().__anext__()\n",
    "    return await login(mock_form, db)\n",
    "\n",
    "@app.post(\"/iris/predict\", include_in_schema=False)\n",
    "async def iris_predict_alias(request: Request):\n",
    "    \"\"\"Alias for /api/v1/iris/predict\"\"\"\n",
    "    from .schemas.iris import IrisPredictRequest\n",
    "\n",
    "    # Parse JSON body\n",
    "    body = await request.json()\n",
    "    iris_request = IrisPredictRequest(**body)\n",
    "\n",
    "    # Reuse the existing prediction logic without authentication for testing\n",
    "    background_tasks = BackgroundTasks()\n",
    "    current_user = \"test_user\"  # Skip authentication for alias endpoints\n",
    "    return await predict_iris(iris_request, background_tasks, current_user)\n",
    "\n",
    "@app.post(\"/cancer/predict\", include_in_schema=False)\n",
    "async def cancer_predict_alias(request: Request):\n",
    "    \"\"\"Alias for /api/v1/cancer/predict\"\"\"\n",
    "    from .schemas.cancer import CancerPredictRequest\n",
    "\n",
    "    # Parse JSON body\n",
    "    body = await request.json()\n",
    "    cancer_request = CancerPredictRequest(**body)\n",
    "\n",
    "    # Reuse the existing prediction logic without authentication for testing\n",
    "    background_tasks = BackgroundTasks()\n",
    "    current_user = \"test_user\"  # Skip authentication for alias endpoints\n",
    "    return await predict_cancer(cancer_request, background_tasks, current_user)\n",
    "\n",
    "# ----- on-demand training endpoints ----------------------------------\n",
    "@app.post(\"/api/v1/iris/train\", status_code=202, dependencies=[Depends(training_limit)])\n",
    "async def train_iris(\n",
    "    request: IrisTrainRequest,\n",
    "    background_tasks: BackgroundTasks,\n",
    "    current_user: str = Depends(get_current_user)\n",
    "):\n",
    "    \"\"\"\n",
    "    Kick off training of the chosen Iris model.\n",
    "    \"\"\"\n",
    "    background_tasks.add_task(\n",
    "        model_service.train_iris,\n",
    "        request.model_type\n",
    "    )\n",
    "    return {\"status\": f\"started iris training ({request.model_type})\"}\n",
    "\n",
    "@app.post(\"/api/v1/cancer/train\", status_code=202, dependencies=[Depends(training_limit)])\n",
    "async def train_cancer(\n",
    "    request: CancerTrainRequest,\n",
    "    background_tasks: BackgroundTasks,\n",
    "    current_user: str = Depends(get_current_user)\n",
    "):\n",
    "    \"\"\"\n",
    "    Kick off training of the chosen Cancer model.\n",
    "    \"\"\"\n",
    "    background_tasks.add_task(\n",
    "        model_service.train_cancer,\n",
    "        request.model_type\n",
    "    )\n",
    "    return {\"status\": f\"started cancer training ({request.model_type})\"}\n",
    "\n",
    "@app.get(\"/api/v1/cancer/bayes/config\", response_model=BayesConfigResponse)\n",
    "async def get_bayes_config(current_user: str = Depends(get_current_user)):\n",
    "    \"\"\"\n",
    "    Get Bayesian training configuration for frontend form generation.\n",
    "    \"\"\"\n",
    "    from .schemas.bayes import BayesCancerParams\n",
    "    \n",
    "    defaults = BayesCancerParams()\n",
    "    \n",
    "    return BayesConfigResponse(\n",
    "        defaults=defaults,\n",
    "        bounds={\n",
    "            \"draws\": {\"min\": 200, \"max\": 20000},\n",
    "            \"tune\": {\"min\": 200, \"max\": 20000},\n",
    "            \"target_accept\": {\"min\": 0.80, \"max\": 0.999},\n",
    "            \"max_rhat_warn\": {\"min\": 1.0, \"max\": 1.1},\n",
    "            \"min_ess_warn\": {\"min\": 50, \"max\": 5000},\n",
    "        },\n",
    "        descriptions={\n",
    "            \"draws\": \"Number of posterior draws retained. More draws = better MCSE but longer runtime.\",\n",
    "            \"tune\": \"Warmup steps for NUTS adaptation. Should be â‰¥ 0.2 * draws for good convergence.\",\n",
    "            \"target_accept\": \"Target acceptance rate. Higher values reduce divergences but increase runtime.\",\n",
    "            \"compute_waic\": \"Compute Widely Applicable Information Criterion. Fast but may be less robust than LOO.\",\n",
    "            \"compute_loo\": \"Compute Leave-One-Out cross-validation. More reliable but slower.\",\n",
    "        },\n",
    "        runtime_estimate={\n",
    "            \"base_seconds_per_sample\": 0.001,  # rough estimate\n",
    "            \"chains\": 4,\n",
    "            \"overhead_seconds\": 5.0,  # model setup, data loading, etc.\n",
    "        }\n",
    "    )\n",
    "\n",
    "@app.post(\"/api/v1/cancer/bayes/train\", response_model=BayesTrainResponse, dependencies=[Depends(training_limit)])\n",
    "async def train_bayes_cancer(\n",
    "    request: BayesTrainRequest,\n",
    "    background_tasks: BackgroundTasks,\n",
    "    current_user: str = Depends(get_current_user)\n",
    "):\n",
    "    \"\"\"\n",
    "    Train Bayesian cancer model with validated hyperparameters.\n",
    "    \"\"\"\n",
    "    if not get_app_ready():\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,\n",
    "            detail=\"Backend still loading models. Try again in a moment.\",\n",
    "            headers={\"Retryâ€‘After\": \"10\"},\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        if request.async_training:\n",
    "            # Queue for background processing\n",
    "            job_id = f\"bayes_{int(time.time())}\"\n",
    "            background_tasks.add_task(\n",
    "                model_service.train_bayes_cancer_with_params, \n",
    "                request.params\n",
    "            )\n",
    "            return BayesTrainResponse(\n",
    "                run_id=\"\",  # will be set when job completes\n",
    "                job_id=job_id,\n",
    "                status=\"queued\",\n",
    "                message=\"Training queued for background processing\"\n",
    "            )\n",
    "        else:\n",
    "            # Synchronous training\n",
    "            run_id = await model_service.train_bayes_cancer_with_params(request.params)\n",
    "            return BayesTrainResponse(\n",
    "                run_id=run_id,\n",
    "                status=\"completed\",\n",
    "                message=\"Training completed successfully\"\n",
    "            )\n",
    "    except Exception as e:\n",
    "        logger.error(\"Bayesian training failed: %s\", e)\n",
    "        return BayesTrainResponse(\n",
    "            run_id=\"\",\n",
    "            status=\"failed\",\n",
    "            message=str(e)\n",
    "        )\n",
    "\n",
    "@app.get(\"/api/v1/cancer/bayes/runs/{run_id}\", response_model=BayesRunMetrics)\n",
    "async def get_bayes_run_metrics(\n",
    "    run_id: str,\n",
    "    current_user: str = Depends(get_current_user)\n",
    "):\n",
    "    \"\"\"\n",
    "    Get metrics for a specific Bayesian training run.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import mlflow\n",
    "        run = mlflow.get_run(run_id)\n",
    "        metrics = run.data.metrics\n",
    "        params = run.data.params\n",
    "        \n",
    "        warnings = []\n",
    "        if metrics.get(\"rhat_max\", 0) > 1.01:\n",
    "            warnings.append(f\"R-hat exceeds threshold: {metrics['rhat_max']:.4f} > 1.01\")\n",
    "        if metrics.get(\"ess_bulk_min\", 0) < 400:\n",
    "            warnings.append(f\"Bulk ESS below threshold: {metrics['ess_bulk_min']:.1f} < 400\")\n",
    "        \n",
    "        return BayesRunMetrics(\n",
    "            run_id=run_id,\n",
    "            accuracy=metrics.get(\"accuracy\", 0.0),\n",
    "            rhat_max=metrics.get(\"rhat_max\"),\n",
    "            ess_bulk_min=metrics.get(\"ess_bulk_min\"),\n",
    "            ess_tail_min=metrics.get(\"ess_tail_min\"),\n",
    "            waic=metrics.get(\"waic\"),\n",
    "            loo=metrics.get(\"loo\"),\n",
    "            status=\"completed\",\n",
    "            warnings=warnings\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(\"Failed to get run metrics for %s: %s\", run_id, e)\n",
    "        raise HTTPException(status_code=404, detail=f\"Run {run_id} not found or metrics unavailable\")\n",
    "\n",
    "# ----- debug endpoints ----------------------------------\n",
    "@app.get(\"/api/v1/debug/ready\")\n",
    "async def debug_ready():\n",
    "    \"\"\"Debug endpoint to verify configuration loading.\"\"\"\n",
    "    return {\n",
    "        \"status\": \"ready\",\n",
    "        \"environment\": settings.ENVIRONMENT,\n",
    "        \"rate_limits\": {\n",
    "            \"default\": settings.RATE_LIMIT_DEFAULT,\n",
    "            \"cancer\": settings.RATE_LIMIT_CANCER,\n",
    "            \"login\": settings.RATE_LIMIT_LOGIN,\n",
    "            \"training\": settings.RATE_LIMIT_TRAINING,\n",
    "            \"window\": settings.RATE_LIMIT_WINDOW,\n",
    "            \"window_light\": settings.RATE_LIMIT_WINDOW_LIGHT,\n",
    "        },\n",
    "        \"quality_gates\": {\n",
    "            \"accuracy_threshold\": settings.QUALITY_GATE_ACCURACY_THRESHOLD,\n",
    "            \"f1_threshold\": settings.QUALITY_GATE_F1_THRESHOLD,\n",
    "        },\n",
    "        \"mlflow\": {\n",
    "            \"experiment\": settings.MLFLOW_EXPERIMENT,\n",
    "            \"tracking_uri\": settings.MLFLOW_TRACKING_URI,\n",
    "            \"registry_uri\": settings.MLFLOW_REGISTRY_URI,\n",
    "        },\n",
    "        \"training\": {\n",
    "            \"skip_background\": settings.SKIP_BACKGROUND_TRAINING,\n",
    "            \"auto_train_missing\": settings.AUTO_TRAIN_MISSING,\n",
    "        },\n",
    "        \"debug\": {\n",
    "            \"debug_ratelimit\": settings.DEBUG_RATELIMIT,\n",
    "        }\n",
    "    }\n",
    "\n",
    "# --- effective config debug --------------------------------------------------\n",
    "@app.get(\"/api/v1/debug/effective-config\")\n",
    "async def effective_config(current_user: str = Depends(get_current_user)):\n",
    "    \"\"\"\n",
    "    Inspect the *effective* runtime configuration (after YAML + env overrides).\n",
    "\n",
    "    Sensitive fields are redacted. Use to debug environment drift across\n",
    "    dev/staging/production deployments.\n",
    "    \"\"\"\n",
    "    from app.core.config import settings\n",
    "\n",
    "    redacted = {\"SECRET_KEY\", \"DATABASE_URL\"}\n",
    "    cfg = settings.model_dump()\n",
    "    for k in list(cfg):\n",
    "        if k.upper() in redacted and cfg[k] is not None:\n",
    "            cfg[k] = \"***redacted***\"\n",
    "    return {\n",
    "        \"environment\": settings.ENVIRONMENT_CANONICAL,\n",
    "        \"config\": cfg,\n",
    "    }\n",
    "\n",
    "# ----- MLOps endpoints (new) ----------------------------------------\n",
    "@app.post(\"/api/v1/mlops/evaluate/{model_name}\")\n",
    "async def evaluate_model(\n",
    "    model_name: str,\n",
    "    run_id: str,\n",
    "    current_user: str = Depends(get_current_user)\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate a candidate model against production baseline.\n",
    "\n",
    "    This endpoint is used by CI/CD pipelines to implement quality gates.\n",
    "    The model is evaluated on a fixed test set and compared to production.\n",
    "    \"\"\"\n",
    "    logger.info(f\"User {current_user} evaluating model {model_name} (run: {run_id})\")\n",
    "\n",
    "    result = await model_service.evaluate_model_quality(model_name, run_id)\n",
    "    return result\n",
    "\n",
    "@app.post(\"/api/v1/mlops/promote/{model_name}/staging\")\n",
    "async def promote_to_staging(\n",
    "    model_name: str,\n",
    "    run_id: str,\n",
    "    current_user: str = Depends(get_current_user)\n",
    "):\n",
    "    \"\"\"\n",
    "    Promote a model to staging after quality gate evaluation.\n",
    "\n",
    "    This endpoint:\n",
    "    1. Evaluates the model quality\n",
    "    2. If passed, registers as staging version\n",
    "    3. Sets @staging alias for atomic promotion\n",
    "    \"\"\"\n",
    "    logger.info(f\"User {current_user} promoting {model_name} to staging (run: {run_id})\")\n",
    "\n",
    "    result = await model_service.promote_model_to_staging(model_name, run_id)\n",
    "    return result\n",
    "\n",
    "@app.post(\"/api/v1/mlops/promote/{model_name}/production\")\n",
    "async def promote_to_production(\n",
    "    model_name: str,\n",
    "    version: Optional[int] = None,\n",
    "    approved_by: Optional[str] = None,\n",
    "    current_user: str = Depends(get_current_user)\n",
    "):\n",
    "    \"\"\"\n",
    "    Promote a staging model to production.\n",
    "\n",
    "    This can be called manually or by CI/CD:\n",
    "    - If version specified, promotes that specific version\n",
    "    - Otherwise, promotes the current @staging alias\n",
    "    - Sets @prod alias for atomic promotion\n",
    "    \"\"\"\n",
    "    logger.info(f\"User {current_user} promoting {model_name} to production (version: {version})\")\n",
    "\n",
    "    result = await model_service.promote_model_to_production(model_name, version, approved_by)\n",
    "    return result\n",
    "\n",
    "@app.post(\"/api/v1/mlops/reload-model\")\n",
    "async def reload_model(\n",
    "    model_name: Optional[str] = None,\n",
    "    current_user: str = Depends(get_current_user)\n",
    "):\n",
    "    \"\"\"\n",
    "    Hot-reload models from MLflow registry.\n",
    "\n",
    "    This endpoint allows the container to pick up new models\n",
    "    without restarting the entire service. Useful for:\n",
    "    - CI/CD deployments that update models\n",
    "    - Manual model promotions\n",
    "    - Testing new model versions\n",
    "\n",
    "    Args:\n",
    "        model_name: Specific model to reload (optional, reloads all if None)\n",
    "    \"\"\"\n",
    "    logger.info(f\"User {current_user} reloading models (specific: {model_name})\")\n",
    "\n",
    "    try:\n",
    "        if model_name:\n",
    "            # Reload specific model\n",
    "            success = await model_service._try_load(model_name)\n",
    "            if success:\n",
    "                return {\n",
    "                    \"reloaded\": True,\n",
    "                    \"model\": model_name,\n",
    "                    \"status\": model_service.status.get(model_name, \"unknown\")\n",
    "                }\n",
    "            else:\n",
    "                raise HTTPException(\n",
    "                    status_code=404,\n",
    "                    detail=f\"Failed to reload model {model_name}\"\n",
    "                )\n",
    "        else:\n",
    "            # Reload all models\n",
    "            reloaded = []\n",
    "            failed = []\n",
    "\n",
    "            for name in [\"iris_random_forest\", \"iris_logreg\", \n",
    "                        \"breast_cancer_bayes\", \"breast_cancer_stub\"]:\n",
    "                try:\n",
    "                    success = await model_service._try_load(name)\n",
    "                    if success:\n",
    "                        reloaded.append(name)\n",
    "                    else:\n",
    "                        failed.append(name)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Failed to reload {name}: {e}\")\n",
    "                    failed.append(name)\n",
    "\n",
    "            return {\n",
    "                \"reloaded\": len(failed) == 0,\n",
    "                \"reloaded_models\": reloaded,\n",
    "                \"failed_models\": failed,\n",
    "                \"status\": model_service.status\n",
    "            }\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Model reload failed: {e}\")\n",
    "        raise HTTPException(\n",
    "            status_code=500,\n",
    "            detail=f\"Model reload failed: {e}\"\n",
    "        )\n",
    "\n",
    "@app.get(\"/api/v1/mlops/status\")\n",
    "async def mlops_status(current_user: str = Depends(get_current_user)):\n",
    "    \"\"\"\n",
    "    Get MLOps status including model versions and stages.\n",
    "\n",
    "    Returns comprehensive information about:\n",
    "    - Model loading status\n",
    "    - Registry versions and stages\n",
    "    - Alias assignments\n",
    "    - Training status\n",
    "    \"\"\"\n",
    "    logger.info(f\"User {current_user} requesting MLOps status\")\n",
    "\n",
    "    try:\n",
    "        client = model_service.mlflow_client\n",
    "\n",
    "        # Get model registry information\n",
    "        registry_info = {}\n",
    "        for model_name in [\"iris_random_forest\", \"iris_logreg\", \n",
    "                          \"breast_cancer_bayes\", \"breast_cancer_stub\"]:\n",
    "            try:\n",
    "                versions = client.search_model_versions(f\"name='{model_name}'\")\n",
    "                registry_info[model_name] = {\n",
    "                    \"versions\": len(versions),\n",
    "                    \"stages\": {},\n",
    "                    \"aliases\": {}\n",
    "                }\n",
    "\n",
    "                # Group by stage\n",
    "                for v in versions:\n",
    "                    stage = v.current_stage\n",
    "                    if stage not in registry_info[model_name][\"stages\"]:\n",
    "                        registry_info[model_name][\"stages\"][stage] = []\n",
    "                    registry_info[model_name][\"stages\"][stage].append({\n",
    "                        \"version\": v.version,\n",
    "                        \"run_id\": v.run_id,\n",
    "                        \"created_at\": v.creation_timestamp\n",
    "                    })\n",
    "\n",
    "                # Get aliases\n",
    "                try:\n",
    "                    aliases = client.get_registered_model_aliases(model_name)\n",
    "                    registry_info[model_name][\"aliases\"] = {\n",
    "                        alias: version for alias, version in aliases.items()\n",
    "                    }\n",
    "                except Exception as e:\n",
    "                    logger.debug(f\"Could not get aliases for {model_name}: {e}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not get registry info for {model_name}: {e}\")\n",
    "                registry_info[model_name] = {\"error\": str(e)}\n",
    "\n",
    "        return {\n",
    "            \"model_status\": model_service.status,\n",
    "            \"loaded_models\": list(model_service.models.keys()),\n",
    "            \"registry_info\": registry_info,\n",
    "            \"app_ready\": get_app_ready(),\n",
    "            \"mlflow_uri\": settings.MLFLOW_TRACKING_URI\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"MLOps status failed: {e}\")\n",
    "        raise HTTPException(\n",
    "            status_code=500,\n",
    "            detail=f\"MLOps status failed: {e}\"\n",
    "        )\n",
    "\n",
    "@app.get(\"/api/v1/mlops/models/{model_name}/metrics\")\n",
    "async def get_model_metrics(\n",
    "    model_name: str,\n",
    "    current_user: str = Depends(get_current_user)\n",
    "):\n",
    "    \"\"\"\n",
    "    Get metrics for all versions of a registered model.\n",
    "\n",
    "    This endpoint enables MLOps comparison between different model versions\n",
    "    for quality gate decisions and promotion workflows.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        metrics = await model_service.get_model_metrics(model_name)\n",
    "        if not metrics:\n",
    "            raise HTTPException(\n",
    "                status_code=404, \n",
    "                detail=f\"No registered model found with name '{model_name}'\"\n",
    "            )\n",
    "        return {\n",
    "            \"model_name\": model_name,\n",
    "            \"versions\": metrics,\n",
    "            \"total_versions\": len(metrics)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching metrics for {model_name}: {e}\")\n",
    "        raise HTTPException(\n",
    "            status_code=500,\n",
    "            detail=f\"Failed to fetch model metrics: {str(e)}\"\n",
    "        )\n",
    "\n",
    "@app.get(\"/api/v1/mlops/models/{model_name}/compare\")\n",
    "async def compare_model_versions(\n",
    "    model_name: str,\n",
    "    version_a: int,\n",
    "    version_b: int,\n",
    "    current_user: str = Depends(get_current_user)\n",
    "):\n",
    "    \"\"\"\n",
    "    Compare two specific model versions for MLOps decision making.\n",
    "\n",
    "    This endpoint helps determine which model version performs better\n",
    "    across key metrics like accuracy, F1-score, precision, and recall.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        comparison = await model_service.compare_model_versions(\n",
    "            model_name, version_a, version_b\n",
    "        )\n",
    "\n",
    "        if \"error\" in comparison:\n",
    "            raise HTTPException(\n",
    "                status_code=400,\n",
    "                detail=comparison[\"error\"]\n",
    "            )\n",
    "\n",
    "        return comparison\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error comparing versions for {model_name}: {e}\")\n",
    "        raise HTTPException(\n",
    "            status_code=500,\n",
    "            detail=f\"Failed to compare model versions: {str(e)}\"\n",
    "        )\n",
    "\n",
    "@app.get(\"/api/v1/mlops/models/{model_name}/quality-gate\")\n",
    "async def check_quality_gate(\n",
    "    model_name: str,\n",
    "    version: Optional[int] = None,\n",
    "    current_user: str = Depends(get_current_user)\n",
    "):\n",
    "    \"\"\"\n",
    "    Check if a model version passes quality gates.\n",
    "\n",
    "    This endpoint evaluates a model against production baseline\n",
    "    or absolute thresholds to determine if it's ready for promotion.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # If no version specified, use the latest staging version\n",
    "        if version is None:\n",
    "            client = model_service.mlflow_client\n",
    "            staging_versions = client.search_model_versions(\n",
    "                f\"name='{model_name}' AND stage='Staging'\"\n",
    "            )\n",
    "            if not staging_versions:\n",
    "                raise HTTPException(\n",
    "                    status_code=404,\n",
    "                    detail=f\"No staging version found for {model_name}\"\n",
    "                )\n",
    "            version = staging_versions[0].version\n",
    "\n",
    "        # Get the run_id for this version\n",
    "        version_info = client.get_model_version(model_name, version)\n",
    "        run_id = version_info.run_id\n",
    "\n",
    "        # Evaluate quality gate\n",
    "        eval_result = await model_service.evaluate_model_quality(model_name, run_id)\n",
    "\n",
    "        return {\n",
    "            \"model_name\": model_name,\n",
    "            \"version\": version,\n",
    "            \"run_id\": run_id,\n",
    "            \"quality_gate_result\": eval_result,\n",
    "            \"passes_gate\": eval_result[\"promoted\"],\n",
    "            \"reason\": eval_result[\"reason\"]\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error checking quality gate for {model_name}: {e}\")\n",
    "        raise HTTPException(\n",
    "            status_code=500,\n",
    "            detail=f\"Failed to check quality gate: {str(e)}\"\n",
    "        )\n",
    "\n",
    "@app.get(\"/api/v1/iris/ready\")\n",
    "async def iris_ready():\n",
    "    \"\"\"Check if Iris model is loaded and ready.\"\"\"\n",
    "    return {\"loaded\": \"iris_random_forest\" in model_service.models}\n",
    "\n",
    "@app.get(\"/api/v1/cancer/ready\")\n",
    "async def cancer_ready():\n",
    "    \"\"\"Check if Cancer model is loaded and ready.\"\"\"\n",
    "    return {\"loaded\": \"breast_cancer_bayes\" in model_service.models}\n",
    "\n",
    "@app.post(\n",
    "    \"/api/v1/iris/predict\",\n",
    "    response_model=IrisPredictResponse,\n",
    "    status_code=status.HTTP_200_OK,\n",
    "    dependencies=[Depends(light_limit)]\n",
    ")\n",
    "async def predict_iris(\n",
    "    request: IrisPredictRequest,\n",
    "    background_tasks: BackgroundTasks,\n",
    "    current_user: str = Depends(get_current_user),\n",
    "):\n",
    "    \"\"\"\n",
    "    Predict iris species from measurements, with optional Redis caching.\n",
    "    \"\"\"\n",
    "    logger.info(f\"User {current_user} called /iris/predict with {len(request.samples)} samples\")\n",
    "\n",
    "    # Ensure model is loaded\n",
    "    model_name = \"iris_random_forest\" if request.model_type == \"rf\" else \"iris_logreg\"\n",
    "    if model_name not in model_service.models:\n",
    "        raise HTTPException(\n",
    "            status_code=503,\n",
    "            detail=\"Iris model still loading. Try again shortly.\",\n",
    "            headers={\"Retry-After\": \"30\"},\n",
    "        )\n",
    "\n",
    "    # Build cache key from primitives (avoid Pydantic models)\n",
    "    serialized_samples = [s.dict() for s in request.samples]\n",
    "    key = f\"iris:{request.model_type}:{json.dumps(serialized_samples, sort_keys=True)}\"\n",
    "\n",
    "    # Try Redis GET if caching enabled\n",
    "    if settings.CACHE_ENABLED:\n",
    "        cached = await cache.get(key)\n",
    "        if cached:\n",
    "            logger.debug(\"Cache hit for key %s\", key)\n",
    "            return IrisPredictResponse(**json.loads(cached))\n",
    "\n",
    "    # Perform prediction\n",
    "    preds, probs = await model_service.predict_iris(\n",
    "        features=serialized_samples,\n",
    "        model_type=request.model_type,\n",
    "    )\n",
    "\n",
    "    # Prepare a fully-serializable result dict\n",
    "    result = {\n",
    "        \"predictions\": preds,\n",
    "        \"probabilities\": probs,\n",
    "        \"input_received\": serialized_samples,\n",
    "    }\n",
    "\n",
    "    # Store in cache if enabled\n",
    "    if settings.CACHE_ENABLED:\n",
    "        ttl = settings.CACHE_TTL_MINUTES * 60\n",
    "        await cache.set(key, json.dumps(result), ex=ttl)\n",
    "\n",
    "    # Audit log in background\n",
    "    background_tasks.add_task(\n",
    "        logger.info,\n",
    "        f\"[audit] user={current_user} endpoint=iris input={serialized_samples} output={preds}\"\n",
    "    )\n",
    "\n",
    "    return IrisPredictResponse(**result)\n",
    "\n",
    "@app.post(\n",
    "    \"/api/v1/cancer/predict\",\n",
    "    response_model=CancerPredictResponse,\n",
    "    status_code=status.HTTP_200_OK,\n",
    "    dependencies=[Depends(heavy_limit)]\n",
    ")\n",
    "async def predict_cancer(\n",
    "    request: CancerPredictRequest,\n",
    "    background_tasks: BackgroundTasks,\n",
    "    current_user: str = Depends(get_current_user),\n",
    "):\n",
    "    \"\"\"\n",
    "    Predict breast-cancer diagnosis, with optional Redis caching.\n",
    "    \"\"\"\n",
    "    logger.info(f\"User {current_user} called /cancer/predict with {len(request.samples)} samples\")\n",
    "\n",
    "    # Build cache key from primitives (includes posterior_samples)\n",
    "    serialized_samples = [s.dict() for s in request.samples]\n",
    "    key = (\n",
    "        f\"cancer:{request.model_type}:\"\n",
    "        f\"{request.posterior_samples or 0}:\"\n",
    "        f\"{json.dumps(serialized_samples, sort_keys=True)}\"\n",
    "    )\n",
    "\n",
    "    # Try Redis GET if caching enabled\n",
    "    if settings.CACHE_ENABLED:\n",
    "        cached = await cache.get(key)\n",
    "        if cached:\n",
    "            logger.debug(\"Cache hit for key %s\", key)\n",
    "            return CancerPredictResponse(**json.loads(cached))\n",
    "\n",
    "    # Perform prediction\n",
    "    preds, probs, uncertainties = await model_service.predict_cancer(\n",
    "        features=serialized_samples,\n",
    "        model_type=request.model_type,\n",
    "        posterior_samples=request.posterior_samples,\n",
    "    )\n",
    "\n",
    "    # Prepare a fully-serializable result dict\n",
    "    result = {\n",
    "        \"predictions\": preds,\n",
    "        \"probabilities\": probs,\n",
    "        \"uncertainties\": uncertainties,\n",
    "        \"input_received\": serialized_samples,\n",
    "    }\n",
    "\n",
    "    # Store in cache if enabled\n",
    "    if settings.CACHE_ENABLED:\n",
    "        ttl = settings.CACHE_TTL_MINUTES * 60\n",
    "        await cache.set(key, json.dumps(result), ex=ttl)\n",
    "\n",
    "    # Audit log in background\n",
    "    background_tasks.add_task(\n",
    "        logger.info,\n",
    "        f\"[audit] user={current_user} endpoint=cancer input={serialized_samples} output={preds}\"\n",
    "    )\n",
    "\n",
    "    return CancerPredictResponse(**result) \n",
    "\n",
    "@app.get(\"/api/v1/debug/compiler\")\n",
    "async def debug_compiler():\n",
    "    \"\"\"\n",
    "    Debug endpoint to check JAX/NumPyro backend configuration.\n",
    "    Returns information about the JAX backend setup.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import jax\n",
    "        import numpyro\n",
    "        import pymc as pm\n",
    "\n",
    "        return {\n",
    "            \"backend\": \"jax_numpyro\",\n",
    "            \"jax_version\": jax.__version__,\n",
    "            \"numpyro_version\": numpyro.__version__,\n",
    "            \"pymc_version\": pm.__version__,\n",
    "            \"jax_devices\": str(jax.devices()),\n",
    "            \"jax_platform\": jax.default_backend(),\n",
    "            \"status\": \"jax_backend_configured\"\n",
    "        }\n",
    "    except ImportError as e:\n",
    "        return {\n",
    "            \"backend\": \"unknown\",\n",
    "            \"error\": f\"Import error: {e}\",\n",
    "            \"status\": \"missing_dependencies\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"backend\": \"unknown\", \n",
    "            \"error\": f\"Configuration error: {e}\",\n",
    "            \"status\": \"configuration_failed\"\n",
    "        }\n",
    "\n",
    "@app.get(\"/api/v1/debug/psutil\")\n",
    "async def debug_psutil():\n",
    "    \"\"\"\n",
    "    Debug endpoint to check psutil status and configuration.\n",
    "    Returns information about psutil module and its Process class.\n",
    "    \"\"\"\n",
    "    import sys, types\n",
    "    try:\n",
    "        import psutil\n",
    "        module_info = {\n",
    "            \"module_path\": getattr(psutil, \"__file__\", \"?\"),\n",
    "            \"version\": getattr(psutil, \"__version__\", \"?\"),\n",
    "            \"has_Process\": hasattr(psutil, \"Process\"),\n",
    "            \"sys_path\": sys.path\n",
    "        }\n",
    "\n",
    "        # Try a safe Process call\n",
    "        try:\n",
    "            proc = psutil.Process()\n",
    "            module_info[\"process_test\"] = {\n",
    "                \"success\": True,\n",
    "                \"pid\": proc.pid,\n",
    "                \"cpu_count\": psutil.cpu_count()\n",
    "            }\n",
    "        except Exception as e:\n",
    "            module_info[\"process_test\"] = {\n",
    "                \"success\": False,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "\n",
    "        return {\n",
    "            \"status\": \"loaded\",\n",
    "            \"info\": module_info\n",
    "        }\n",
    "    except ImportError as e:\n",
    "        return {\n",
    "            \"status\": \"import_failed\",\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"error\": str(e)\n",
    "        } \n",
    "\n",
    "@app.get(\"/api/v1/debug/deps\")\n",
    "async def debug_deps():\n",
    "    \"\"\"\n",
    "    Report recorded vs. runtime dependency versions for each loaded model.\n",
    "\n",
    "    Uses audit data collected during ModelService._load_production_model().\n",
    "    Helpful when MLflow logs 'requirements_utils' mismatch warnings.\n",
    "\n",
    "    NOTE: purely diagnostic â€“ no secrets.\n",
    "    \"\"\"\n",
    "    import importlib.metadata as im\n",
    "    runtime = {}\n",
    "    for pkg in (\"numpy\", \"scipy\", \"scikit-learn\", \"psutil\", \"pandas\"):\n",
    "        try:\n",
    "            runtime[pkg] = im.version(pkg)\n",
    "        except Exception:\n",
    "            runtime[pkg] = None\n",
    "\n",
    "    audits = {k: v for k, v in model_service.status.items() if k.endswith(\"_dep_audit\")}\n",
    "    return {\n",
    "        \"runtime\": runtime,\n",
    "        \"model_audits\": audits,\n",
    "        \"enforcement_policy\": os.getenv(\"MODEL_ENV_ENFORCEMENT\", \"warn\"),\n",
    "    }\n",
    "\n",
    "\n",
    "@app.get(\"/api/v1/test/401\")\n",
    "async def test_401():\n",
    "    \"\"\"Test endpoint that returns 401 for testing session expiry.\"\"\"\n",
    "    raise HTTPException(\n",
    "        status_code=status.HTTP_401_UNAUTHORIZED,\n",
    "        detail=\"Test 401 response\"\n",
    "    )\n",
    "\n",
    "# â”€â”€ Debugâ€‘only ratelimit helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from .deps.limits import get_redis, _user_or_ip as user_or_ip\n",
    "\n",
    "@app.post(\"/api/v1/debug/ratelimit/reset\", include_in_schema=False)\n",
    "async def rl_reset(request: Request):\n",
    "    \"\"\"\n",
    "    Flush **all** rateâ€‘limit counters bound to the caller (JWT _or_ IP).\n",
    "\n",
    "    We match every fragment that contains the identifier to survive\n",
    "    future changes in FastAPIâ€‘Limiter's key schema.\n",
    "    \"\"\"\n",
    "    r = get_redis()\n",
    "    if not r:\n",
    "        raise HTTPException(status_code=503, detail=\"Rateâ€‘limiter not initialised\")\n",
    "\n",
    "    ident = await user_or_ip(request)\n",
    "    keys = await r.keys(f\"ratelimit:*{ident}*\")        # <â€” broader pattern\n",
    "    if keys:\n",
    "        await r.delete(*keys)\n",
    "    return {\"reset\": len(keys)}\n",
    "\n",
    "if settings.DEBUG_RATELIMIT:          # OFF by default\n",
    "    @app.get(\"/api/v1/debug/ratelimit/{bucket}\", include_in_schema=False)\n",
    "    async def rl_status(bucket: str, request: Request):\n",
    "        \"\"\"\n",
    "        Inspect Redis keys for the current identifier + bucket.\n",
    "        Handy for CI tests â€“ **never enable in prod**.\n",
    "        \"\"\"\n",
    "        key_prefix = f\"ratelimit:{bucket}:{await user_or_ip(request)}\"\n",
    "        r = get_redis()\n",
    "        keys = await r.keys(f\"{key_prefix}*\")\n",
    "        values = await r.mget(keys) if keys else []\n",
    "        return dict(zip(keys, values)) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
