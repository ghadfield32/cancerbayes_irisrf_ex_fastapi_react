{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "63246651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting package.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile package.json\n",
    "{\n",
    "  \"name\": \"fastapi-react-monorepo\",\n",
    "  \"private\": true,\n",
    "  \"scripts\": {\n",
    "    \"install:all\": \"python -m venv .venv && .venv\\\\Scripts\\\\python.exe -m pip install uv && .venv\\\\Scripts\\\\uv.exe pip install -e api && .venv\\\\Scripts\\\\python.exe -m pip install bcrypt passlib[bcrypt] python-dotenv && (cd web && npm install)\",\n",
    "    \"seed\": \".venv\\\\Scripts\\\\python.exe api/scripts/seed_user.py\",\n",
    "    \"dev\": \"concurrently -n \\\"API,WEB\\\" -c \\\"cyan,magenta\\\" \\\".venv\\\\Scripts\\\\python.exe -m uvicorn api.app.main:app --reload --env-file .env\\\" \\\"npm --prefix web run dev\\\"\",\n",
    "    \"dev:backend\": \"python api/scripts/ensure_models.py && .venv\\\\Scripts\\\\python.exe -m uvicorn api.app.main:app --reload --env-file .env\",\n",
    "    \"dev:full\": \"concurrently -n \\\"API,WEB\\\" -c \\\"cyan,magenta\\\" \\\"npm run dev:backend\\\" \\\"npm --prefix web run dev\\\"\",\n",
    "    \"backend\": \".venv\\\\Scripts\\\\python.exe -m uvicorn api.app.main:app --host 0.0.0.0 --port 8000 --env-file .env\",\n",
    "    \"backend:dev\": \".venv\\\\Scripts\\\\python.exe -m uvicorn api.app.main:app --reload --host 0.0.0.0 --port 8000 --env-file .env\",\n",
    "    \"backend:fast\": \"set SKIP_BACKGROUND_TRAINING=1 && .venv\\\\Scripts\\\\python.exe -m uvicorn api.app.main:app --host 0.0.0.0 --port 8000 --env-file .env\",\n",
    "    \"frontend\": \"npm --prefix web run dev\",\n",
    "    \"ensure:models\": \"python api/scripts/ensure_models.py\",\n",
    "    \"test:self-healing\": \"python test_self_healing.py\",\n",
    "    \"test:import\": \"python test_import.py\",\n",
    "    \"build:web\": \"npm --prefix web run build\",\n",
    "    \"debug\": \"timeout /T 3 && curl -s http://127.0.0.1:8000/api/health && echo. && curl -s -X POST -d \\\"username=alice&password=secret\\\" -H \\\"Content-Type: application/x-www-form-urlencoded\\\" http://127.0.0.1:8000/api/token\"\n",
    "  },\n",
    "  \"devDependencies\": {\n",
    "    \"concurrently\": \"^8.2.2\"\n",
    "  }\n",
    "} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "17a95ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting invoke.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile invoke.yml\n",
    "# invoke.yml\n",
    "tasks:\n",
    "  dev:\n",
    "    - uv venv\n",
    "    - uv sync\n",
    "    - uvicorn api.app.main:app --reload\n",
    "  test:\n",
    "    - uv pip install pytest coverage\n",
    "    - pytest -q\n",
    "  lint:\n",
    "    - uv pip install black isort flake8\n",
    "    - black .\n",
    "    - isort .\n",
    "    - flake8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8b816b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting .gitignore\n"
     ]
    }
   ],
   "source": [
    "%%writefile .gitignore\n",
    ".env\n",
    "dev.env\n",
    ".devcontainer/.env.runtime\n",
    "\n",
    "mlruns/\n",
    "mlflow_db/\n",
    "mlruns_local/\n",
    "\n",
    "node_modules/\n",
    "frontend/node_modules/\n",
    "\n",
    "archive/\n",
    ".venv\n",
    "uv.lock\n",
    "\n",
    "test_iris.json\n",
    "#.env.template\n",
    "\n",
    "# Railway CLI (never commit tokens)\n",
    ".railway/config.json\n",
    "\n",
    "archive/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "edd77621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting .env.template\n"
     ]
    }
   ],
   "source": [
    "%%writefile .env.template\n",
    "ENV_NAME=react_fastapi_railway\n",
    "CUDA_TAG=12.8.0\n",
    "DOCKER_BUILDKIT=1\n",
    "HOST_JUPYTER_PORT=8890\n",
    "HOST_TENSORBOARD_PORT=6008\n",
    "HOST_EXPLAINER_PORT=8050\n",
    "HOST_STREAMLIT_PORT=8501\n",
    "HOST_MLFLOW_PORT=5000\n",
    "HOST_APP_PORT=5100\n",
    "HOST_BACKEND_DEV_PORT=5002\n",
    "MLFLOW_TRACKING_URI=http://mlflow:5000\n",
    "MLFLOW_VERSION=2.12.2\n",
    "PYTHON_VER=3.10\n",
    "JAX_PLATFORM_NAME=gpu\n",
    "XLA_PYTHON_CLIENT_PREALLOCATE=true\n",
    "XLA_PYTHON_CLIENT_ALLOCATOR=platform\n",
    "XLA_PYTHON_CLIENT_MEM_FRACTION=0.95\n",
    "XLA_FLAGS=--xla_force_host_platform_device_count=1\n",
    "JAX_DISABLE_JIT=false\n",
    "JAX_ENABLE_X64=false\n",
    "TF_FORCE_GPU_ALLOW_GROWTH=false\n",
    "JAX_PREALLOCATION_SIZE_LIMIT_BYTES=8589934592\n",
    "\n",
    "RAILWAY_TOKEN=\n",
    "RAILWAY_VITE_API_URL=https://fastapi-production-1d13.up.railway.app\n",
    "VITE_API_URL=http://127.0.0.1:8000\n",
    "REACT_APP_API_URL=https://react-frontend-production-2805.up.railway.app\n",
    "\n",
    "SECRET_KEY=change-me-in-prod\n",
    "USERNAME_KEY=alice\n",
    "USER_PASSWORD=supersecretvalue\n",
    "DATABASE_URL=sqlite+aiosqlite:///./app.db\n",
    "\n",
    "# CORS\n",
    "ALLOWED_ORIGINS=*\n",
    "\n",
    "# Model training flags\n",
    "SKIP_BACKGROUND_TRAINING=0\n",
    "AUTO_TRAIN_MISSING=1\n",
    "UNIT_TESTING=0 \n",
    "\n",
    "\n",
    "RAILWAY_ENVIRONMENT=production\n",
    "RAILWAY_ENVIRONMENT_ID=fa10dc06-75ec-4c11-93d4-a0fde17996d0\n",
    "RAILWAY_ENVIRONMENT_NAME=production\n",
    "RAILWAY_PRIVATE_DOMAIN=empowering-appreciation.railway.internal\n",
    "RAILWAY_PROJECT_ID=fc9da558-31d6-4b28-9eda-2bbe56cc7390\n",
    "RAILWAY_PROJECT_NAME=responsible-abundance\n",
    "RAILWAY_SERVICE_ID=87c129ab-ba49-471a-88bb-853ace60180d\n",
    "RAILWAY_SERVICE_NAME=empowering-appreciation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "042c4e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting logging.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile logging.yaml\n",
    "version: 1\n",
    "disable_existing_loggers: False\n",
    "formatters:\n",
    "  default: \n",
    "    format: \"[%(levelname).1s] %(asctime)s %(name)s â–¶ %(message)s\"\n",
    "handlers:\n",
    "  console:\n",
    "    class: logging.StreamHandler\n",
    "    formatter: default\n",
    "  file:\n",
    "    class: logging.FileHandler\n",
    "    filename: logs/backend.log\n",
    "    formatter: default\n",
    "loggers:\n",
    "  uvicorn.error:  \n",
    "    level: INFO\n",
    "    handlers: [console, file]\n",
    "  uvicorn.access: \n",
    "    level: INFO\n",
    "    handlers: [console, file]\n",
    "  app:            \n",
    "    level: DEBUG\n",
    "    handlers: [console, file]\n",
    "    propagate: False\n",
    "  app.services.ml.model_service:\n",
    "    level: DEBUG\n",
    "    handlers: [console, file]\n",
    "    propagate: False\n",
    "root:\n",
    "  level: INFO\n",
    "  handlers: [console, file] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "333a76f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pyproject.toml\n"
     ]
    }
   ],
   "source": [
    "%%writefile pyproject.toml\n",
    "[project]\n",
    "name = \"react_fastapi_railway\"\n",
    "version = \"0.1.0\"\n",
    "description = \"Pytorch and Jax GPU docker container\"\n",
    "authors = [\n",
    "  { name = \"Geoffrey Hadfield\" },\n",
    "]\n",
    "license = \"MIT\"\n",
    "readme = \"README.md\"\n",
    "\n",
    "# â”€â”€â”€ Restrict to Python 3.10â€“3.12 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "requires-python = \">=3.10,<3.13\"\n",
    "\n",
    "dependencies = [\n",
    "  # Core web framework\n",
    "  \"fastapi>=0.104.0\",\n",
    "  \"uvicorn[standard]>=0.24.0\",\n",
    "  \"python-dotenv>=1.0.0\",\n",
    "\n",
    "  # Settings and validation\n",
    "  \"pydantic>=2.0.0\",\n",
    "  \"pydantic-settings>=2.0.0\",\n",
    "\n",
    "  # HTTP client and multipart parsing\n",
    "  \"httpx>=0.24.0\",\n",
    "  \"python-multipart>=0.0.6\",\n",
    "\n",
    "  # Data & ML basics\n",
    "  \"numpy>=1.24.0\",\n",
    "  \"pandas>=2.1.0\",\n",
    "  \"scikit-learn>=1.3.0\",\n",
    "  \"mlflow>=2.8.0\",\n",
    "\n",
    "  # (Your existing extrasâ€”keep if you still need them)\n",
    "  \"matplotlib>=3.4.0\",\n",
    "  \"pymc>=5.0.0\",\n",
    "  \"arviz>=0.14.0\",\n",
    "  \"statsmodels>=0.13.0\",\n",
    "  \"jupyterlab>=3.0.0\",\n",
    "  \"seaborn>=0.11.0\",\n",
    "  \"tabulate>=0.9.0\",\n",
    "  \"shap>=0.40.0\",\n",
    "  \"xgboost>=1.5.0\",\n",
    "  \"lightgbm>=3.3.0\",\n",
    "  \"catboost>=1.2.8,<1.3.0\",\n",
    "  \"scipy>=1.7.0\",\n",
    "  \"shapash[report]>=2.3.0\",\n",
    "  \"shapiq>=0.1.0\",\n",
    "  \"explainerdashboard==0.5.1\",\n",
    "  \"ipywidgets>=8.0.0\",\n",
    "  \"nutpie>=0.7.1\",\n",
    "  \"numpyro>=0.18.0,<1.0.0\",\n",
    "  \"jax==0.6.0\",\n",
    "  \"jaxlib==0.6.0\",\n",
    "  \"pytensor>=2.18.3\",\n",
    "  \"aesara>=2.9.4\",\n",
    "  \"tqdm>=4.67.0\",\n",
    "  \"pyarrow>=12.0.0\",\n",
    "  \"optuna>=3.0.0\",\n",
    "  \"optuna-integration[mlflow]>=0.2.0\",\n",
    "  \"omegaconf>=2.3.0,<2.4.0\",\n",
    "  \"hydra-core>=1.3.2,<1.4.0\",\n",
    "  \"aiosqlite>=0.19.0\", \n",
    "  \"python-jose[cryptography]>=3.3.0\",\n",
    "  \"passlib[bcrypt]>=1.7.4\",\n",
    "  \"bcrypt==4.0.1\",  # Pin bcrypt version to resolve warning\n",
    "]\n",
    "\n",
    "[project.optional-dependencies]\n",
    "dev = [\n",
    "  \"pytest>=7.0.0\",\n",
    "  \"black>=23.0.0\",\n",
    "  \"isort>=5.0.0\",\n",
    "  \"flake8>=5.0.0\",\n",
    "  \"mypy>=1.0.0\",\n",
    "  \"invoke>=2.2\",\n",
    "]\n",
    "\n",
    "cuda = [\n",
    "  \"cupy-cuda12x>=12.0.0\",\n",
    "]\n",
    "\n",
    "[tool.pytensor]\n",
    "device    = \"cuda\"\n",
    "floatX    = \"float32\"\n",
    "allow_gc  = true\n",
    "optimizer = \"fast_run\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8cfd1084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/pyproject.toml\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/pyproject.toml\n",
    "[project]\n",
    "name = \"api\"\n",
    "version = \"1.0.0\"\n",
    "description = \"FastAPI backend with React frontend\"\n",
    "requires-python = \">=3.8\"\n",
    "dependencies = [\n",
    "    \"fastapi>=0.104.0\",\n",
    "    \"uvicorn>=0.24.0\",\n",
    "    \"sqlalchemy>=2.0.23\",\n",
    "    \"aiosqlite>=0.19.0\",\n",
    "    \"python-jose[cryptography]>=3.3.0\",\n",
    "    \"passlib[bcrypt]>=1.7.4\",\n",
    "    \"python-multipart>=0.0.6\",\n",
    "    \"pydantic>=2.4.2\",\n",
    "    \"bcrypt==4.0.1\",  # Pin bcrypt version to resolve warning\n",
    "    # ML dependencies\n",
    "    \"mlflow>=2.8.0\",\n",
    "    \"scikit-learn>=1.3.0\",\n",
    "    \"pandas>=2.0.0\",\n",
    "    \"numpy>=1.24.0\",\n",
    "    \"pymc>=5.7.0\",\n",
    "    \"arviz>=0.15.0\",\n",
    "    \"requests>=2.31.0\",\n",
    "    \"jax>=0.4.23\",\n",
    "    \"jaxlib>=0.4.23\"\n",
    "]\n",
    "\n",
    "[project.optional-dependencies]\n",
    "dev = [\n",
    "    \"pytest>=7.0.0\",\n",
    "    \"pytest-asyncio>=0.21.0\",\n",
    "    \"httpx>=0.24.0\"\n",
    "]\n",
    "\n",
    "[build-system]\n",
    "requires = [\"hatchling\"]\n",
    "build-backend = \"hatchling.build\"\n",
    "\n",
    "[tool.hatch.build.targets.wheel]\n",
    "packages = [\"app\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "12f1f765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/railway.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/railway.json\n",
    "{\n",
    "  \"$schema\": \"https://railway.app/railway.schema.json\",\n",
    "  \"build\": { \"builder\": \"NIXPACKS\" },\n",
    "  \"deploy\": {\n",
    "    \"startCommand\": \"bash ./start.sh\",\n",
    "    \"healthcheckPath\": \"/api/v1/health\",\n",
    "    \"healthcheckInterval\": 10,\n",
    "    \"healthcheckTimeout\": 300,\n",
    "    \"restartPolicyType\": \"ON_FAILURE\",\n",
    "    \"restartPolicyMaxRetries\": 10\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4e68f0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/start.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/start.sh\n",
    "#!/usr/bin/env bash\n",
    "set -euo pipefail\n",
    "\n",
    "# â”€â”€ sanity â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if [[ -z \"${PORT:-}\" ]]; then\n",
    "  echo \"âŒ  PORT not set â€“ Railway always provides it.\" >&2\n",
    "  exit 1\n",
    "fi\n",
    "\n",
    "if [[ -z \"${SECRET_KEY:-}\" ]]; then\n",
    "  echo \"âŒ  SECRET_KEY is not set for the backend service â€“ aborting.\" >&2\n",
    "  exit 1\n",
    "fi\n",
    "\n",
    "echo \"ðŸš€  FastAPI boot; PORT=$PORT  PY=$(python -V)\"\n",
    "env | grep -E 'RAILWAY_|PORT|DATABASE_URL' | sed 's/SECRET_KEY=.*/SECRET_KEY=***/'\n",
    "\n",
    "# â”€â”€ optional local .env ------------------------------------------------------\n",
    "[[ -f .env ]] && export $(grep -Ev '^#' .env | xargs)\n",
    "\n",
    "# â”€â”€ one-shot DB migrate + seed (blocks until done) ---------------------------\n",
    "python -m scripts.seed_user\n",
    "\n",
    "# â”€â”€ run the API --------------------------------------------------------------\n",
    "exec uvicorn app.main:app \\\n",
    "  --host 0.0.0.0 --port \"$PORT\" \\\n",
    "  --proxy-headers --forwarded-allow-ips=\"*\" --log-level info\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2dcbfaca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/scripts/seed_user.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/scripts/seed_user.py\n",
    "from pathlib import Path\n",
    "from passlib.context import CryptContext\n",
    "from sqlalchemy.ext.asyncio import create_async_engine, async_sessionmaker\n",
    "from sqlalchemy import select\n",
    "import os, asyncio\n",
    "\n",
    "# â”€â”€ optional .env load (UTF-8 only) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ENV_PATH = Path(__file__).resolve().parents[2] / \".env\"\n",
    "if ENV_PATH.exists():\n",
    "    try:\n",
    "        from dotenv import load_dotenv\n",
    "        load_dotenv(ENV_PATH, encoding=\"utf-8\")\n",
    "    except UnicodeDecodeError:\n",
    "        print(\"âš ï¸  .env not UTF-8 â€“ skipped\")\n",
    "\n",
    "# â”€â”€ model import (kept same) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import sys; sys.path.append(str(Path(__file__).resolve().parents[1]))\n",
    "from app.models import Base, User\n",
    "\n",
    "USERNAME = os.getenv(\"USERNAME_KEY\", \"alice\")\n",
    "PASSWORD = os.getenv(\"USER_PASSWORD\", \"supersecretvalue\")\n",
    "\n",
    "pwd = CryptContext(schemes=[\"bcrypt\"], deprecated=\"auto\")\n",
    "engine = create_async_engine(\"sqlite+aiosqlite:///./app.db\")\n",
    "session_factory = async_sessionmaker(engine, expire_on_commit=False)\n",
    "\n",
    "async def main():\n",
    "    async with engine.begin() as conn:\n",
    "        await conn.run_sync(Base.metadata.create_all)\n",
    "\n",
    "    async with session_factory() as db:\n",
    "        result = await db.execute(select(User).where(User.username == USERNAME))\n",
    "        user = result.scalar_one_or_none()\n",
    "        hashed = pwd.hash(PASSWORD)\n",
    "\n",
    "        if user:\n",
    "            user.hashed_password = hashed\n",
    "            action = \"Updated\"\n",
    "        else:\n",
    "            db.add(User(username=USERNAME, hashed_password=hashed))\n",
    "            action = \"Created\"\n",
    "        await db.commit()\n",
    "        print(f\"{action} user {USERNAME}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "850e919f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/app/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/app/__init__.py\n",
    "# FastAPI backend package "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3a5b7392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/app/core/config.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/app/core/config.py\n",
    "\"\"\"\n",
    "Core configuration settings for the FastAPI application.\n",
    "Centralizes environment variables and provides sensible defaults.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "class Settings:\n",
    "    \"\"\"Application settings with environment-based configuration.\"\"\"\n",
    "    \n",
    "    # Database\n",
    "    DATABASE_URL: str = os.getenv(\"DATABASE_URL\", \"sqlite+aiosqlite:///./app.db\")\n",
    "    \n",
    "    # Security\n",
    "    SECRET_KEY: Optional[str] = os.getenv(\"SECRET_KEY\")\n",
    "    ACCESS_TOKEN_EXPIRE_MINUTES: int = int(os.getenv(\"ACCESS_TOKEN_EXPIRE_MINUTES\", \"30\"))\n",
    "    \n",
    "    # CORS\n",
    "    ALLOWED_ORIGINS: str = os.getenv(\"ALLOWED_ORIGINS\", \"*\")\n",
    "    \n",
    "    # MLflow in local-file mode by default\n",
    "    MLFLOW_TRACKING_URI: str = os.getenv(\n",
    "        \"MLFLOW_TRACKING_URI\",\n",
    "        \"file:./mlruns_local\"\n",
    "    )\n",
    "    MLFLOW_REGISTRY_URI: str = os.getenv(\n",
    "        \"MLFLOW_REGISTRY_URI\",\n",
    "        MLFLOW_TRACKING_URI\n",
    "    )\n",
    "    \n",
    "    # Model training flags\n",
    "    SKIP_BACKGROUND_TRAINING: bool = os.getenv(\"SKIP_BACKGROUND_TRAINING\", \"0\") == \"1\"\n",
    "    AUTO_TRAIN_MISSING: bool = os.getenv(\"AUTO_TRAIN_MISSING\", \"1\") == \"1\"\n",
    "    UNIT_TESTING: bool = os.getenv(\"UNIT_TESTING\", \"0\") == \"1\"\n",
    "\n",
    "settings = Settings() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "49871cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/app/crud.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/app/crud.py\n",
    "from sqlalchemy import select\n",
    "from sqlalchemy.ext.asyncio import AsyncSession\n",
    "from .models import User\n",
    "\n",
    "async def get_user_by_username(db: AsyncSession, username: str):\n",
    "    stmt = select(User).where(User.username == username)\n",
    "    res = await db.execute(stmt)\n",
    "    return res.scalar_one_or_none() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "61dc31d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/app/models.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/app/models.py\n",
    "from sqlalchemy import Column, Integer, String\n",
    "from sqlalchemy.orm import declarative_base\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "class User(Base):\n",
    "    __tablename__ = \"users\"\n",
    "    id = Column(Integer, primary_key=True, index=True)\n",
    "    username = Column(String, unique=True, index=True)\n",
    "    hashed_password = Column(String) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a86ef40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/app/db.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/app/db.py\n",
    "# api/app/db.py\n",
    "from contextlib import asynccontextmanager\n",
    "import os, logging, asyncio\n",
    "from sqlalchemy.ext.asyncio import (\n",
    "    AsyncSession,\n",
    "    create_async_engine,\n",
    "    async_sessionmaker,\n",
    ")\n",
    "from .models import Base\n",
    "from .services.ml.model_service import model_service\n",
    "from .core.config import settings\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Database engine & session factory (module-level singletons â€“ cheap & safe)\n",
    "# ---------------------------------------------------------------------------\n",
    "DATABASE_URL = os.getenv(\"DATABASE_URL\", \"sqlite+aiosqlite:///./app.db\")\n",
    "engine = create_async_engine(DATABASE_URL, echo=False, future=True)\n",
    "AsyncSessionLocal = async_sessionmaker(engine, expire_on_commit=False)\n",
    "\n",
    "# Global readiness flag\n",
    "_app_ready: bool = False\n",
    "\n",
    "def get_app_ready():\n",
    "    \"\"\"Get the current app ready status.\"\"\"\n",
    "    return _app_ready\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# FastAPI lifespan â€“ runs ONCE at startup / shutdown\n",
    "# ---------------------------------------------------------------------------\n",
    "@asynccontextmanager\n",
    "async def lifespan(app):\n",
    "    \"\"\"Open & dispose engine at app startup/shutdown; create all tables.\"\"\"\n",
    "    global _app_ready\n",
    "    \n",
    "    logger.info(\"ðŸ—„ï¸  Initializing databaseâ€¦  URL=%s\", DATABASE_URL)\n",
    "    try:\n",
    "        async with engine.begin() as conn:\n",
    "            # DDL is safe here; it blocks startup until complete\n",
    "            await conn.run_sync(Base.metadata.create_all)\n",
    "        logger.info(\"âœ… Database tables created/verified successfully\")\n",
    "        \n",
    "        # Initialize application readiness\n",
    "        logger.info(\"ðŸš€ Startup event starting - _app_ready=%s\", _app_ready)\n",
    "\n",
    "        if settings.UNIT_TESTING:\n",
    "            logger.info(\"ðŸ”’ UNIT_TESTING=1 â€“ startup hooks bypassed\")\n",
    "            _app_ready = True\n",
    "            logger.info(\"âœ… _app_ready set to True (unit testing)\")\n",
    "        else:\n",
    "            try:\n",
    "                # Initialize ModelService first\n",
    "                logger.info(\"ðŸ”§ Initializing ModelService\")\n",
    "                await model_service.initialize()\n",
    "                logger.info(\"âœ… ModelService initialized successfully\")\n",
    "\n",
    "                # Start background training tasks\n",
    "                logger.info(\"ðŸ”„ Starting background training tasks\")\n",
    "                asyncio.create_task(model_service.startup())\n",
    "                logger.info(\"âœ… Background training tasks started\")\n",
    "\n",
    "                # Set ready to true after initialization (models will load in background)\n",
    "                _app_ready = True\n",
    "                logger.info(\"ðŸš€ FastAPI ready â€“ _app_ready=%s, health probes will pass immediately\", _app_ready)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(\"âŒ Startup event failed: %s\", e)\n",
    "                import traceback\n",
    "                logger.error(\"âŒ Startup traceback: %s\", traceback.format_exc())\n",
    "                # Set ready to true anyway so the API can serve requests\n",
    "                _app_ready = True\n",
    "                logger.warning(\"âš ï¸  Setting _app_ready=True despite startup errors\")\n",
    "        \n",
    "        logger.info(\"ðŸŽ¯ Lifespan startup complete - _app_ready=%s\", _app_ready)\n",
    "        yield\n",
    "    finally:\n",
    "        logger.info(\"ðŸ”’ Disposing database engineâ€¦\")\n",
    "        await engine.dispose()\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Dependency injection helper\n",
    "# ---------------------------------------------------------------------------\n",
    "async def get_db() -> AsyncSession:\n",
    "    \"\"\"Yield a new DB session per request.\"\"\"\n",
    "    async with AsyncSessionLocal() as session:\n",
    "        yield session\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9fe2096b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/app/security.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/app/security.py\n",
    "from __future__ import annotations\n",
    "import os, logging, secrets\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Optional\n",
    "\n",
    "from fastapi import Depends, HTTPException, status\n",
    "from fastapi.security import OAuth2PasswordBearer, OAuth2PasswordRequestForm\n",
    "from jose import jwt, JWTError\n",
    "from passlib.context import CryptContext\n",
    "from pydantic import BaseModel\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 1.  SECRET_KEY ***must*** be provided in the environment in production.\n",
    "# ---------------------------------------------------------------------------\n",
    "SECRET_KEY = os.getenv(\"SECRET_KEY\")\n",
    "if not SECRET_KEY:\n",
    "    log.critical(\n",
    "        \"ENV variable SECRET_KEY is missing -- generating a temporary key. \"\n",
    "        \"ALL issued JWTs will be invalid after a pod restart! \"\n",
    "        \"Set it in Railway â†’ Variables to disable this warning.\"\n",
    "    )\n",
    "    SECRET_KEY = secrets.token_urlsafe(32)   # fallback only for dev\n",
    "\n",
    "ALGORITHM = \"HS256\"\n",
    "ACCESS_TOKEN_EXPIRE_MINUTES = int(os.getenv(\"ACCESS_TOKEN_EXPIRE_MINUTES\", 30))\n",
    "\n",
    "pwd_ctx = CryptContext(schemes=[\"bcrypt\"], deprecated=\"auto\")\n",
    "oauth2_scheme = OAuth2PasswordBearer(tokenUrl=\"/api/v1/token\")\n",
    "\n",
    "class TokenData(BaseModel):\n",
    "    username: Optional[str] = None\n",
    "\n",
    "def verify_password(raw: str, hashed: str) -> bool:\n",
    "    return pwd_ctx.verify(raw, hashed)\n",
    "\n",
    "def get_password_hash(pw: str) -> str:\n",
    "    return pwd_ctx.hash(pw)\n",
    "\n",
    "def create_access_token(subject: str) -> str:\n",
    "    expire = datetime.utcnow() + timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)\n",
    "    return jwt.encode({\"sub\": subject, \"exp\": expire}, SECRET_KEY, algorithm=ALGORITHM)\n",
    "\n",
    "async def get_current_user(token: str = Depends(oauth2_scheme)) -> str:\n",
    "    try:\n",
    "        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])\n",
    "        username: str = payload.get(\"sub\")\n",
    "        if not username:\n",
    "            raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED)\n",
    "        return username\n",
    "    except JWTError as exc:\n",
    "        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED) from exc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c6e5ee",
   "metadata": {},
   "source": [
    "# additional models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0cc65796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/app/schemas/cancer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/app/schemas/cancer.py\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "\n",
    "class CancerFeatures(BaseModel):\n",
    "    \"\"\"Breast cancer diagnostic features.\"\"\"\n",
    "    mean_radius: float = Field(..., description=\"Mean of distances from center to points on perimeter\")\n",
    "    mean_texture: float = Field(..., description=\"Standard deviation of gray-scale values\")\n",
    "    mean_perimeter: float = Field(..., description=\"Mean size of the core tumor\")\n",
    "    mean_area: float = Field(..., description=\"Mean area of the core tumor\")\n",
    "    mean_smoothness: float = Field(..., description=\"Mean of local variation in radius lengths\")\n",
    "    mean_compactness: float = Field(..., description=\"Mean of perimeter^2 / area - 1.0\")\n",
    "    mean_concavity: float = Field(..., description=\"Mean of severity of concave portions of the contour\")\n",
    "    mean_concave_points: float = Field(..., description=\"Mean for number of concave portions of the contour\")\n",
    "    mean_symmetry: float = Field(..., description=\"Mean symmetry\")\n",
    "    mean_fractal_dimension: float = Field(..., description=\"Mean for 'coastline approximation' - 1\")\n",
    "    \n",
    "    # SE features (standard error)\n",
    "    se_radius: float = Field(..., description=\"Standard error of radius\")\n",
    "    se_texture: float = Field(..., description=\"Standard error of texture\")\n",
    "    se_perimeter: float = Field(..., description=\"Standard error of perimeter\")\n",
    "    se_area: float = Field(..., description=\"Standard error of area\")\n",
    "    se_smoothness: float = Field(..., description=\"Standard error of smoothness\")\n",
    "    se_compactness: float = Field(..., description=\"Standard error of compactness\")\n",
    "    se_concavity: float = Field(..., description=\"Standard error of concavity\")\n",
    "    se_concave_points: float = Field(..., description=\"Standard error of concave points\")\n",
    "    se_symmetry: float = Field(..., description=\"Standard error of symmetry\")\n",
    "    se_fractal_dimension: float = Field(..., description=\"Standard error of fractal dimension\")\n",
    "    \n",
    "    # Worst features\n",
    "    worst_radius: float = Field(..., description=\"Worst radius\")\n",
    "    worst_texture: float = Field(..., description=\"Worst texture\")\n",
    "    worst_perimeter: float = Field(..., description=\"Worst perimeter\")\n",
    "    worst_area: float = Field(..., description=\"Worst area\")\n",
    "    worst_smoothness: float = Field(..., description=\"Worst smoothness\")\n",
    "    worst_compactness: float = Field(..., description=\"Worst compactness\")\n",
    "    worst_concavity: float = Field(..., description=\"Worst concavity\")\n",
    "    worst_concave_points: float = Field(..., description=\"Worst concave points\")\n",
    "    worst_symmetry: float = Field(..., description=\"Worst symmetry\")\n",
    "    worst_fractal_dimension: float = Field(..., description=\"Worst fractal dimension\")\n",
    "\n",
    "class CancerPredictRequest(BaseModel):\n",
    "    \"\"\"Cancer prediction request (allows 'rows' alias).\"\"\"\n",
    "    model_type: str = Field(\"bayes\", description=\"Model type: 'bayes', 'logreg', or 'rf'\")\n",
    "    samples: List[CancerFeatures] = Field(\n",
    "        ...,\n",
    "        description=\"Breast-cancer feature vectors\",\n",
    "        alias=\"rows\",\n",
    "    )\n",
    "    posterior_samples: Optional[int] = Field(\n",
    "        None, ge=10, le=10_000, description=\"Posterior draws for uncertainty\"\n",
    "    )\n",
    "\n",
    "    class Config:\n",
    "        populate_by_name = True\n",
    "        extra = \"forbid\"\n",
    "\n",
    "class CancerPredictResponse(BaseModel):\n",
    "    \"\"\"Cancer prediction response.\"\"\"\n",
    "    predictions: List[str] = Field(..., description=\"Predicted diagnosis (M=malignant, B=benign)\")\n",
    "    probabilities: List[float] = Field(..., description=\"Probability of malignancy\")\n",
    "    uncertainties: Optional[List[float]] = Field(None, description=\"Uncertainty estimates (if requested)\")\n",
    "    input_received: List[CancerFeatures] = Field(..., description=\"Echo of input features\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1fd0f1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/app/schemas/iris.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/app/schemas/iris.py\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "\n",
    "class IrisFeatures(BaseModel):\n",
    "    \"\"\"Iris measurement features.\"\"\"\n",
    "    sepal_length: float = Field(..., description=\"Sepal length in cm\", ge=0, le=10)\n",
    "    sepal_width: float = Field(..., description=\"Sepal width in cm\", ge=0, le=10)\n",
    "    petal_length: float = Field(..., description=\"Petal length in cm\", ge=0, le=10)\n",
    "    petal_width: float = Field(..., description=\"Petal width in cm\", ge=0, le=10)\n",
    "\n",
    "class IrisPredictRequest(BaseModel):\n",
    "    \"\"\"Iris prediction request (accepts legacy 'rows' alias).\"\"\"\n",
    "    model_type: str = Field(\"rf\", description=\"Model type: 'rf' or 'logreg'\")\n",
    "    samples: List[IrisFeatures] = Field(\n",
    "        ...,\n",
    "        description=\"List of iris measurements\",\n",
    "        alias=\"rows\",\n",
    "    )\n",
    "\n",
    "    class Config:\n",
    "        populate_by_name = True\n",
    "        extra = \"forbid\"\n",
    "\n",
    "class IrisPredictResponse(BaseModel):\n",
    "    \"\"\"Iris prediction response.\"\"\"\n",
    "    predictions: List[str] = Field(..., description=\"Predicted iris species\")\n",
    "    probabilities: List[List[float]] = Field(..., description=\"Class probabilities\")\n",
    "    input_received: List[IrisFeatures] = Field(..., description=\"Echo of input features\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "70bc6c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/app/ml/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/app/ml/__init__.py\n",
    "\"\"\"\n",
    "ML sub-package â€“ exposes built-in trainers so the service can import\n",
    "`app.ml.builtin_trainers` with an absolute import.\n",
    "\"\"\"\n",
    "\n",
    "from .builtin_trainers import (\n",
    "    train_iris_random_forest,\n",
    "    train_breast_cancer_bayes,\n",
    "    train_breast_cancer_stub,\n",
    ")\n",
    "\n",
    "__all__ = [\n",
    "    \"train_iris_random_forest\",\n",
    "    \"train_breast_cancer_bayes\",\n",
    "    \"train_breast_cancer_stub\",\n",
    "] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "374475aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/app/ml/utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/app/ml/utils.py\n",
    "\"\"\"\n",
    "Utility functions for ML model training and compiler detection.\n",
    "\"\"\"\n",
    "\n",
    "import shutil\n",
    "import logging\n",
    "import os\n",
    "import subprocess\n",
    "import shlex\n",
    "import platform\n",
    "import glob\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "def find_compiler() -> str | None:\n",
    "    \"\"\"\n",
    "    Return absolute path to a working C/C++ compiler or None.\n",
    "\n",
    "    Searches in order:\n",
    "    1. Explicit override via PYTENSOR_CXX environment variable\n",
    "    2. Common compiler names (g++, gcc, cl.exe, cl)\n",
    "    3. Windows Visual Studio BuildTools typical location (last resort)\n",
    "\n",
    "    Returns:\n",
    "        str | None: Absolute path to compiler executable, or None if not found\n",
    "    \"\"\"\n",
    "    # 1ï¸âƒ£ explicit override via env\n",
    "    override = os.getenv(\"PYTENSOR_CXX\")\n",
    "    if override and shutil.which(override):\n",
    "        log.info(f\"Using compiler from PYTENSOR_CXX: {override}\")\n",
    "        return override\n",
    "\n",
    "    # 2ï¸âƒ£ try common names\n",
    "    for exe in (\"g++\", \"gcc\", \"cl.exe\", \"cl\"):\n",
    "        path = shutil.which(exe)\n",
    "        if path:\n",
    "            log.info(f\"Found compiler: {path}\")\n",
    "            return path\n",
    "\n",
    "    # 3ï¸âƒ£ Windows VS BuildTools typical location (last resort)\n",
    "    if platform.system() == \"Windows\":\n",
    "        vswhere = r\"C:\\Program Files (x86)\\Microsoft Visual Studio\\Installer\\vswhere.exe\"\n",
    "        if os.path.exists(vswhere):\n",
    "            try:\n",
    "                log.debug(\"Probing for Visual Studio BuildTools via vswhere...\")\n",
    "                out = subprocess.check_output(\n",
    "                    [vswhere, \"-latest\", \"-products\", \"*\", \"-requires\", \n",
    "                     \"Microsoft.VisualStudio.Component.VC.Tools.x86.x64\", \n",
    "                     \"-property\", \"installationPath\"],\n",
    "                    text=True,\n",
    "                    timeout=5,\n",
    "                ).strip()\n",
    "\n",
    "                if out:\n",
    "                    # Look for cl.exe in the typical location\n",
    "                    cand = rf\"{out}\\VC\\Tools\\MSVC\\*\\bin\\Hostx64\\x64\\cl.exe\"\n",
    "                    matches = glob.glob(cand)\n",
    "                    if matches:\n",
    "                        log.info(f\"Found Visual Studio compiler: {matches[0]}\")\n",
    "                        return matches[0]\n",
    "                    else:\n",
    "                        log.debug(f\"VS installation found at {out} but cl.exe not found\")\n",
    "                else:\n",
    "                    log.debug(\"vswhere found no Visual Studio installations\")\n",
    "\n",
    "            except subprocess.TimeoutExpired:\n",
    "                log.debug(\"vswhere probe timed out\")\n",
    "            except subprocess.CalledProcessError as exc:\n",
    "                log.debug(f\"vswhere probe failed with return code {exc.returncode}\")\n",
    "            except Exception as exc:\n",
    "                log.debug(f\"vswhere probe failed: {exc}\")\n",
    "\n",
    "    log.warning(\"No C/C++ compiler found\")\n",
    "    return None\n",
    "\n",
    "def configure_pytensor_compiler(compiler_path: str | None = None) -> bool:\n",
    "    \"\"\"\n",
    "    Configure PyTensor to use a specific compiler with MSVC-safe flags.\n",
    "\n",
    "    Args:\n",
    "        compiler_path: Path to compiler executable. If None, will search for one.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if compiler was configured successfully, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import pytensor  # late import so function can be called very early\n",
    "    except ImportError:\n",
    "        log.warning(\"PyTensor not available â€“ cannot configure compiler\")\n",
    "        return False\n",
    "\n",
    "    # 1ï¸âƒ£ Resolve the compiler path ------------------------------------------------\n",
    "    if compiler_path is None:\n",
    "        compiler_path = find_compiler()\n",
    "    if compiler_path is None:\n",
    "        log.warning(\"No compiler found â€“ PyTensor will fall back to defaults\")\n",
    "        return False\n",
    "\n",
    "    # 2ï¸âƒ£ Write settings into PyTensor's global config -----------------------------\n",
    "    system_is_windows = platform.system() == \"Windows\"\n",
    "    basename = os.path.basename(compiler_path).lower()\n",
    "\n",
    "    if system_is_windows:\n",
    "        # Quote path so spaces in \"Program Files (x86)\" don't break the command line\n",
    "        pytensor.config.cxx = f'\"{compiler_path}\"'\n",
    "\n",
    "        # If this *is* MSVC, strip every GCC flag and substitute safe disables\n",
    "        if \"cl\" in basename:\n",
    "            # MSVC understands /wdXXXX but not -Wno-â€¦  âžœ  map the common ones\n",
    "            pytensor.config.cxxflags = \"/wd4100 /wd4244 /wd4267 /wd4996\"\n",
    "            log.info(\"âœ… Configured MSVC with safe warning suppressions\")\n",
    "    else:\n",
    "        pytensor.config.cxx = compiler_path  # GCC / Clang path is fine\n",
    "\n",
    "    # 3ï¸âƒ£ NUCLEAR OPTION: Blank ALL environment variables that PyTensor uses to inject flags\n",
    "    # PyTensor checks these environment variables in multiple places\n",
    "    flag_vars = [\n",
    "        \"PYTENSOR_FLAGS\",\n",
    "        \"THEANO_FLAGS\",  # Legacy but still checked\n",
    "        \"PYTENSOR_CXXFLAGS\",\n",
    "        \"THEANO_CXXFLAGS\",  # Legacy but still checked\n",
    "    ]\n",
    "    \n",
    "    for var in flag_vars:\n",
    "        os.environ[var] = \"cxxflags=\"\n",
    "    \n",
    "    # 4ï¸âƒ£ Additional PyTensor config overrides to prevent flag injection\n",
    "    if system_is_windows and \"cl\" in basename:\n",
    "        # Disable PyTensor's automatic flag injection\n",
    "        pytensor.config.mode = \"FAST_COMPILE\"  # Avoid some optimizations that inject flags\n",
    "        pytensor.config.optimizer = \"fast_compile\"  # Use simpler optimizer\n",
    "        \n",
    "        # Set additional config to prevent GCC flag injection\n",
    "        pytensor.config.cmodule__compilation_warning = False\n",
    "        pytensor.config.cmodule__warn_no_version = False\n",
    "        \n",
    "        # Force PyTensor to use our flags only\n",
    "        pytensor.config.cxxflags = \"/wd4100 /wd4244 /wd4267 /wd4996\"\n",
    "        \n",
    "        log.info(\"ðŸ›¡ï¸ Applied nuclear option: disabled all GCC flag injection\")\n",
    "\n",
    "    # 5ï¸âƒ£ Optional verbose diagnostics --------------------------------------------\n",
    "    if os.getenv(\"DEBUG_COMPILER\") == \"1\":\n",
    "        log.debug(\"PyTensor.cxx      = %s\", pytensor.config.cxx)\n",
    "        log.debug(\"PyTensor.cxxflags = %s\", getattr(pytensor.config, \"cxxflags\", \"\"))\n",
    "        log.debug(\"PYTENSOR_FLAGS    = %s\", os.getenv(\"PYTENSOR_FLAGS\", \"NOT_SET\"))\n",
    "        log.debug(\"THEANO_FLAGS      = %s\", os.getenv(\"THEANO_FLAGS\", \"NOT_SET\"))\n",
    "\n",
    "    log.info(\"ðŸ›  PyTensor now uses compiler: %s\", compiler_path)\n",
    "    return True\n",
    "\n",
    "def test_compiler_availability() -> dict:\n",
    "    \"\"\"\n",
    "    Test what compilers are available on the system.\n",
    "\n",
    "    Returns:\n",
    "        dict: Mapping of compiler names to availability status\n",
    "    \"\"\"\n",
    "    compilers = [\"g++\", \"gcc\", \"cl.exe\", \"cl\"]\n",
    "    available = {}\n",
    "\n",
    "    for compiler in compilers:\n",
    "        try:\n",
    "            result = subprocess.run([compiler, \"--version\"], \n",
    "                                  capture_output=True, text=True, timeout=5)\n",
    "            available[compiler] = result.returncode == 0\n",
    "            if result.returncode == 0:\n",
    "                log.info(f\"âœ… {compiler}: Available\")\n",
    "                log.debug(f\"   Version: {result.stdout.split()[0] if result.stdout else 'Unknown'}\")\n",
    "            else:\n",
    "                log.debug(f\"âŒ {compiler}: Not available (return code: {result.returncode})\")\n",
    "        except FileNotFoundError:\n",
    "            log.debug(f\"âŒ {compiler}: Not found\")\n",
    "            available[compiler] = False\n",
    "        except subprocess.TimeoutExpired:\n",
    "            log.debug(f\"â° {compiler}: Timeout\")\n",
    "            available[compiler] = False\n",
    "        except Exception as e:\n",
    "            log.debug(f\"âŒ {compiler}: Error - {e}\")\n",
    "            available[compiler] = False\n",
    "\n",
    "    return available \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "72a78a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/app/ml/builtin_trainers.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/app/ml/builtin_trainers.py\n",
    "# api/ml/builtin_trainers.py\n",
    "\"\"\"\n",
    "Built-in trainers for Iris RF and Breast-Cancer Bayesian LogReg.\n",
    "Executed automatically by ModelService when a model is missing.\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from pathlib import Path\n",
    "import mlflow, mlflow.sklearn, mlflow.pyfunc\n",
    "from sklearn.datasets import load_iris, load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import pickle\n",
    "import warnings\n",
    "import subprocess\n",
    "import os\n",
    "import platform\n",
    "\n",
    "# Conditional imports for heavy dependencies\n",
    "if os.getenv(\"UNIT_TESTING\") != \"1\" and os.getenv(\"SKIP_BACKGROUND_TRAINING\") != \"1\":\n",
    "    import pymc as pm\n",
    "    import arviz as az\n",
    "else:\n",
    "    pm = None\n",
    "    az = None\n",
    "\n",
    "# â”€â”€ NEW: Configure MLflow to use local file storage â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Set MLflow to use local file storage instead of remote server\n",
    "os.environ.setdefault(\"MLFLOW_TRACKING_URI\", \"file:./mlruns_local\")\n",
    "os.environ.setdefault(\"MLFLOW_REGISTRY_URI\", \"file:./mlruns_local\")\n",
    "\n",
    "# Configure MLflow tracking URI immediately\n",
    "mlflow.set_tracking_uri(\"file:./mlruns_local\")\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "MLFLOW_EXPERIMENT = \"ml_fullstack_models\"\n",
    "\n",
    "# Only set experiment if not in unit test mode and after tracking URI is set\n",
    "if os.getenv(\"UNIT_TESTING\") != \"1\":\n",
    "    try:\n",
    "        mlflow.set_experiment(MLFLOW_EXPERIMENT)\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Could not set MLflow experiment: {e}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "#  IRIS â€“ point-estimate Random-Forest (enhanced with better parameters)\n",
    "# -----------------------------------------------------------------------------\n",
    "def train_iris_random_forest(\n",
    "    n_estimators: int = 300,\n",
    "    max_depth: int | None = None,\n",
    "    random_state: int = 42\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Train + register a Random-Forest on the Iris data and push it to MLflow.\n",
    "    Returns the run_id (string). Enhanced with better parameters and stratified split.\n",
    "    \"\"\"\n",
    "    iris = load_iris(as_frame=True)\n",
    "    X, y = iris.data, iris.target\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.25,\n",
    "                                              stratify=y, random_state=random_state)\n",
    "\n",
    "    # Enhanced Random Forest with better parameters\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1,  # Use all available cores\n",
    "        class_weight='balanced'  # Handle any class imbalance\n",
    "    ).fit(X_tr, y_tr)\n",
    "\n",
    "    preds = rf.predict(X_te)\n",
    "    metrics = {\n",
    "        \"accuracy\":  accuracy_score(y_te, preds),\n",
    "        \"f1_macro\":  f1_score(y_te, preds, average=\"macro\"),\n",
    "        \"precision_macro\": precision_score(y_te, preds, average=\"macro\"),\n",
    "        \"recall_macro\":    recall_score(y_te, preds, average=\"macro\"),\n",
    "    }\n",
    "\n",
    "    with mlflow.start_run(run_name=\"iris_random_forest\") as run:\n",
    "        # Log hyperparameters\n",
    "        mlflow.log_params({\n",
    "            \"n_estimators\": n_estimators,\n",
    "            \"max_depth\": max_depth,\n",
    "            \"random_state\": random_state\n",
    "        })\n",
    "\n",
    "        # Log metrics\n",
    "        mlflow.log_metrics(metrics)\n",
    "\n",
    "        # Create a custom pyfunc wrapper that exposes both predict and predict_proba\n",
    "        class IrisRFWrapper(mlflow.pyfunc.PythonModel):\n",
    "            def __init__(self, model):\n",
    "                self.model = model\n",
    "\n",
    "            def predict(self, model_input, params=None):\n",
    "                # Return class probabilities for pyfunc interface\n",
    "                # Convert to numpy array if it's a DataFrame\n",
    "                if hasattr(model_input, 'values'):\n",
    "                    X = model_input.values\n",
    "                else:\n",
    "                    X = model_input\n",
    "                return self.model.predict_proba(X)\n",
    "\n",
    "            def predict_proba(self, X):\n",
    "                # Expose predict_proba for direct access\n",
    "                if hasattr(X, 'values'):\n",
    "                    X = X.values\n",
    "                return self.model.predict_proba(X)\n",
    "\n",
    "            def predict_classes(self, X):\n",
    "                # Expose class prediction\n",
    "                if hasattr(X, 'values'):\n",
    "                    X = X.values\n",
    "                return self.model.predict(X)\n",
    "\n",
    "        iris_wrapper = IrisRFWrapper(rf)\n",
    "\n",
    "        # Log model with proper signature\n",
    "        mlflow.pyfunc.log_model(\n",
    "            artifact_path=\"model\",\n",
    "            python_model=iris_wrapper,\n",
    "            registered_model_name=\"iris_random_forest\",\n",
    "            input_example=X.head(),\n",
    "            signature=mlflow.models.signature.infer_signature(X, iris_wrapper.predict(X))\n",
    "        )\n",
    "        return run.info.run_id\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "#  BREAST-CANCER STUB â€“ ultra-fast fallback model\n",
    "# -----------------------------------------------------------------------------\n",
    "def train_breast_cancer_stub(random_state: int = 42) -> str:\n",
    "    \"\"\"\n",
    "    *Ultra-fast* fallback â€“  < 100 ms on any laptop.\n",
    "    Trains vanilla LogisticRegression so the API can\n",
    "    answer probability queries while the PyMC model cooks.\n",
    "    \"\"\"\n",
    "    from sklearn.datasets import load_breast_cancer\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    import mlflow, tempfile, pickle, pandas as pd\n",
    "\n",
    "    X, y = load_breast_cancer(return_X_y=True, as_frame=True)\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.3,\n",
    "                                          stratify=y, random_state=random_state)\n",
    "\n",
    "    clf = LogisticRegression(max_iter=200, n_jobs=-1).fit(Xtr, ytr)\n",
    "    acc = accuracy_score(yte, clf.predict(Xte))\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as td, mlflow.start_run(run_name=\"breast_cancer_stub\") as run:\n",
    "        mlflow.log_metric(\"accuracy\", acc)\n",
    "        mlflow.sklearn.log_model(\n",
    "            clf, \"model\",\n",
    "            registered_model_name=\"breast_cancer_stub\",\n",
    "            input_example=X.head()\n",
    "        )\n",
    "        return run.info.run_id\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "#  BREAST-CANCER â€“ hierarchical Bayesian logistic regression\n",
    "# -----------------------------------------------------------------------------\n",
    "class _HierBayesLogReg(mlflow.pyfunc.PythonModel):\n",
    "    \"\"\"\n",
    "    Hierarchical Bayesian Logistic Regression wrapper for MLflow serving.\n",
    "    Implements varying intercepts by mean_texture quintiles with global slopes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, trace, scaler, group_edges, feature_names):\n",
    "        self.trace = trace                # ArviZ InferenceData for posterior samples\n",
    "        self.scaler = scaler              # sklearn StandardScaler for feature normalization\n",
    "        self.group_edges = group_edges    # bin edges for creating group indices\n",
    "        self.feature_names = feature_names # column names for proper DataFrame handling\n",
    "\n",
    "    def _group_index(self, X_df):\n",
    "        \"\"\"Create group indices based on mean_texture quintiles.\"\"\"\n",
    "        tex = X_df[\"mean texture\"].to_numpy()\n",
    "        # Use same quintile edges as training, clipping to valid range\n",
    "        return np.clip(np.digitize(tex, self.group_edges, right=False), 0, 4)\n",
    "\n",
    "    def predict(self, model_input, params=None):\n",
    "        \"\"\"\n",
    "        MLflow-required prediction method.\n",
    "\n",
    "        Args:\n",
    "            model_input: pandas.DataFrame with breast cancer features\n",
    "            params: Optional parameters (unused)\n",
    "\n",
    "        Returns:\n",
    "            np.array: Probability of malignancy [0,1] for each sample\n",
    "        \"\"\"\n",
    "        # Ensure we have a DataFrame with proper column order\n",
    "        if isinstance(model_input, pd.DataFrame):\n",
    "            X_df = model_input\n",
    "        else:\n",
    "            X_df = pd.DataFrame(model_input, columns=self.feature_names)\n",
    "\n",
    "        # Standardize features using training scaler\n",
    "        Xs = self.scaler.transform(X_df)\n",
    "\n",
    "        # Get group indices for hierarchical structure\n",
    "        g = self._group_index(X_df)\n",
    "\n",
    "        # Extract posterior medians for prediction\n",
    "        Î± = self.trace.posterior[\"Î±_group\"].median((\"chain\", \"draw\")).values\n",
    "        Î² = self.trace.posterior[\"Î²\"].median((\"chain\", \"draw\")).values\n",
    "\n",
    "        # Compute predictions: logit = Î±_group[g] + X @ Î²\n",
    "        logits = Î±[g] + np.dot(Xs, Î²)\n",
    "\n",
    "        # Convert to probabilities\n",
    "        return 1 / (1 + np.exp(-logits))\n",
    "\n",
    "def train_breast_cancer_bayes(\n",
    "    draws: int = 800,\n",
    "    tune: int = 400,\n",
    "    target_accept: float = 0.90,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Train a hierarchical Bayesian logistic-regression model.\n",
    "\n",
    "    *On Windows* we first configure PyTensor so MSVC builds succeed; if that\n",
    "    fails we raise â€“ the caller will fall back to the stub model instead.\n",
    "    \"\"\"\n",
    "    # ------------------------------------------------------------------ compiler\n",
    "    from app.ml.utils import find_compiler, configure_pytensor_compiler\n",
    "\n",
    "    cxx = find_compiler()\n",
    "    if cxx is None or not configure_pytensor_compiler(cxx):\n",
    "        msg = (\n",
    "            \"No compatible C/C++ compiler (or mis-configuration) â€“ \"\n",
    "            \"skipping Bayesian build.\"\n",
    "        )\n",
    "        logger.warning(\"âš ï¸ %s\", msg)\n",
    "        raise RuntimeError(msg)\n",
    "\n",
    "    # ---------------------------------------------------------------- NumPyro opt\n",
    "    # If we *still* end up on MSVC we can opt-out of C-thunks entirely.\n",
    "    nuts_backend = (\n",
    "        \"numpyro\"\n",
    "        if platform.system() == \"Windows\" and \"cl\" in os.path.basename(cxx).lower()\n",
    "        else \"nuts\"\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------------------ modelling\n",
    "    import pymc as pm, pandas as pd, numpy as np\n",
    "    from sklearn.datasets import load_breast_cancer\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    import mlflow, tempfile, pickle, arviz as az\n",
    "\n",
    "    X_df, y = load_breast_cancer(as_frame=True, return_X_y=True)\n",
    "    quint, edges = pd.qcut(X_df[\"mean texture\"], 5, labels=False, retbins=True)\n",
    "    g = quint.astype(\"int64\").to_numpy()\n",
    "    scaler = StandardScaler().fit(X_df)\n",
    "    Xs = scaler.transform(X_df)\n",
    "\n",
    "    logger.info(\"ðŸ§  Building hierarchical Bayesian model (backend=%s)â€¦\", nuts_backend)\n",
    "    with pm.Model() as mdl:\n",
    "        Î±_group = pm.Normal(\"Î±_group\", mu=0, sigma=1, shape=5)\n",
    "        Î² = pm.Normal(\"Î²\", mu=0, sigma=1, shape=Xs.shape[1])\n",
    "        logits = Î±_group[g] + pm.math.dot(Xs, Î²)\n",
    "        pm.Bernoulli(\"obs\", logit_p=logits, observed=y)\n",
    "        trace = pm.sample(\n",
    "            draws=draws, tune=tune,\n",
    "            target_accept=target_accept,\n",
    "            nuts_sampler=nuts_backend,\n",
    "            progressbar=False,\n",
    "            chains=2, random_seed=123,\n",
    "        )\n",
    "\n",
    "    # ------------------------------------------------------------------ wrapping\n",
    "    class _HierBayesLogReg(mlflow.pyfunc.PythonModel):\n",
    "        def __init__(self, trc, sc, edge, cols):\n",
    "            self.trace, self.scaler, self.edges, self.cols = trc, sc, edge, cols\n",
    "\n",
    "        def _grp(self, df):  # replicate training quintiles\n",
    "            tex = df[\"mean texture\"].to_numpy()\n",
    "            return np.clip(np.digitize(tex, self.edges, right=False), 0, 4)\n",
    "\n",
    "        def predict(self, X, params=None):\n",
    "            df = X if isinstance(X, pd.DataFrame) else pd.DataFrame(X, columns=self.cols)\n",
    "            Xs = self.scaler.transform(df)\n",
    "            g = self._grp(df)\n",
    "            Î±g = self.trace.posterior[\"Î±_group\"].median((\"chain\", \"draw\")).values\n",
    "            Î² = self.trace.posterior[\"Î²\"].median((\"chain\", \"draw\")).values\n",
    "            lg = Î±g[g] + np.dot(Xs, Î²)\n",
    "            return 1 / (1 + np.exp(-lg))\n",
    "\n",
    "    wrapper = _HierBayesLogReg(trace, scaler, edges[1:-1], X_df.columns.tolist())\n",
    "    preds = (wrapper.predict(X_df) > 0.5).astype(int)\n",
    "    acc = float((preds == y).mean())\n",
    "\n",
    "    # -------------------------------------------------------------------- MLflow\n",
    "    with tempfile.TemporaryDirectory() as td, mlflow.start_run(run_name=\"breast_cancer_bayes\") as run:\n",
    "        scaler_path = Path(td) / \"scaler.pkl\"\n",
    "        with open(scaler_path, \"wb\") as fh:\n",
    "            pickle.dump(scaler, fh)\n",
    "\n",
    "        mlflow.log_params({\"draws\": draws, \"tune\": tune, \"target_accept\": target_accept})\n",
    "        mlflow.log_metric(\"accuracy\", acc)\n",
    "        mlflow.pyfunc.log_model(\n",
    "            \"model\", python_model=wrapper,\n",
    "            artifacts={\"scaler\": str(scaler_path)},\n",
    "            registered_model_name=\"breast_cancer_bayes\",\n",
    "            input_example=X_df.head(),\n",
    "            signature=mlflow.models.signature.infer_signature(X_df, wrapper.predict(X_df)),\n",
    "        )\n",
    "        logger.info(\"ðŸ“¦ Bayesian model logged â€“ run_id=%s  acc=%.3f\", run.info.run_id, acc)\n",
    "        return run.info.run_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "fe3394a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/app/services/ml/model_service.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/app/services/ml/model_service.py\n",
    "\"\"\"\n",
    "Model service â€“ self-healing startup with background training.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import asyncio, logging, os, time, socket\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "\n",
    "import mlflow, pandas as pd, numpy as np\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.exceptions import MlflowException\n",
    "\n",
    "from app.core.config import settings\n",
    "from app.ml.builtin_trainers import (\n",
    "    train_iris_random_forest,\n",
    "    train_breast_cancer_bayes,\n",
    "    train_breast_cancer_stub,\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Trainer mapping for self-healing\n",
    "TRAINERS = {\n",
    "    \"iris_random_forest\": train_iris_random_forest,\n",
    "    \"breast_cancer_bayes\": train_breast_cancer_bayes,\n",
    "    \"breast_cancer_stub\": train_breast_cancer_stub,\n",
    "}\n",
    "\n",
    "class ModelService:\n",
    "    \"\"\"\n",
    "    Self-healing model service that loads existing models and schedules\n",
    "    background training for missing ones.\n",
    "    \"\"\"\n",
    "\n",
    "    _EXECUTOR = ThreadPoolExecutor(max_workers=2)\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self._unit_test_mode = settings.UNIT_TESTING\n",
    "        self.initialized = False\n",
    "\n",
    "        # ðŸš« Heavy clients only when NOT unit-testing\n",
    "        self.client = None if self._unit_test_mode else None  # Will be set in initialize()\n",
    "        self.mlflow_client = None\n",
    "\n",
    "        self.models: Dict[str, Any] = {}\n",
    "        self.status: Dict[str, str] = {\n",
    "            \"iris_random_forest\": \"missing\",\n",
    "            \"breast_cancer_bayes\": \"missing\",\n",
    "            \"breast_cancer_stub\": \"missing\",\n",
    "        }\n",
    "\n",
    "    async def initialize(self) -> None:\n",
    "        \"\"\"\n",
    "        Connect to MLflow â€“ fall back to local file store if the configured\n",
    "        tracking URI is unreachable *or* the client is missing critical methods\n",
    "        (e.g. when mlflow-skinny accidentally shadows the full package).\n",
    "        \"\"\"\n",
    "        if self.initialized:\n",
    "            return\n",
    "\n",
    "        def _needs_fallback(client) -> bool:\n",
    "            # any missing attr is a strong signal we are on mlflow-skinny\n",
    "            return not callable(getattr(client, \"list_experiments\", None))\n",
    "\n",
    "        try:\n",
    "            mlflow.set_tracking_uri(settings.MLFLOW_TRACKING_URI)\n",
    "            self.mlflow_client = MlflowClient(settings.MLFLOW_TRACKING_URI)\n",
    "\n",
    "            if _needs_fallback(self.mlflow_client):\n",
    "                raise AttributeError(\"list_experiments not implemented â€“ skinny build detected\")\n",
    "\n",
    "            # minimal probe (cheap & always present)\n",
    "            self.mlflow_client.search_experiments(max_results=1)\n",
    "            logger.info(\"ðŸŸ¢  Connected to MLflow @ %s\", settings.MLFLOW_TRACKING_URI)\n",
    "\n",
    "        except (MlflowException, socket.gaierror, AttributeError) as exc:\n",
    "            logger.warning(\"ðŸ”„  Falling back to local MLflow store â€“ %s\", exc)\n",
    "            mlflow.set_tracking_uri(\"file:./mlruns_local\")\n",
    "            self.mlflow_client = MlflowClient(\"file:./mlruns_local\")\n",
    "            logger.info(\"ðŸ“‚  Using local file store ./mlruns_local\")\n",
    "\n",
    "        await self._load_models()\n",
    "        self.initialized = True\n",
    "\n",
    "    async def _load_models(self) -> None:\n",
    "        \"\"\"Load existing models from MLflow.\"\"\"\n",
    "        await self._try_load(\"iris_random_forest\")\n",
    "        await self._try_load(\"breast_cancer_bayes\")\n",
    "        await self._try_load(\"breast_cancer_stub\")\n",
    "\n",
    "    async def startup(self, auto_train: bool | None = None) -> None:\n",
    "        \"\"\"\n",
    "        Faster: serve stub immediately; heavy Bayesian job in background.\n",
    "        \"\"\"\n",
    "        if self._unit_test_mode:\n",
    "            logger.info(\"ðŸ”’ UNIT_TESTING=1 â€“ skipping model loading\")\n",
    "            return                      # ðŸ‘‰ nothing else runs\n",
    "\n",
    "        # Initialize MLflow connection first\n",
    "        await self.initialize()\n",
    "\n",
    "        if settings.SKIP_BACKGROUND_TRAINING:\n",
    "            logger.warning(\"â© SKIP_BACKGROUND_TRAINING=1 â€“ models will load on-demand\")\n",
    "            # We still *try* to load existing artefacts so prod works\n",
    "            await self._try_load(\"iris_random_forest\")\n",
    "            await self._try_load(\"breast_cancer_bayes\")\n",
    "            return\n",
    "\n",
    "        auto = auto_train if auto_train is not None else settings.AUTO_TRAIN_MISSING\n",
    "        logger.info(\"ðŸ”„ Model-service startup (auto_train=%s)\", auto)\n",
    "\n",
    "        # 1ï¸âƒ£ try to load whatever already exists\n",
    "        await self._try_load(\"iris_random_forest\")\n",
    "\n",
    "        # 2ï¸âƒ£ Load bayes â€“ if exists we're done\n",
    "        if not await self._try_load(\"breast_cancer_bayes\"):\n",
    "            # 3ï¸âƒ£ Ensure stub is *synchronously* available\n",
    "            if not await self._try_load(\"breast_cancer_stub\"):\n",
    "                logger.info(\"Training stub cancer model â€¦\")\n",
    "                await asyncio.get_running_loop().run_in_executor(\n",
    "                    self._EXECUTOR, train_breast_cancer_stub\n",
    "                )\n",
    "                await self._try_load(\"breast_cancer_stub\")\n",
    "\n",
    "            # 4ï¸âƒ£ Fire full PyMC build in background unless disabled\n",
    "            if not settings.SKIP_BACKGROUND_TRAINING:\n",
    "                logger.info(\"Scheduling full Bayesian retrain in background\")\n",
    "                asyncio.create_task(\n",
    "                    self._train_and_reload(\"breast_cancer_bayes\", train_breast_cancer_bayes)\n",
    "                )\n",
    "\n",
    "        # 5ï¸âƒ£ Train iris if missing\n",
    "        if not await self._try_load(\"iris_random_forest\"):\n",
    "            logger.info(\"Training iris model â€¦\")\n",
    "            await asyncio.get_running_loop().run_in_executor(\n",
    "                self._EXECUTOR, train_iris_random_forest\n",
    "            )\n",
    "            await self._try_load(\"iris_random_forest\")\n",
    "\n",
    "    async def _try_load(self, name: str) -> None:\n",
    "        \"\"\"Try to load a model and update status.\"\"\"\n",
    "        model = await self._load_production_model(name)\n",
    "        if model:\n",
    "            self.models[name] = model\n",
    "            self.status[name] = \"loaded\"\n",
    "            logger.info(\"âœ… %s loaded\", name)\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    async def _train_and_reload(self, name: str, trainer) -> None:\n",
    "        \"\"\"Train a model in background and reload it, with verbose phase logs.\"\"\"\n",
    "        try:\n",
    "            t0 = time.perf_counter()\n",
    "            logger.info(\"ðŸ—ï¸  BEGIN training %s\", name)\n",
    "            self.status[name] = \"training\"\n",
    "\n",
    "            loop = asyncio.get_running_loop()\n",
    "            await loop.run_in_executor(self._EXECUTOR, trainer)\n",
    "\n",
    "            logger.info(\"ðŸ“¦ Training %s complete in %.1fs â€“ re-loading\", name,\n",
    "                        time.perf_counter() - t0)\n",
    "            model = await self._load_production_model(name)\n",
    "            if not model:\n",
    "                raise RuntimeError(f\"{name} trained but could not be re-loaded\")\n",
    "\n",
    "            self.models[name] = model\n",
    "            self.status[name] = \"loaded\"\n",
    "            logger.info(\"âœ… %s trained & loaded\", name)\n",
    "\n",
    "        except Exception as exc:\n",
    "            self.status[name] = \"failed\"\n",
    "            logger.error(\"âŒ %s failed: %s\", name, exc, exc_info=True)  # â† keeps trace\n",
    "            # NEW: persist last_error for UI / debug endpoint\n",
    "            self.status[f\"{name}_last_error\"] = str(exc)\n",
    "\n",
    "    async def _load_production_model(self, name: str) -> Optional[Any]:\n",
    "        \"\"\"\n",
    "        1. Registry 'Production' stage â†’ load.  \n",
    "        2. Otherwise most recent run with runName == name.\n",
    "        Returns None if not found.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            versions = self.mlflow_client.search_model_versions(f\"name='{name}'\")\n",
    "            prod = [v for v in versions if v.current_stage == \"Production\"]\n",
    "            if prod:\n",
    "                uri = f\"models:/{name}/{prod[0].version}\"\n",
    "                logger.info(\"â†ªï¸Ž  Loading %s from registry:%s\", name, prod[0].version)\n",
    "                return mlflow.pyfunc.load_model(uri)\n",
    "        except MlflowException:\n",
    "            pass\n",
    "\n",
    "        # Fallback â€“ scan experiments for latest run\n",
    "        runs = []\n",
    "        for exp in self.mlflow_client.search_experiments():\n",
    "            runs.extend(self.mlflow_client.search_runs(\n",
    "                [exp.experiment_id],\n",
    "                f\"tags.mlflow.runName = '{name}'\",\n",
    "                order_by=[\"attributes.start_time DESC\"],\n",
    "                max_results=1))\n",
    "        if runs:\n",
    "            uri = f\"runs:/{runs[0].info.run_id}/model\"\n",
    "            logger.info(\"â†ªï¸Ž  Loading %s from latest run:%s\", name, runs[0].info.run_id)\n",
    "            return mlflow.pyfunc.load_model(uri)\n",
    "        return None\n",
    "\n",
    "    # Manual training endpoints (for UI)\n",
    "    async def train_iris(self) -> None:\n",
    "        await self._train_and_reload(\"iris_random_forest\", TRAINERS[\"iris_random_forest\"])\n",
    "\n",
    "    async def train_cancer(self) -> None:\n",
    "        await self._train_and_reload(\"breast_cancer_bayes\", TRAINERS[\"breast_cancer_bayes\"])\n",
    "\n",
    "    # Predict methods (unchanged from your previous version)\n",
    "    async def predict_iris(\n",
    "        self,\n",
    "        features: List[Dict[str, float]],\n",
    "        model_type: str = \"rf\",\n",
    "    ) -> Tuple[List[str], List[List[float]]]:\n",
    "        \"\"\"\n",
    "        Predict Iris species from measurements.\n",
    "\n",
    "        Args:\n",
    "            features: List of iris measurements as dictionaries\n",
    "            model_type: Model type to use (only 'rf' supported)\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (predicted_class_names, class_probabilities)\n",
    "        \"\"\"\n",
    "        if model_type != \"rf\":\n",
    "            raise ValueError(\"Only 'rf' supported for iris\")\n",
    "\n",
    "        model = self.models.get(\"iris_random_forest\")\n",
    "        if not model:\n",
    "            raise RuntimeError(\"Iris model not loaded\")\n",
    "\n",
    "        # Convert to DataFrame with proper column names (matching training data)\n",
    "        X_df = pd.DataFrame([{\n",
    "            \"sepal length (cm)\": sample[\"sepal_length\"],\n",
    "            \"sepal width (cm)\": sample[\"sepal_width\"], \n",
    "            \"petal length (cm)\": sample[\"petal_length\"],\n",
    "            \"petal width (cm)\": sample[\"petal_width\"]\n",
    "        } for sample in features])\n",
    "\n",
    "        # The iris model wrapper returns probabilities via predict() method\n",
    "        probs = model.predict(X_df)                  # shape (n, 3) - probabilities\n",
    "        preds = probs.argmax(axis=1)                 # numerical class indices\n",
    "\n",
    "        # Map numerical classes to species names\n",
    "        class_names = [\"setosa\", \"versicolor\", \"virginica\"]\n",
    "        pred_names = [class_names[i] for i in preds]\n",
    "\n",
    "        return pred_names, probs.tolist()\n",
    "\n",
    "    async def predict_cancer(\n",
    "        self,\n",
    "        features: List[Dict[str, float]],\n",
    "        model_type: str = \"bayes\",\n",
    "        posterior_samples: Optional[int] = None,\n",
    "    ) -> Tuple[List[str], List[float], Optional[List[Tuple[float, float]]]]:\n",
    "        \"\"\"\n",
    "        Predict breast cancer diagnosis from features using hierarchical Bayesian model.\n",
    "        Falls back to stub model if Bayesian model is not available.\n",
    "\n",
    "        Args:\n",
    "            features: List of cancer measurements as dictionaries\n",
    "            model_type: Model type to use ('bayes' or 'stub')\n",
    "            posterior_samples: Number of posterior samples for uncertainty (Bayesian only)\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (predicted_labels, probabilities, uncertainty_intervals)\n",
    "        \"\"\"\n",
    "        # Determine which model to use\n",
    "        if model_type == \"bayes\":\n",
    "            model = self.models.get(\"breast_cancer_bayes\")\n",
    "            if not model:\n",
    "                # Fall back to stub model\n",
    "                model = self.models.get(\"breast_cancer_stub\")\n",
    "                if not model:\n",
    "                    raise RuntimeError(\"No cancer model available\")\n",
    "                logger.info(\"Using stub cancer model (Bayesian model not ready)\")\n",
    "        elif model_type == \"stub\":\n",
    "            model = self.models.get(\"breast_cancer_stub\")\n",
    "            if not model:\n",
    "                raise RuntimeError(\"Stub cancer model not loaded\")\n",
    "        else:\n",
    "            raise ValueError(\"model_type must be 'bayes' or 'stub'\")\n",
    "\n",
    "        # Convert to DataFrame with proper column names\n",
    "        X_df = pd.DataFrame(features)\n",
    "\n",
    "        # Get predictions\n",
    "        if model_type == \"bayes\" and \"breast_cancer_bayes\" in self.models:\n",
    "            # Use Bayesian model with uncertainty\n",
    "            probs = model.predict(X_df)\n",
    "            labels = [\"malignant\" if p > 0.5 else \"benign\" for p in probs]\n",
    "        else:\n",
    "            # Use stub model (sklearn LogisticRegression)\n",
    "            probs = model.predict_proba(X_df)[:, 1]  # Probability of malignant\n",
    "            labels = [\"malignant\" if p > 0.5 else \"benign\" for p in probs]\n",
    "\n",
    "        # Compute uncertainty intervals if requested (Bayesian model only)\n",
    "        ci = None\n",
    "        if posterior_samples and model_type == \"bayes\" and \"breast_cancer_bayes\" in self.models:\n",
    "            try:\n",
    "                # Access the underlying python model to get the trace\n",
    "                python_model = model.unwrap_python_model()\n",
    "\n",
    "                # Access posterior samples for uncertainty quantification\n",
    "                draws = python_model.trace.posterior\n",
    "                Î±g = draws[\"Î±_group\"].stack(samples=(\"chain\", \"draw\"))\n",
    "                Î² = draws[\"Î²\"].stack(samples=(\"chain\", \"draw\"))\n",
    "\n",
    "                # Get group indices and standardized features\n",
    "                g = python_model._group_index(X_df)\n",
    "                Xs = python_model.scaler.transform(X_df)\n",
    "\n",
    "                # Compute posterior predictive samples\n",
    "                logits = Î±g.values[:, g] + np.dot(Î².values.T, Xs.T)      # shape (S, N)\n",
    "                pp = 1 / (1 + np.exp(-logits))\n",
    "\n",
    "                # Compute 95% credible intervals\n",
    "                lo, hi = np.percentile(pp, [2.5, 97.5], axis=0)\n",
    "                ci = list(zip(lo.tolist(), hi.tolist()))\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to compute uncertainty intervals: {e}\")\n",
    "                ci = None\n",
    "\n",
    "        return labels, probs.tolist(), ci\n",
    "\n",
    "\n",
    "# Global singleton\n",
    "model_service = ModelService()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "01981457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/app/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/app/main.py\n",
    "import logging\n",
    "import os\n",
    "import asyncio\n",
    "from fastapi import FastAPI, Request, Depends, BackgroundTasks, status, HTTPException\n",
    "from fastapi.security import OAuth2PasswordRequestForm\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.responses import JSONResponse\n",
    "from sqlalchemy.ext.asyncio import AsyncSession\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "import time\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from .db import lifespan, get_db, get_app_ready\n",
    "from .security import create_access_token, get_current_user, verify_password\n",
    "from .crud import get_user_by_username\n",
    "from .schemas.iris import IrisPredictRequest, IrisPredictResponse, IrisFeatures\n",
    "from .schemas.cancer import CancerPredictRequest, CancerPredictResponse, CancerFeatures\n",
    "from .services.ml.model_service import model_service\n",
    "from .core.config import settings\n",
    "\n",
    "# â”€â”€ NEW: guarantee log directory exists â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Pydantic models\n",
    "class Payload(BaseModel):\n",
    "    count: int\n",
    "\n",
    "class PredictionRequest(BaseModel):\n",
    "    data: Payload\n",
    "\n",
    "class PredictionResponse(BaseModel):\n",
    "    prediction: str\n",
    "    confidence: float\n",
    "    input_received: Payload  # Echo back the input for verification\n",
    "\n",
    "class Token(BaseModel):\n",
    "    access_token: str\n",
    "    token_type: str\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"FastAPI + React ML App\",\n",
    "    version=\"1.0.0\",\n",
    "    docs_url=\"/api/v1/docs\",\n",
    "    redoc_url=\"/api/v1/redoc\",\n",
    "    openapi_url=\"/api/v1/openapi.json\",\n",
    "    swagger_ui_parameters={\"persistAuthorization\": True},\n",
    "    lifespan=lifespan,  # register startup/shutdown events\n",
    ")\n",
    "\n",
    "# Configure CORS with environment-based origins\n",
    "origins_env = settings.ALLOWED_ORIGINS\n",
    "origins: list[str] = [o.strip() for o in origins_env.split(\",\")] if origins_env != \"*\" else [\"*\"]\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],  # In production, replace with specific origins\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "@app.middleware(\"http\")\n",
    "async def add_process_time_header(request: Request, call_next):\n",
    "    \"\"\"Measure request time and add X-Process-Time header.\"\"\"\n",
    "    start = time.perf_counter()\n",
    "    response = await call_next(request)\n",
    "    elapsed = time.perf_counter() - start\n",
    "    response.headers[\"X-Process-Time\"] = f\"{elapsed:.4f}\"\n",
    "    return response\n",
    "\n",
    "# Health check endpoint\n",
    "@app.get(\"/api/v1/health\")\n",
    "async def health_check():\n",
    "    \"\"\"Basic health check - always returns 200 if server is running.\"\"\"\n",
    "    return {\"status\": \"healthy\", \"timestamp\": time.time()}\n",
    "\n",
    "@app.get(\"/api/v1/hello\")\n",
    "async def hello(current_user: str = Depends(get_current_user)):\n",
    "    \"\"\"Simple endpoint for token validation.\"\"\"\n",
    "    return {\"message\": f\"Hello {current_user}!\", \"status\": \"authenticated\"}\n",
    "\n",
    "@app.get(\"/api/v1/ready\")\n",
    "async def ready():\n",
    "    \"\"\"Basic readiness check.\"\"\"\n",
    "    return {\"ready\": get_app_ready()}\n",
    "\n",
    "@app.post(\"/api/v1/token\", response_model=Token)\n",
    "async def login(\n",
    "    form_data: OAuth2PasswordRequestForm = Depends(),\n",
    "    db: AsyncSession = Depends(get_db),\n",
    "):\n",
    "    \"\"\"Authenticate user and issue JWT.\"\"\"\n",
    "    if not get_app_ready():\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,\n",
    "            detail=\"Backend still loading models. Try again in a moment.\",\n",
    "            headers={\"Retry-After\": \"10\"}\n",
    "        )\n",
    "\n",
    "    user = await get_user_by_username(db, form_data.username)\n",
    "    if not user or not verify_password(form_data.password, user.hashed_password):\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_401_UNAUTHORIZED,\n",
    "            detail=\"Invalid credentials\"\n",
    "        )\n",
    "    token = create_access_token(subject=user.username)\n",
    "    return Token(access_token=token, token_type=\"bearer\")\n",
    "\n",
    "@app.get(\"/api/v1/ready/full\")\n",
    "async def ready_full() -> dict:\n",
    "    \"\"\"\n",
    "    Extended readiness probe:\n",
    "    - ready: API server is ready to accept requests (login allowed)\n",
    "    - model_status: dict of {model_name: status} where status is 'loaded'|'training'|'failed'|'missing'\n",
    "    - all_models_loaded: true if all models are in 'loaded' state\n",
    "    \"\"\"\n",
    "    # Allow login if API is ready, regardless of model status\n",
    "    ready_for_login = get_app_ready()\n",
    "\n",
    "    expected = {\"iris_random_forest\", \"breast_cancer_bayes\"}\n",
    "    loaded = set(model_service.models.keys())\n",
    "    training = set(model_service.status.keys()) - loaded\n",
    "\n",
    "    response = {\n",
    "        \"ready\": ready_for_login,  # Allow login immediately\n",
    "        \"model_status\": model_service.status,\n",
    "        \"all_models_loaded\": all(s == \"loaded\" for s in model_service.status.values()),\n",
    "        \"models\": {m: (m in loaded) for m in expected},\n",
    "        \"training\": list(training)\n",
    "    }\n",
    "\n",
    "    logger.debug(\"READY endpoint â€“ _app_ready=%s, response=%s\", get_app_ready(), response)\n",
    "    return response\n",
    "\n",
    "# ----- on-demand training endpoints ----------------------------------\n",
    "@app.post(\"/api/v1/iris/train\", status_code=202)\n",
    "async def train_iris(background_tasks: BackgroundTasks,\n",
    "                     current_user: str = Depends(get_current_user)):\n",
    "    background_tasks.add_task(model_service.train_iris)\n",
    "    return {\"status\": \"started\"}\n",
    "\n",
    "@app.post(\"/api/v1/cancer/train\", status_code=202)\n",
    "async def train_cancer(background_tasks: BackgroundTasks,\n",
    "                       current_user: str = Depends(get_current_user)):\n",
    "    background_tasks.add_task(model_service.train_cancer)\n",
    "    return {\"status\": \"started\"}\n",
    "\n",
    "@app.get(\"/api/v1/iris/ready\")\n",
    "async def iris_ready():\n",
    "    \"\"\"Check if Iris model is loaded and ready.\"\"\"\n",
    "    return {\"loaded\": \"iris_random_forest\" in model_service.models}\n",
    "\n",
    "@app.get(\"/api/v1/cancer/ready\")\n",
    "async def cancer_ready():\n",
    "    \"\"\"Check if Cancer model is loaded and ready.\"\"\"\n",
    "    return {\"loaded\": \"breast_cancer_bayes\" in model_service.models}\n",
    "\n",
    "@app.post(\n",
    "    \"/api/v1/iris/predict\",\n",
    "    response_model=IrisPredictResponse,\n",
    "    status_code=status.HTTP_200_OK\n",
    ")\n",
    "async def predict_iris(\n",
    "    request: IrisPredictRequest,\n",
    "    background_tasks: BackgroundTasks,\n",
    "    current_user: str = Depends(get_current_user),\n",
    "):\n",
    "    \"\"\"\n",
    "    Predict iris species from measurements.\n",
    "\n",
    "    Example request:\n",
    "        {\n",
    "            \"model_type\": \"rf\",\n",
    "            \"samples\": [\n",
    "                {\n",
    "                    \"sepal_length\": 5.1,\n",
    "                    \"sepal_width\": 3.5,\n",
    "                    \"petal_length\": 1.4,\n",
    "                    \"petal_width\": 0.2\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    \"\"\"\n",
    "    logger.info(f\"User {current_user} called /iris/predict with {len(request.samples)} samples\")\n",
    "    logger.debug(f\"â†’ Iris payload: {request.samples}\")\n",
    "\n",
    "    # Check if iris model is ready\n",
    "    if request.model_type == \"rf\" and \"iris_random_forest\" not in model_service.models:\n",
    "        logger.warning(\"Iris model not ready - returning 503\")\n",
    "        raise HTTPException(\n",
    "            status_code=503,\n",
    "            detail=\"Iris model is still loading. Please try again in a few seconds.\",\n",
    "            headers={\"Retry-After\": \"30\"}\n",
    "        )\n",
    "\n",
    "    # Convert Pydantic models to dicts\n",
    "    features = [sample.dict() for sample in request.samples]\n",
    "    logger.debug(f\"â†’ Iris features: {features}\")\n",
    "\n",
    "    # Get predictions\n",
    "    predictions, probabilities = await model_service.predict_iris(\n",
    "        features=features,\n",
    "        model_type=request.model_type\n",
    "    )\n",
    "    logger.debug(f\"â† Iris predictions: {predictions}\")\n",
    "    logger.debug(f\"â† Iris probabilities: {probabilities}\")\n",
    "\n",
    "    result = {\n",
    "        \"predictions\": predictions,\n",
    "        \"probabilities\": probabilities,\n",
    "        \"input_received\": request.samples\n",
    "    }\n",
    "\n",
    "    # Background task for audit logging\n",
    "    background_tasks.add_task(\n",
    "        logger.info,\n",
    "        f\"[audit] user={current_user} endpoint=iris input={request.samples} output={predictions}\"\n",
    "    )\n",
    "\n",
    "    return IrisPredictResponse(**result)\n",
    "\n",
    "@app.post(\n",
    "    \"/api/v1/cancer/predict\",\n",
    "    response_model=CancerPredictResponse,\n",
    "    status_code=status.HTTP_200_OK\n",
    ")\n",
    "async def predict_cancer(\n",
    "    request: CancerPredictRequest,\n",
    "    background_tasks: BackgroundTasks,\n",
    "    current_user: str = Depends(get_current_user),\n",
    "):\n",
    "    \"\"\"\n",
    "    Predict breast cancer diagnosis from features.\n",
    "\n",
    "    Example request:\n",
    "        {\n",
    "            \"model_type\": \"bayes\",\n",
    "            \"samples\": [\n",
    "                {\n",
    "                    \"mean_radius\": 17.99,\n",
    "                    \"mean_texture\": 10.38,\n",
    "                    ...\n",
    "                }\n",
    "            ],\n",
    "            \"posterior_samples\": 1000  # optional\n",
    "        }\n",
    "    \"\"\"\n",
    "    logger.info(f\"User {current_user} called /cancer/predict with {len(request.samples)} samples\")\n",
    "    logger.debug(f\"â†’ Cancer payload: {request.samples}\")\n",
    "\n",
    "    # Check if cancer model is ready\n",
    "    if request.model_type == \"bayes\" and \"breast_cancer_bayes\" not in model_service.models:\n",
    "        logger.warning(\"Cancer model not ready - returning 503\")\n",
    "        raise HTTPException(\n",
    "            status_code=503,\n",
    "            detail=\"Cancer model is still loading. Please try again in a few seconds.\",\n",
    "            headers={\"Retry-After\": \"30\"}\n",
    "        )\n",
    "\n",
    "    # Convert Pydantic models to dicts\n",
    "    features = [sample.dict() for sample in request.samples]\n",
    "    logger.debug(f\"â†’ Cancer features: {features}\")\n",
    "\n",
    "    # Get predictions\n",
    "    predictions, probabilities, uncertainties = await model_service.predict_cancer(\n",
    "        features=features,\n",
    "        model_type=request.model_type,\n",
    "        posterior_samples=request.posterior_samples\n",
    "    )\n",
    "    logger.debug(f\"â† Cancer predictions: {predictions}\")\n",
    "    logger.debug(f\"â† Cancer probabilities: {probabilities}\")\n",
    "    logger.debug(f\"â† Cancer uncertainties: {uncertainties}\")\n",
    "\n",
    "    result = {\n",
    "        \"predictions\": predictions,\n",
    "        \"probabilities\": probabilities,\n",
    "        \"uncertainties\": uncertainties,\n",
    "        \"input_received\": request.samples\n",
    "    }\n",
    "\n",
    "    # Background task for audit logging\n",
    "    background_tasks.add_task(\n",
    "        logger.info,\n",
    "        f\"[audit] user={current_user} endpoint=cancer input={request.samples} output={predictions}\"\n",
    "    )\n",
    "\n",
    "    return CancerPredictResponse(**result) \n",
    "\n",
    "@app.get(\"/api/v1/debug/ready\")\n",
    "async def debug_ready():\n",
    "    \"\"\"Debug endpoint to check _app_ready status.\"\"\"\n",
    "    return {\n",
    "        \"app_ready\": get_app_ready(),\n",
    "        \"model_service_initialized\": model_service.initialized,\n",
    "        \"models\": list(model_service.models.keys()),\n",
    "        \"status\": model_service.status,\n",
    "        \"errors\": {k: v for k, v in model_service.status.items() if k.endswith(\"_last_error\")}\n",
    "    } \n",
    "\n",
    "@app.get(\"/api/v1/test/401\")\n",
    "async def test_401():\n",
    "    \"\"\"Test endpoint that returns 401 for testing session expiry.\"\"\"\n",
    "    raise HTTPException(\n",
    "        status_code=status.HTTP_401_UNAUTHORIZED,\n",
    "        detail=\"Test 401 response\"\n",
    "    ) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f687bc30",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2200c56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/scripts/ensure_models.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/scripts/ensure_models.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Ensure models script - pre-trains all models before starting the API.\n",
    "This can be used in development or CI to ensure models are ready.\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "import logging\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the api directory to the Python path\n",
    "sys.path.insert(0, str(Path(__file__).parent.parent))\n",
    "\n",
    "from app.services.ml.model_service import TRAINERS, ModelService\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "async def main():\n",
    "    \"\"\"Ensure all models are trained and loaded.\"\"\"\n",
    "    logger.info(\"ðŸš€ Starting model ensure script...\")\n",
    "    \n",
    "    svc = ModelService()\n",
    "    \n",
    "    # Start the self-healing process\n",
    "    await svc.startup(auto_train=True)\n",
    "    \n",
    "    # Wait until all models are loaded\n",
    "    max_wait = 300  # 5 minutes max\n",
    "    start_time = asyncio.get_event_loop().time()\n",
    "    \n",
    "    while len(svc.models) < len(TRAINERS):\n",
    "        if asyncio.get_event_loop().time() - start_time > max_wait:\n",
    "            logger.error(\"âŒ Timeout waiting for models to load\")\n",
    "            return False\n",
    "            \n",
    "        logger.info(f\"â³ Waiting for models... ({len(svc.models)}/{len(TRAINERS)} loaded)\")\n",
    "        \n",
    "        # Check for failed models\n",
    "        failed = [name for name, status in svc.status.items() if status == \"failed\"]\n",
    "        if failed:\n",
    "            logger.error(f\"âŒ Models failed to train: {failed}\")\n",
    "            return False\n",
    "            \n",
    "        await asyncio.sleep(5)\n",
    "    \n",
    "    logger.info(\"âœ… All models loaded successfully!\")\n",
    "    return True\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        success = asyncio.run(main())\n",
    "        sys.exit(0 if success else 1)\n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"â¹ï¸  Interrupted by user\")\n",
    "        sys.exit(1)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"âŒ Unexpected error: {e}\")\n",
    "        sys.exit(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9a1f7094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/__init__.py\n",
    "# Create logs dir early when package is imported by Uvicorn workers\n",
    "import os\n",
    "os.makedirs(\"logs\", exist_ok=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "729e66b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_self_healing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_self_healing.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Test script for the self-healing model system.\n",
    "This script tests the new startup pattern and status tracking.\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import pathlib\n",
    "from typing import Dict, Any\n",
    "\n",
    "def start_backend():\n",
    "    \"\"\"\n",
    "    Launch uvicorn in a subprocess, stream its output in real time and\n",
    "    fail fast if it crashes or if port 8000 is already taken.\n",
    "    \"\"\"\n",
    "    import socket, threading, os, pathlib, sys, subprocess, time, shutil\n",
    "\n",
    "    # â”€â”€ quick port-availability probe â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n",
    "        if sock.connect_ex((\"127.0.0.1\", 8000)) == 0:\n",
    "            raise RuntimeError(\"Port 8000 already in use â€“ aborting tests\")\n",
    "\n",
    "    print(\"ðŸš€  Spawning backend â€¦\")\n",
    "\n",
    "    uvicorn_cmd = [\n",
    "        sys.executable, \"-m\", \"uvicorn\",\n",
    "        \"api.app.main:app\",\n",
    "        \"--port\", \"8000\",\n",
    "        \"--env-file\", \".env\",\n",
    "        \"--log-level\", \"info\",\n",
    "    ]\n",
    "\n",
    "    env = os.environ.copy()\n",
    "    # Ensure the project root is on PYTHONPATH so 'api' is importable\n",
    "    env[\"PYTHONPATH\"] = str(pathlib.Path(__file__).parent) + os.pathsep + env.get(\"PYTHONPATH\", \"\")\n",
    "\n",
    "    proc = subprocess.Popen(\n",
    "        uvicorn_cmd,\n",
    "        cwd=pathlib.Path(__file__).parent,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.STDOUT,\n",
    "        text=True,\n",
    "        start_new_session=True,          # avoid Zombie children on ^C\n",
    "        env=env,\n",
    "    )\n",
    "\n",
    "    # â”€â”€ stream output in background thread â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    def _pump(pipe):\n",
    "        for ln in iter(pipe.readline, \"\"):\n",
    "            print(\"[backend]\", ln.rstrip())\n",
    "\n",
    "    t = threading.Thread(target=_pump, args=(proc.stdout,), daemon=True)\n",
    "    t.start()\n",
    "\n",
    "    deadline = time.time() + 60\n",
    "    while time.time() < deadline:\n",
    "        if proc.poll() is not None:\n",
    "            raise RuntimeError(\"Backend process exited early â€“ see above log\")\n",
    "\n",
    "        try:\n",
    "            import requests\n",
    "            r = requests.get(\"http://127.0.0.1:8000/api/v1/health\", timeout=1)\n",
    "            if r.status_code == 200:\n",
    "                print(\"âœ… Backend responded to /health\")\n",
    "                return proc\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            pass\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "    proc.terminate()\n",
    "    raise RuntimeError(\"Backend did not become healthy within 60 seconds\")\n",
    "\n",
    "def cleanup_mlruns():\n",
    "    \"\"\"Remove mlruns directory to simulate fresh start.\"\"\"\n",
    "    mlruns_path = \"mlruns\"\n",
    "    if os.path.exists(mlruns_path):\n",
    "        print(\"ðŸ§¹ Cleaning up mlruns directory...\")\n",
    "        shutil.rmtree(mlruns_path)\n",
    "        print(\"âœ… Cleanup complete\")\n",
    "\n",
    "def test_backend_startup():\n",
    "    \"\"\"Test that the backend starts up immediately.\"\"\"\n",
    "    print(\"ðŸ” Testing backend startup...\")\n",
    "\n",
    "    # Wait for backend to be ready\n",
    "    max_wait = 30\n",
    "    start_time = time.time()\n",
    "\n",
    "    while time.time() - start_time < max_wait:\n",
    "        try:\n",
    "            # Test basic health\n",
    "            health_response = requests.get(\"http://localhost:8000/api/v1/health\")\n",
    "            if health_response.status_code == 200:\n",
    "                print(\"âœ… Backend health check passed\")\n",
    "                break\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            print(\"â³ Waiting for backend to start...\")\n",
    "            time.sleep(2)\n",
    "    else:\n",
    "        print(\"âŒ Backend failed to start within 30 seconds\")\n",
    "        return False\n",
    "\n",
    "    # Test readiness endpoint - should be ready immediately\n",
    "    try:\n",
    "        ready_response = requests.get(\"http://localhost:8000/api/v1/ready\")\n",
    "        if ready_response.status_code == 200:\n",
    "            ready_data = ready_response.json()\n",
    "            if ready_data.get(\"ready\"):\n",
    "                print(\"âœ… Backend is ready for requests immediately\")\n",
    "            else:\n",
    "                print(\"âŒ Backend not ready\")\n",
    "                return False\n",
    "        else:\n",
    "            print(f\"âŒ Readiness endpoint failed: {ready_response.status_code}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error testing readiness: {e}\")\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def test_model_status_evolution():\n",
    "    \"\"\"Test that model status evolves correctly over time.\"\"\"\n",
    "    print(\"\\nðŸ” Testing model status evolution...\")\n",
    "\n",
    "    status_history = []\n",
    "    max_polls = 30  # Poll for up to 60 seconds\n",
    "\n",
    "    for i in range(max_polls):\n",
    "        try:\n",
    "            response = requests.get(\"http://localhost:8000/api/v1/ready/full\")\n",
    "            if response.status_code == 200:\n",
    "                status_data = response.json()\n",
    "                status_history.append(status_data)\n",
    "\n",
    "                print(f\"Poll {i+1}: Ready={status_data.get('ready')}, \"\n",
    "                      f\"All loaded={status_data.get('all_models_loaded')}\")\n",
    "\n",
    "                # Show individual model status\n",
    "                model_status = status_data.get('model_status', {})\n",
    "                for model, status in model_status.items():\n",
    "                    print(f\"  {model}: {status}\")\n",
    "\n",
    "                # Check if all models are loaded\n",
    "                if status_data.get('all_models_loaded'):\n",
    "                    print(\"âœ… All models loaded successfully!\")\n",
    "                    return True\n",
    "\n",
    "            time.sleep(2)\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error polling status: {e}\")\n",
    "            time.sleep(2)\n",
    "\n",
    "    print(\"âŒ Models did not load within expected time\")\n",
    "    return False\n",
    "\n",
    "def test_login_immediate():\n",
    "    \"\"\"Test that login works immediately even when models are training.\"\"\"\n",
    "    print(\"\\nðŸ” Testing immediate login capability...\")\n",
    "\n",
    "    try:\n",
    "        # Try to get a token immediately after startup\n",
    "        token_response = requests.post(\n",
    "            \"http://localhost:8000/api/v1/token\",\n",
    "            data={\"username\": \"alice\", \"password\": \"supersecretvalue\"}\n",
    "        )\n",
    "\n",
    "        if token_response.status_code == 200:\n",
    "            token_data = token_response.json()\n",
    "            print(\"âœ… Login successful immediately after startup\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"âŒ Login failed: {token_response.status_code}\")\n",
    "            return False\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error testing login: {e}\")\n",
    "        return False\n",
    "\n",
    "def test_prediction_with_training_models():\n",
    "    \"\"\"Test prediction behavior when models are still training.\"\"\"\n",
    "    print(\"\\nðŸ” Testing prediction behavior during training...\")\n",
    "\n",
    "    try:\n",
    "        # Get a token\n",
    "        token_response = requests.post(\n",
    "            \"http://localhost:8000/api/v1/token\",\n",
    "            data={\"username\": \"alice\", \"password\": \"supersecretvalue\"}\n",
    "        )\n",
    "\n",
    "        if token_response.status_code != 200:\n",
    "            print(\"âŒ Failed to get authentication token\")\n",
    "            return False\n",
    "\n",
    "        token_data = token_response.json()\n",
    "        headers = {\"Authorization\": f\"Bearer {token_data['access_token']}\"}\n",
    "\n",
    "        # Try iris prediction (should work if model is loaded)\n",
    "        iris_data = {\n",
    "            \"model_type\": \"rf\",\n",
    "            \"samples\": [{\n",
    "                \"sepal_length\": 5.1,\n",
    "                \"sepal_width\": 3.5,\n",
    "                \"petal_length\": 1.4,\n",
    "                \"petal_width\": 0.2\n",
    "            }]\n",
    "        }\n",
    "\n",
    "        iris_response = requests.post(\n",
    "            \"http://localhost:8000/api/v1/iris/predict\",\n",
    "            json=iris_data,\n",
    "            headers=headers\n",
    "        )\n",
    "\n",
    "        if iris_response.status_code == 200:\n",
    "            iris_result = iris_response.json()\n",
    "            print(f\"âœ… Iris prediction successful: {iris_result['predictions']}\")\n",
    "        elif iris_response.status_code == 503:\n",
    "            print(\"âœ… Iris prediction correctly rejected (model still training)\")\n",
    "        else:\n",
    "            print(f\"âŒ Unexpected iris response: {iris_response.status_code}\")\n",
    "            return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error testing predictions: {e}\")\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run all tests.\"\"\"\n",
    "    print(\"ðŸš€ Starting self-healing system tests...\\n\")\n",
    "\n",
    "    # Optional: Clean up for fresh start\n",
    "    if input(\"Clean up mlruns directory for fresh start? (y/N): \").lower() == 'y':\n",
    "        cleanup_mlruns()\n",
    "\n",
    "    # ðŸ”‘ NEW: Launch backend in-process\n",
    "    backend = start_backend()\n",
    "    try:\n",
    "        # Test 1: Backend startup\n",
    "        if not test_backend_startup():\n",
    "            print(\"âŒ Backend startup test failed\")\n",
    "            return\n",
    "\n",
    "        # Test 2: Immediate login\n",
    "        if not test_login_immediate():\n",
    "            print(\"âŒ Immediate login test failed\")\n",
    "            return\n",
    "\n",
    "        # Test 3: Model status evolution\n",
    "        if not test_model_status_evolution():\n",
    "            print(\"âŒ Model status evolution test failed\")\n",
    "            return\n",
    "\n",
    "        # Test 4: Prediction behavior during training\n",
    "        if not test_prediction_with_training_models():\n",
    "            print(\"âŒ Prediction behavior test failed\")\n",
    "            return\n",
    "\n",
    "        print(\"\\nðŸŽ‰ All tests passed! The self-healing system is working correctly.\")\n",
    "        print(\"\\nðŸ“‹ Summary:\")\n",
    "        print(\"âœ… Backend starts immediately\")\n",
    "        print(\"âœ… Login works immediately\")\n",
    "        print(\"âœ… Models train in background\")\n",
    "        print(\"âœ… Status updates in real-time\")\n",
    "        print(\"âœ… Predictions work when models are ready\")\n",
    "\n",
    "    finally:\n",
    "        # Clean shutdown\n",
    "        print(\"\\nðŸ›‘ Shutting down backend...\")\n",
    "        backend.terminate()\n",
    "        backend.wait(timeout=10)\n",
    "        print(\"âœ… Backend shutdown complete\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b7b5f200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_manual.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_manual.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Manual test script for the self-healing system.\n",
    "This script assumes the backend is already running (e.g., via npm run dev).\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "def test_backend_status():\n",
    "    \"\"\"Test basic backend status.\"\"\"\n",
    "    print(\"ðŸ” Testing backend status...\")\n",
    "    \n",
    "    try:\n",
    "        # Test health endpoint\n",
    "        health_response = requests.get(\"http://localhost:8000/api/v1/health\")\n",
    "        if health_response.status_code == 200:\n",
    "            print(\"âœ… Backend health check passed\")\n",
    "        else:\n",
    "            print(f\"âŒ Backend health failed: {health_response.status_code}\")\n",
    "            return False\n",
    "            \n",
    "        # Test readiness endpoint\n",
    "        ready_response = requests.get(\"http://localhost:8000/api/v1/ready\")\n",
    "        if ready_response.status_code == 200:\n",
    "            ready_data = ready_response.json()\n",
    "            print(f\"âœ… Backend ready: {ready_data}\")\n",
    "        else:\n",
    "            print(f\"âŒ Readiness check failed: {ready_response.status_code}\")\n",
    "            return False\n",
    "            \n",
    "        return True\n",
    "        \n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"âŒ Backend not running. Start it with: npm run dev\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "        return False\n",
    "\n",
    "def test_model_status():\n",
    "    \"\"\"Test model status endpoint.\"\"\"\n",
    "    print(\"\\nðŸ” Testing model status...\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(\"http://localhost:8000/api/v1/ready/full\")\n",
    "        if response.status_code == 200:\n",
    "            status_data = response.json()\n",
    "            print(f\"âœ… Model status: {json.dumps(status_data, indent=2)}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"âŒ Model status failed: {response.status_code}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "        return False\n",
    "\n",
    "def test_login():\n",
    "    \"\"\"Test login functionality.\"\"\"\n",
    "    print(\"\\nðŸ” Testing login...\")\n",
    "    \n",
    "    try:\n",
    "        token_response = requests.post(\n",
    "            \"http://localhost:8000/api/v1/token\",\n",
    "            data={\"username\": \"alice\", \"password\": \"supersecretvalue\"}\n",
    "        )\n",
    "        \n",
    "        if token_response.status_code == 200:\n",
    "            token_data = token_response.json()\n",
    "            print(\"âœ… Login successful\")\n",
    "            return token_data['access_token']\n",
    "        else:\n",
    "            print(f\"âŒ Login failed: {token_response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "        return None\n",
    "\n",
    "def test_prediction(token):\n",
    "    \"\"\"Test prediction with authentication.\"\"\"\n",
    "    print(\"\\nðŸ” Testing prediction...\")\n",
    "    \n",
    "    if not token:\n",
    "        print(\"âŒ No token available\")\n",
    "        return False\n",
    "        \n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    \n",
    "    # Test iris prediction\n",
    "    iris_data = {\n",
    "        \"model_type\": \"rf\",\n",
    "        \"samples\": [{\n",
    "            \"sepal_length\": 5.1,\n",
    "            \"sepal_width\": 3.5,\n",
    "            \"petal_length\": 1.4,\n",
    "            \"petal_width\": 0.2\n",
    "        }]\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        iris_response = requests.post(\n",
    "            \"http://localhost:8000/api/v1/iris/predict\",\n",
    "            json=iris_data,\n",
    "            headers=headers\n",
    "        )\n",
    "        \n",
    "        if iris_response.status_code == 200:\n",
    "            iris_result = iris_response.json()\n",
    "            print(f\"âœ… Iris prediction: {iris_result['predictions']}\")\n",
    "        elif iris_response.status_code == 503:\n",
    "            print(\"âœ… Iris prediction rejected (model still training)\")\n",
    "        else:\n",
    "            print(f\"âŒ Iris prediction failed: {iris_response.status_code}\")\n",
    "            return False\n",
    "            \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run manual tests.\"\"\"\n",
    "    print(\"ðŸš€ Manual self-healing system tests\\n\")\n",
    "    print(\"Make sure the backend is running with: npm run dev\\n\")\n",
    "    \n",
    "    # Test 1: Backend status\n",
    "    if not test_backend_status():\n",
    "        print(\"\\nâŒ Backend status test failed\")\n",
    "        return\n",
    "    \n",
    "    # Test 2: Model status\n",
    "    if not test_model_status():\n",
    "        print(\"\\nâŒ Model status test failed\")\n",
    "        return\n",
    "    \n",
    "    # Test 3: Login\n",
    "    token = test_login()\n",
    "    if not token:\n",
    "        print(\"\\nâŒ Login test failed\")\n",
    "        return\n",
    "    \n",
    "    # Test 4: Prediction\n",
    "    if not test_prediction(token):\n",
    "        print(\"\\nâŒ Prediction test failed\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nðŸŽ‰ All manual tests passed!\")\n",
    "    print(\"\\nðŸ“‹ Summary:\")\n",
    "    print(\"âœ… Backend is running and responsive\")\n",
    "    print(\"âœ… Model status is being tracked\")\n",
    "    print(\"âœ… Login works with authentication\")\n",
    "    print(\"âœ… Predictions work (or are properly rejected)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a4b0bb71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_import.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_import.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Enhanced test to verify that api.app.main can be imported without errors.\n",
    "Captures detailed logs and implements unit test mode for fast imports.\n",
    "\"\"\"\n",
    "\n",
    "import importlib\n",
    "import io\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import traceback\n",
    "\n",
    "def setup_test_environment():\n",
    "    \"\"\"Configure environment for fast, safe imports.\"\"\"\n",
    "    print(\"ðŸ”§ Setting up test environment...\")\n",
    "\n",
    "    # Tell the backend we are in unit-test-mode BEFORE we touch it\n",
    "    os.environ[\"UNIT_TESTING\"] = \"1\"\n",
    "    os.environ.setdefault(\"MLFLOW_TRACKING_URI\", \"file://./mlruns_tests\")\n",
    "\n",
    "    # Ensure logs directory exists\n",
    "    os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "    print(\"âœ… Test environment configured\")\n",
    "\n",
    "def capture_import_logs():\n",
    "    \"\"\"Capture all logs during import for debugging.\"\"\"\n",
    "    print(\"ðŸ“ Setting up log capture...\")\n",
    "\n",
    "    # Create a string buffer to capture all logs\n",
    "    log_stream = io.StringIO()\n",
    "\n",
    "    # Configure logging to capture everything\n",
    "    logging.basicConfig(\n",
    "        level=logging.DEBUG,\n",
    "        handlers=[\n",
    "            logging.StreamHandler(log_stream),\n",
    "            logging.StreamHandler(sys.stdout)  # Also show in console\n",
    "        ],\n",
    "        force=True  # Override any existing config\n",
    "    )\n",
    "\n",
    "    return log_stream\n",
    "\n",
    "def test_import_with_timing():\n",
    "    \"\"\"Test that the main module can be imported with timing and detailed logs.\"\"\"\n",
    "    print(\"ðŸ” Testing api.app.main import...\")\n",
    "\n",
    "    # Capture logs during import\n",
    "    log_stream = capture_import_logs()\n",
    "\n",
    "    # Time the import\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    try:\n",
    "        # Add the current directory to Python path\n",
    "        sys.path.insert(0, os.getcwd())\n",
    "\n",
    "        # Try to import the main module\n",
    "        import api.app.main\n",
    "        dt = time.perf_counter() - t0\n",
    "\n",
    "        print(f\"âœ… api.app.main imported successfully in {dt:.3f}s\")\n",
    "\n",
    "        # Show captured logs if any\n",
    "        log_content = log_stream.getvalue()\n",
    "        if log_content.strip():\n",
    "            print(\"ðŸ“‹ Import logs:\")\n",
    "            print(log_content)\n",
    "\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        dt = time.perf_counter() - t0\n",
    "        print(f\"âŒ Import failed after {dt:.3f}s\")\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "\n",
    "        # Show captured logs\n",
    "        log_content = log_stream.getvalue()\n",
    "        if log_content.strip():\n",
    "            print(\"ðŸ“‹ Logs during failed import:\")\n",
    "            print(log_content)\n",
    "\n",
    "        print(\"ðŸ“‹ Full traceback:\")\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "def test_logs_directory():\n",
    "    \"\"\"Test that logs directory exists.\"\"\"\n",
    "    print(\"ðŸ” Testing logs directory...\")\n",
    "\n",
    "    logs_dir = \"logs\"\n",
    "    if os.path.exists(logs_dir):\n",
    "        print(f\"âœ… Logs directory exists: {logs_dir}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"âŒ Logs directory missing: {logs_dir}\")\n",
    "        return False\n",
    "\n",
    "def test_mlflow_config():\n",
    "    \"\"\"Test MLflow configuration.\"\"\"\n",
    "    print(\"ðŸ” Testing MLflow configuration...\")\n",
    "\n",
    "    tracking_uri = os.environ.get(\"MLFLOW_TRACKING_URI\", \"not set\")\n",
    "    unit_testing = os.environ.get(\"UNIT_TESTING\", \"not set\")\n",
    "\n",
    "    print(f\"âœ… MLFLOW_TRACKING_URI: {tracking_uri}\")\n",
    "    print(f\"âœ… UNIT_TESTING: {unit_testing}\")\n",
    "\n",
    "    return True\n",
    "\n",
    "def test_compiler_probe():\n",
    "    \"\"\"Test compiler detection functionality.\"\"\"\n",
    "    print(\"ðŸ” Testing compiler probe...\")\n",
    "\n",
    "    try:\n",
    "        from api.app.ml.utils import find_compiler, test_compiler_availability\n",
    "\n",
    "        # Test compiler availability\n",
    "        compilers = test_compiler_availability()\n",
    "        print(f\"âœ… Compiler test completed: {sum(compilers.values())}/{len(compilers)} available\")\n",
    "\n",
    "        # Test find_compiler\n",
    "        compiler_path = find_compiler()\n",
    "        if compiler_path:\n",
    "            print(f\"âœ… Found compiler: {compiler_path}\")\n",
    "        else:\n",
    "            print(\"âš ï¸ No compiler found (expected on CI or dev machines without build tools)\")\n",
    "\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Compiler probe test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run comprehensive import tests.\"\"\"\n",
    "    print(\"ðŸš€ Testing module imports with detailed diagnostics...\\n\")\n",
    "\n",
    "    success = True\n",
    "\n",
    "    # Test 1: Setup environment\n",
    "    setup_test_environment()\n",
    "\n",
    "    # Test 2: Logs directory\n",
    "    if not test_logs_directory():\n",
    "        success = False\n",
    "\n",
    "    # Test 3: MLflow config\n",
    "    if not test_mlflow_config():\n",
    "        success = False\n",
    "\n",
    "    # Test 4: Compiler probe\n",
    "    if not test_compiler_probe():\n",
    "        success = False\n",
    "\n",
    "    # Test 5: Main module import (with timing and logs)\n",
    "    if not test_import_with_timing():\n",
    "        success = False\n",
    "\n",
    "    if success:\n",
    "        print(\"\\nðŸŽ‰ All import tests passed!\")\n",
    "    else:\n",
    "        print(\"\\nâŒ Some import tests failed\")\n",
    "        sys.exit(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main() \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
