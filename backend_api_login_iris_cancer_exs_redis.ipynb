{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63246651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting package.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile package.json\n",
    "{\n",
    "  \"name\": \"fastapi-react-monorepo\",\n",
    "  \"private\": true,\n",
    "  \"scripts\": {\n",
    "    \"install:all\": \"python -m venv .venv && .venv\\\\Scripts\\\\python.exe -m pip install uv && .venv\\\\Scripts\\\\uv.exe pip install -e api && .venv\\\\Scripts\\\\python.exe -m pip install bcrypt passlib[bcrypt] python-dotenv && (cd web && npm install)\",\n",
    "    \"seed\": \".venv\\\\Scripts\\\\python.exe api/scripts/seed_user.py\",\n",
    "    \"dev\": \"concurrently -n \\\"API,WEB\\\" -c \\\"cyan,magenta\\\" \\\".venv\\\\Scripts\\\\python.exe -m uvicorn api.app.main:app --reload --env-file .env\\\" \\\"npm --prefix web run dev\\\"\",\n",
    "    \"dev:backend\": \"python api/scripts/ensure_models.py && .venv\\\\Scripts\\\\python.exe -m uvicorn api.app.main:app --reload --env-file .env\",\n",
    "    \"dev:full\": \"concurrently -n \\\"API,WEB\\\" -c \\\"cyan,magenta\\\" \\\"npm run dev:backend\\\" \\\"npm --prefix web run dev\\\"\",\n",
    "    \"backend\": \".venv\\\\Scripts\\\\python.exe -m uvicorn api.app.main:app --host 0.0.0.0 --port 8000 --env-file .env\",\n",
    "    \"backend:dev\": \".venv\\\\Scripts\\\\python.exe -m uvicorn api.app.main:app --reload --host 0.0.0.0 --port 8000 --env-file .env\",\n",
    "    \"backend:fast\": \"set SKIP_BACKGROUND_TRAINING=1 && .venv\\\\Scripts\\\\python.exe -m uvicorn api.app.main:app --host 0.0.0.0 --port 8000 --env-file .env\",\n",
    "    \"frontend\": \"npm --prefix web run dev\",\n",
    "    \"ensure:models\": \"python api/scripts/ensure_models.py\",\n",
    "    \"test:self-healing\": \"python test_self_healing.py\",\n",
    "    \"test:import\": \"python test_import.py\",\n",
    "    \"build:web\": \"npm --prefix web run build\",\n",
    "    \"debug\": \"timeout /T 3 && curl -s http://127.0.0.1:8000/api/health && echo. && curl -s -X POST -d \\\"username=alice&password=secret\\\" -H \\\"Content-Type: application/x-www-form-urlencoded\\\" http://127.0.0.1:8000/api/token\"\n",
    "  },\n",
    "  \"devDependencies\": {\n",
    "    \"concurrently\": \"^8.2.2\"\n",
    "  }\n",
    "} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17a95ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting invoke.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile invoke.yml\n",
    "# invoke.yml\n",
    "tasks:\n",
    "  dev:\n",
    "    - uv venv\n",
    "    - uv sync\n",
    "    - uvicorn api.app.main:app --reload\n",
    "  test:\n",
    "    - uv pip install pytest coverage\n",
    "    - pytest -q\n",
    "  lint:\n",
    "    - uv pip install black isort flake8\n",
    "    - black .\n",
    "    - isort .\n",
    "    - flake8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b816b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting .gitignore\n"
     ]
    }
   ],
   "source": [
    "%%writefile .gitignore\n",
    ".env\n",
    "dev.env\n",
    ".devcontainer/.env.runtime\n",
    "\n",
    "mlruns/\n",
    "mlflow_db/\n",
    "mlruns_local/\n",
    "\n",
    "node_modules/\n",
    "frontend/node_modules/\n",
    "\n",
    "archive/\n",
    ".venv\n",
    "uv.lock\n",
    "\n",
    "test_iris.json\n",
    "#.env.template\n",
    "\n",
    "# Railway CLI (never commit tokens)\n",
    ".railway/config.json\n",
    "\n",
    "archive/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edd77621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting env.template\n"
     ]
    }
   ],
   "source": [
    "%%writefile env.template\n",
    "ENV_NAME=\"react_fastapi_railway\"\n",
    "CUDA_TAG=\"12.8.0\"\n",
    "DOCKER_BUILDKIT=\"1\"\n",
    "HOST_JUPYTER_PORT=\"8890\"\n",
    "HOST_TENSORBOARD_PORT=\"6008\"\n",
    "HOST_EXPLAINER_PORT=\"8050\"\n",
    "HOST_STREAMLIT_PORT=\"8501\"\n",
    "HOST_MLFLOW_PORT=\"5000\"\n",
    "HOST_APP_PORT=\"5100\"\n",
    "HOST_BACKEND_DEV_PORT=\"5002\"\n",
    "MLFLOW_TRACKING_URI=\"http://mlflow:5000\"\n",
    "MLFLOW_VERSION=\"2.12.2\"\n",
    "PYTHON_VER=\"3.10\"\n",
    "JAX_PLATFORM_NAME=\"gpu\"\n",
    "XLA_PYTHON_CLIENT_PREALLOCATE=\"true\"\n",
    "XLA_PYTHON_CLIENT_ALLOCATOR=\"platform\"\n",
    "XLA_PYTHON_CLIENT_MEM_FRACTION=\"0.95\"\n",
    "XLA_FLAGS=\"--xla_force_host_platform_device_count=1\"\n",
    "JAX_DISABLE_JIT=\"false\"\n",
    "JAX_ENABLE_X64=\"false\"\n",
    "TF_FORCE_GPU_ALLOW_GROWTH=\"false\"\n",
    "JAX_PREALLOCATION_SIZE_LIMIT_BYTES=\"8589934592\"\n",
    "RAILWAY_TOKEN=\"78d0d32f-d203-4542-a9a9-32a331aa8c29\"\n",
    "RAILWAY_VITE_API_URL=\"https://fastapi-production-1d13.up.railway.app\"\n",
    "VITE_API_URL=http://127.0.0.1:8000/api/v1\n",
    "REACT_APP_API_URL=\"https://react-frontend-production-2805.up.railway.app\"\n",
    "SECRET_KEY=\"change-me-in-prod\"\n",
    "USERNAME_KEY=\"alice\"\n",
    "USER_PASSWORD=\"supersecretvalue\"\n",
    "DATABASE_URL=\"sqlite+aiosqlite:///./app.db\"\n",
    "RAILWAY_ENVIRONMENT=\"production\"\n",
    "RAILWAY_ENVIRONMENT_ID=\"fa10dc06-75ec-4c11-93d4-a0fde17996d0\"\n",
    "RAILWAY_ENVIRONMENT_NAME=\"production\"\n",
    "RAILWAY_PRIVATE_DOMAIN=\"empowering-appreciation.railway.internal\"\n",
    "RAILWAY_PROJECT_ID=\"fc9da558-31d6-4b28-9eda-2bbe56cc7390\"\n",
    "RAILWAY_PROJECT_NAME=\"responsible-abundance\"\n",
    "RAILWAY_SERVICE_ID=\"87c129ab-ba49-471a-88bb-853ace60180d\"\n",
    "RAILWAY_SERVICE_NAME=\"empowering-appreciation\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33b7570f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/env.template\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/env.template\n",
    "# Local development environment template\n",
    "# Copy this to .env and modify as needed\n",
    "\n",
    "# Database\n",
    "DATABASE_URL=sqlite+aiosqlite:///./app.db\n",
    "\n",
    "# Security (generate a secure key for production)\n",
    "SECRET_KEY=your-secret-key-here\n",
    "ACCESS_TOKEN_EXPIRE_MINUTES=30\n",
    "\n",
    "# CORS\n",
    "ALLOWED_ORIGINS=*\n",
    "\n",
    "# Redis Configuration (for rate limiting)\n",
    "REDIS_URL=redis://localhost:6379\n",
    "\n",
    "# Rate Limiting Configuration\n",
    "RATE_LIMIT_DEFAULT=60\n",
    "RATE_LIMIT_CANCER=30\n",
    "RATE_LIMIT_LOGIN=3\n",
    "RATE_LIMIT_TRAINING=2\n",
    "RATE_LIMIT_WINDOW=60\n",
    "RATE_LIMIT_WINDOW_LIGHT=300   # 5 minutes for light endpoint (iris/predict)\n",
    "RATE_LIMIT_LOGIN_WINDOW=20\n",
    "\n",
    "# MLflow - use local file store for development\n",
    "# Set to http://your-mlflow-server:5000 for production\n",
    "# MLflow Configuration\n",
    "MLFLOW_TRACKING_URI=file:./mlruns_local\n",
    "MLFLOW_REGISTRY_URI=file:./mlruns_local\n",
    "\n",
    "\n",
    "# Model Training Flags\n",
    "SKIP_BACKGROUND_TRAINING=0\n",
    "AUTO_TRAIN_MISSING=1\n",
    "UNIT_TESTING=0\n",
    "\n",
    "# Debug Flags (keep OFF in production)\n",
    "DEBUG_RATELIMIT=0 \n",
    "\n",
    "# JAX/XLA Configuration\n",
    "# Host has a single logical CPU device – prevents JAX allocating all cores\n",
    "XLA_FLAGS=--xla_force_host_platform_device_count=1\n",
    "\n",
    "# Force CPU backend for JAX (uncomment if GPU issues occur)\n",
    "# JAX_PLATFORM_NAME=cpu\n",
    "\n",
    "# PyTensor configuration (CPU only to avoid C++ compilation)\n",
    "PYTENSOR_FLAGS=device=cpu,floatX=float32 \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "042c4e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting logging.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile logging.yaml\n",
    "version: 1\n",
    "disable_existing_loggers: False\n",
    "formatters:\n",
    "  default: \n",
    "    format: \"[%(levelname).1s] %(asctime)s %(name)s ▶ %(message)s\"\n",
    "handlers:\n",
    "  console:\n",
    "    class: logging.StreamHandler\n",
    "    formatter: default\n",
    "  file:\n",
    "    class: logging.FileHandler\n",
    "    filename: logs/backend.log\n",
    "    formatter: default\n",
    "loggers:\n",
    "  uvicorn.error:  \n",
    "    level: INFO\n",
    "    handlers: [console, file]\n",
    "  uvicorn.access: \n",
    "    level: INFO\n",
    "    handlers: [console, file]\n",
    "  app:            \n",
    "    level: DEBUG\n",
    "    handlers: [console, file]\n",
    "    propagate: False\n",
    "  app.services.ml.model_service:\n",
    "    level: DEBUG\n",
    "    handlers: [console, file]\n",
    "    propagate: False\n",
    "root:\n",
    "  level: INFO\n",
    "  handlers: [console, file] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "333a76f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pyproject.toml\n"
     ]
    }
   ],
   "source": [
    "%%writefile pyproject.toml\n",
    "[project]\n",
    "name = \"react_fastapi_railway\"\n",
    "version = \"0.1.0\"\n",
    "description = \"Pytorch and Jax GPU docker container\"\n",
    "authors = [\n",
    "  { name = \"Geoffrey Hadfield\" },\n",
    "]\n",
    "license = \"MIT\"\n",
    "readme = \"README.md\"\n",
    "\n",
    "# ─── Restrict to Python 3.10–3.12 ──────────────────────────────\n",
    "requires-python = \">=3.10,<3.13\"\n",
    "\n",
    "dependencies = [\n",
    "  # Core web framework\n",
    "  \"fastapi>=0.104.0\",\n",
    "  \"uvicorn[standard]>=0.24.0\",\n",
    "  \"python-dotenv>=1.0.0\",\n",
    "\n",
    "  # Settings and validation\n",
    "  \"pydantic>=2.0.0\",\n",
    "  \"pydantic-settings>=2.0.0\",\n",
    "\n",
    "  # HTTP client and multipart parsing\n",
    "  \"httpx>=0.24.0\",\n",
    "  \"python-multipart>=0.0.6\",\n",
    "\n",
    "  # Data & ML basics\n",
    "  \"numpy>=1.24.0\",\n",
    "  \"pandas>=2.1.0\",\n",
    "  \"scikit-learn>=1.3.0\",\n",
    "  \"mlflow>=2.8.0\",\n",
    "\n",
    "  # (Your existing extras—keep if you still need them)\n",
    "  \"matplotlib>=3.4.0\",\n",
    "  \"pymc>=5.0.0\",\n",
    "  \"arviz>=0.14.0\",\n",
    "  \"statsmodels>=0.13.0\",\n",
    "  \"jupyterlab>=3.0.0\",\n",
    "  \"seaborn>=0.11.0\",\n",
    "  \"tabulate>=0.9.0\",\n",
    "  \"shap>=0.40.0\",\n",
    "  \"xgboost>=1.5.0\",\n",
    "  \"lightgbm>=3.3.0\",\n",
    "  \"catboost>=1.2.8,<1.3.0\",\n",
    "  \"scipy>=1.7.0\",\n",
    "  \"shapash[report]>=2.3.0\",\n",
    "  \"shapiq>=0.1.0\",\n",
    "  \"explainerdashboard==0.5.1\",\n",
    "  \"ipywidgets>=8.0.0\",\n",
    "  \"nutpie>=0.7.1\",\n",
    "  \"numpyro>=0.18.0,<1.0.0\",\n",
    "  \"jax==0.6.0\",\n",
    "  \"jaxlib==0.6.0\",\n",
    "  \"pytensor>=2.18.3\",\n",
    "  \"aesara>=2.9.4\",\n",
    "  \"tqdm>=4.67.0\",\n",
    "  \"pyarrow>=12.0.0\",\n",
    "  \"optuna>=3.0.0\",\n",
    "  \"optuna-integration[mlflow]>=0.2.0\",\n",
    "  \"omegaconf>=2.3.0,<2.4.0\",\n",
    "  \"hydra-core>=1.3.2,<1.4.0\",\n",
    "  \"aiosqlite>=0.19.0\", \n",
    "  \"python-jose[cryptography]>=3.3.0\",\n",
    "  \"passlib[bcrypt]>=1.7.4\",\n",
    "  \"bcrypt==4.0.1\",  # Pin bcrypt version to resolve warning\n",
    "  # Rate limiting\n",
    "  \"fastapi-limiter>=0.1.5\",\n",
    "  \"aioredis>=2.0.0\",\n",
    "  \"httpx>=0.24.0\",\n",
    "]\n",
    "\n",
    "[project.optional-dependencies]\n",
    "dev = [\n",
    "  \"pytest>=7.0.0\",\n",
    "  \"black>=23.0.0\",\n",
    "  \"isort>=5.0.0\",\n",
    "  \"flake8>=5.0.0\",\n",
    "  \"mypy>=1.0.0\",\n",
    "  \"invoke>=2.2\",\n",
    "]\n",
    "\n",
    "cuda = [\n",
    "  \"cupy-cuda12x>=12.0.0\",\n",
    "]\n",
    "\n",
    "[tool.pytensor]\n",
    "device    = \"cuda\"\n",
    "floatX    = \"float32\"\n",
    "allow_gc  = true\n",
    "optimizer = \"fast_run\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cfd1084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/pyproject.toml\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/pyproject.toml\n",
    "[project]\n",
    "name = \"api\"\n",
    "version = \"1.0.0\"\n",
    "description = \"FastAPI backend with React frontend\"\n",
    "requires-python = \">=3.8\"\n",
    "dependencies = [\n",
    "    \"fastapi>=0.104.0\",\n",
    "    \"uvicorn>=0.24.0\",\n",
    "    \"sqlalchemy>=2.0.23\",\n",
    "    \"aiosqlite>=0.19.0\",\n",
    "    \"python-jose[cryptography]>=3.3.0\",\n",
    "    \"passlib[bcrypt]>=1.7.4\",\n",
    "    \"python-multipart>=0.0.6\",\n",
    "    \"pydantic>=2.4.2\",\n",
    "    \"bcrypt==4.0.1\",  # Pin bcrypt version to resolve warning\n",
    "    # Rate limiting\n",
    "    \"fastapi-limiter>=0.1.5\",\n",
    "    \"aioredis>=2.0.0\",\n",
    "    \"httpx>=0.24.0\",\n",
    "    # ML dependencies\n",
    "    \"mlflow>=2.8.0\",\n",
    "    \"scikit-learn>=1.3.0\",\n",
    "    \"pandas>=2.0.0\",\n",
    "    \"numpy>=1.24.0\",\n",
    "    \"pymc>=5.7.0\",\n",
    "    \"arviz>=0.15.0\",\n",
    "    \"requests>=2.31.0\",\n",
    "    \"jax[cpu]>=0.5.3,<0.7\",\n",
    "    \"jaxlib>=0.5.3,<0.7\",\n",
    "    \"numpyro>=0.14.1,<0.16\"\n",
    "]\n",
    "\n",
    "[project.optional-dependencies]\n",
    "dev = [\n",
    "    \"pytest>=7.0.0\",\n",
    "    \"pytest-asyncio>=0.21.0\",\n",
    "    \"httpx>=0.24.0\"\n",
    "]\n",
    "\n",
    "[build-system]\n",
    "requires = [\"hatchling\"]\n",
    "build-backend = \"hatchling.build\"\n",
    "\n",
    "[tool.hatch.build.targets.wheel]\n",
    "packages = [\"app\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12f1f765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/railway.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/railway.json\n",
    "{\n",
    "  \"$schema\": \"https://railway.app/railway.schema.json\",\n",
    "  \"build\": { \"builder\": \"NIXPACKS\" },\n",
    "  \"deploy\": {\n",
    "    \"startCommand\": \"bash ./start.sh\",\n",
    "    \"healthcheckPath\": \"/api/v1/health\",\n",
    "    \"healthcheckInterval\": 10,\n",
    "    \"healthcheckTimeout\": 300,\n",
    "    \"restartPolicyType\": \"ON_FAILURE\",\n",
    "    \"restartPolicyMaxRetries\": 10\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e68f0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/start.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/start.sh\n",
    "#!/usr/bin/env bash\n",
    "set -euo pipefail\n",
    "\n",
    "# ── sanity ─────────────────────────────────────────────────────────\n",
    "if [[ -z \"${PORT:-}\" ]]; then\n",
    "  echo \"❌  PORT not set – Railway always provides it.\" >&2\n",
    "  exit 1\n",
    "fi\n",
    "\n",
    "if [[ -z \"${SECRET_KEY:-}\" ]]; then\n",
    "  echo \"❌  SECRET_KEY is not set for the backend service – aborting.\" >&2\n",
    "  exit 1\n",
    "fi\n",
    "\n",
    "echo \"🚀  FastAPI boot; PORT=$PORT  PY=$(python -V)\"\n",
    "env | grep -E 'RAILWAY_|PORT|DATABASE_URL' | sed 's/SECRET_KEY=.*/SECRET_KEY=***/'\n",
    "\n",
    "# ── optional local .env ------------------------------------------------------\n",
    "[[ -f .env ]] && export $(grep -Ev '^#' .env | xargs)\n",
    "\n",
    "# ── one-shot DB migrate + seed (blocks until done) ---------------------------\n",
    "python -m scripts.seed_user\n",
    "\n",
    "# ── run the API --------------------------------------------------------------\n",
    "exec uvicorn app.main:app \\\n",
    "  --host 0.0.0.0 --port \"$PORT\" \\\n",
    "  --proxy-headers --forwarded-allow-ips=\"*\" --log-level info\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2dcbfaca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/scripts/seed_user.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/scripts/seed_user.py\n",
    "from pathlib import Path\n",
    "from passlib.context import CryptContext\n",
    "from sqlalchemy.ext.asyncio import create_async_engine, async_sessionmaker\n",
    "from sqlalchemy import select\n",
    "import os, asyncio\n",
    "\n",
    "# ── optional .env load (UTF-8 only) ───────────────────────────────\n",
    "ENV_PATH = Path(__file__).resolve().parents[2] / \".env\"\n",
    "if ENV_PATH.exists():\n",
    "    try:\n",
    "        from dotenv import load_dotenv\n",
    "        load_dotenv(ENV_PATH, encoding=\"utf-8\")\n",
    "    except UnicodeDecodeError:\n",
    "        print(\"⚠️  .env not UTF-8 – skipped\")\n",
    "\n",
    "# ── model import (kept same) ──────────────────────────────────────\n",
    "import sys; sys.path.append(str(Path(__file__).resolve().parents[1]))\n",
    "from app.models import Base, User\n",
    "\n",
    "USERNAME = os.getenv(\"USERNAME_KEY\", \"alice\")\n",
    "PASSWORD = os.getenv(\"USER_PASSWORD\", \"supersecretvalue\")\n",
    "\n",
    "pwd = CryptContext(schemes=[\"bcrypt\"], deprecated=\"auto\")\n",
    "engine = create_async_engine(\"sqlite+aiosqlite:///./app.db\")\n",
    "session_factory = async_sessionmaker(engine, expire_on_commit=False)\n",
    "\n",
    "async def main():\n",
    "    async with engine.begin() as conn:\n",
    "        await conn.run_sync(Base.metadata.create_all)\n",
    "\n",
    "    async with session_factory() as db:\n",
    "        result = await db.execute(select(User).where(User.username == USERNAME))\n",
    "        user = result.scalar_one_or_none()\n",
    "        hashed = pwd.hash(PASSWORD)\n",
    "\n",
    "        if user:\n",
    "            user.hashed_password = hashed\n",
    "            action = \"Updated\"\n",
    "        else:\n",
    "            db.add(User(username=USERNAME, hashed_password=hashed))\n",
    "            action = \"Created\"\n",
    "        await db.commit()\n",
    "        print(f\"{action} user {USERNAME}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "850e919f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/app/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/app/__init__.py\n",
    "# api/app/utils/__init__.py\n",
    "\"\"\"\n",
    "Utility functions for the FastAPI application.\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "548cf2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/app/utils/env_sanitizer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/app/utils/env_sanitizer.py\n",
    "# api/app/utils/env_sanitizer.py\n",
    "\"\"\"\n",
    "Early‑process clean‑up of env variables that mis‑configure JAX / PyTensor.\n",
    "Import *before* anything touches JAX / PyMC.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, logging, importlib.util\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "_VALID_XLA_PREFIXES = (\"--xla_\", \"--mmap_\", \"--tfrt_\")\n",
    "\n",
    "def _clean_xla_flags() -> None:\n",
    "    \"\"\"Remove invalid XLA_FLAGS tokens that cause crashes.\"\"\"\n",
    "    val = os.getenv(\"XLA_FLAGS\")\n",
    "    if not val:\n",
    "        return\n",
    "    tokens = [t for t in val.split() if t]\n",
    "    bad = [t for t in tokens if not t.startswith(_VALID_XLA_PREFIXES)]\n",
    "    if bad:\n",
    "        log.warning(\"🧹 Removing invalid XLA_FLAGS tokens: %s\", bad)\n",
    "        tokens = [t for t in tokens if t not in bad]\n",
    "    if tokens:\n",
    "        os.environ[\"XLA_FLAGS\"] = \" \".join(tokens)\n",
    "    else:        # was just '--'\n",
    "        os.environ.pop(\"XLA_FLAGS\", None)\n",
    "\n",
    "def _downgrade_jax_backend() -> None:\n",
    "    \"\"\"Force JAX to use CPU if GPU is requested but not available.\"\"\"\n",
    "    # Check if GPU backend is explicitly requested\n",
    "    platform_name = os.getenv(\"JAX_PLATFORM_NAME\", \"\").lower()\n",
    "    if platform_name in (\"gpu\", \"cuda\"):\n",
    "        # Check if CUDA runtime is actually available\n",
    "        cuda_spec = importlib.util.find_spec(\"jaxlib.cuda_extension\")\n",
    "        if cuda_spec is None:\n",
    "            log.warning(\"⚠️ No CUDA runtime found – forcing JAX_PLATFORM_NAME=cpu\")\n",
    "            os.environ[\"JAX_PLATFORM_NAME\"] = \"cpu\"\n",
    "        else:\n",
    "            log.info(\"✅ CUDA runtime detected, keeping GPU backend\")\n",
    "\n",
    "def _force_pytensor_cpu() -> None:\n",
    "    \"\"\"Force PyTensor to use CPU device to avoid C++ compilation issues.\"\"\"\n",
    "    # Only set if not already configured\n",
    "    if \"PYTENSOR_FLAGS\" not in os.environ:\n",
    "        os.environ[\"PYTENSOR_FLAGS\"] = \"device=cpu,floatX=float32\"\n",
    "        log.info(\"🔧 Set PyTensor to CPU device\")\n",
    "    \n",
    "    # Also set legacy config for compatibility\n",
    "    if \"DEVICE\" not in os.environ:\n",
    "        os.environ[\"DEVICE\"] = \"cpu\"\n",
    "\n",
    "def _disable_pytensor_compilation() -> None:\n",
    "    \"\"\"Completely disable PyTensor C compilation to avoid MSVC issues.\"\"\"\n",
    "    # Force PyTensor to use Python backend instead of C compilation\n",
    "    os.environ[\"PYTENSOR_FLAGS\"] = \"device=cpu,floatX=float32\"\n",
    "    \n",
    "    # Disable C compilation entirely\n",
    "    os.environ[\"PYTENSOR_COMPILE_OPTIMIZER\"] = \"fast_compile\"\n",
    "    os.environ[\"PYTENSOR_COMPILE_MODE\"] = \"FAST_COMPILE\"\n",
    "    \n",
    "    # Force Python backend for PyTensor (no C compilation)\n",
    "    os.environ[\"PYTENSOR_LINKER\"] = \"py\"\n",
    "    \n",
    "    log.info(\"🔧 Disabled PyTensor C compilation, using Python backend\")\n",
    "\n",
    "def _check_cuda_environment() -> None:\n",
    "    \"\"\"Log CUDA-related environment variables for debugging.\"\"\"\n",
    "    cuda_vars = {k: v for k, v in os.environ.items() \n",
    "                 if 'CUDA' in k or 'GPU' in k or 'JAX' in k}\n",
    "    if cuda_vars:\n",
    "        log.info(\"🔍 CUDA/JAX environment variables: %s\", cuda_vars)\n",
    "\n",
    "def fix_ml_backends() -> None:\n",
    "    \"\"\"\n",
    "    Comprehensive fix for JAX/PyTensor backend configuration.\n",
    "    \n",
    "    This function should be called **once** at the very top of app.main\n",
    "    before any JAX or PyMC imports.\n",
    "    \"\"\"\n",
    "    log.info(\"🔧 Sanitizing ML backend configuration...\")\n",
    "    \n",
    "    _check_cuda_environment()\n",
    "    _clean_xla_flags()\n",
    "    _downgrade_jax_backend()\n",
    "    _force_pytensor_cpu()\n",
    "    _disable_pytensor_compilation()\n",
    "    \n",
    "    log.info(\"✅ ML backend sanitization complete\")\n",
    "\n",
    "# Legacy function for backward compatibility\n",
    "def fix_xla_flags() -> None:\n",
    "    \"\"\"Legacy function - now calls the comprehensive fix.\"\"\"\n",
    "    fix_ml_backends() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dda1108f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/app/middleware/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/app/middleware/__init__.py\n",
    "# Middleware package "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "887532d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/app/middleware/concurrency.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/app/middleware/concurrency.py\n",
    "\"\"\"\n",
    "Concurrency limiting middleware for heavy endpoints.\n",
    "Provides semaphore-based concurrency control to prevent resource exhaustion.\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "from starlette.middleware.base import BaseHTTPMiddleware\n",
    "from starlette.requests import Request\n",
    "from starlette.responses import Response\n",
    "from fastapi import HTTPException, status\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class ConcurrencyLimiter(BaseHTTPMiddleware):\n",
    "    \"\"\"\n",
    "    Middleware that limits concurrent requests to heavy endpoints.\n",
    "    \n",
    "    This is useful for CPU-intensive operations like Bayesian inference\n",
    "    that could overwhelm the server if too many requests are processed simultaneously.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, app, max_concurrent: int = 4, heavy_endpoints: set = None):\n",
    "        super().__init__(app)\n",
    "        self._sem = asyncio.Semaphore(max_concurrent)\n",
    "        self.heavy_endpoints = heavy_endpoints or {\n",
    "            \"/api/v1/cancer/predict\",\n",
    "            \"/api/v1/iris/train\", \n",
    "            \"/api/v1/cancer/train\"\n",
    "        }\n",
    "        logger.info(f\"Concurrency limiter initialized with max {max_concurrent} concurrent requests\")\n",
    "\n",
    "    async def dispatch(self, request: Request, call_next) -> Response:\n",
    "        \"\"\"Process request with concurrency limiting for heavy endpoints.\"\"\"\n",
    "        path = request.url.path\n",
    "        \n",
    "        # Only apply concurrency limiting to heavy endpoints\n",
    "        if path in self.heavy_endpoints:\n",
    "            try:\n",
    "                async with self._sem:\n",
    "                    logger.debug(f\"Processing heavy endpoint {path} with concurrency control\")\n",
    "                    return await call_next(request)\n",
    "            except asyncio.TimeoutError:\n",
    "                logger.warning(f\"Concurrency timeout for {path}\")\n",
    "                raise HTTPException(\n",
    "                    status_code=status.HTTP_503_SERVICE_UNAVAILABLE,\n",
    "                    detail=\"Server is busy processing other requests. Please try again in a moment.\",\n",
    "                    headers={\"Retry-After\": \"30\"}\n",
    "                )\n",
    "        else:\n",
    "            # Light endpoints bypass concurrency control\n",
    "            return await call_next(request) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d4377fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/app/deps/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/app/deps/__init__.py\n",
    "# Rate limiting dependencies package "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0186617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/app/deps/limits.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/app/deps/limits.py\n",
    "\"\"\"\n",
    "Rate limiting dependencies for FastAPI endpoints.\n",
    "✅ FIX: identifier must be async (fastapi-limiter expects await).\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from fastapi_limiter.depends import RateLimiter\n",
    "from starlette.requests import Request\n",
    "from ..core.config import settings\n",
    "\n",
    "# ───────────────────────── helpers ──────────────────────────\n",
    "async def _path_aware_ip(request: Request) -> str:\n",
    "    \"\"\"\n",
    "    Return `<ip>:<path>` so each endpoint has its own bucket.\n",
    "    Cheap & fully-sync, but declared *async* because fastapi-limiter\n",
    "    always awaits the identifier.\n",
    "    \"\"\"\n",
    "    forwarded = request.headers.get(\"X-Forwarded-For\")\n",
    "    ip = (forwarded.split(\",\")[0].strip() if forwarded else request.client.host)\n",
    "    return f\"{ip}:{request.scope['path']}\"\n",
    "\n",
    "async def user_or_ip(request: Request) -> str:\n",
    "    \"\"\"\n",
    "    Prefer JWT → keeps per-user buckets across NAT; otherwise fall back to IP+path.\n",
    "    Also declared async for compatibility with fastapi-limiter.\n",
    "    \"\"\"\n",
    "    auth = request.headers.get(\"Authorization\", \"\")\n",
    "    if auth.startswith(\"Bearer \"):\n",
    "        return auth[7:]\n",
    "    return await _path_aware_ip(request)\n",
    "\n",
    "# ──────────────────────── limiters ──────────────────────────\n",
    "\n",
    "default_limit = RateLimiter(\n",
    "    times=settings.RATE_LIMIT_DEFAULT,\n",
    "    seconds=settings.RATE_LIMIT_WINDOW,\n",
    "    identifier=user_or_ip,\n",
    ")\n",
    "\n",
    "light_limit = RateLimiter(                # /iris/predict\n",
    "    times=120,\n",
    "    seconds=settings.RATE_LIMIT_WINDOW_LIGHT,  # Use dedicated light window\n",
    "    identifier=user_or_ip,                # ← switched to token-based\n",
    ")\n",
    "\n",
    "heavy_limit = RateLimiter(                # /cancer/predict\n",
    "    times=settings.RATE_LIMIT_CANCER,\n",
    "    seconds=settings.RATE_LIMIT_WINDOW,\n",
    "    identifier=user_or_ip,\n",
    ")\n",
    "\n",
    "login_limit = RateLimiter(                # bad‑login attempts\n",
    "    # `times` is exclusive – allow three failures, block 4th\n",
    "    times=settings.RATE_LIMIT_LOGIN + 1,\n",
    "    seconds=settings.RATE_LIMIT_LOGIN_WINDOW,\n",
    "    identifier=_path_aware_ip,\n",
    ")\n",
    "\n",
    "training_limit = RateLimiter(             # /train endpoints\n",
    "    times=settings.RATE_LIMIT_TRAINING,\n",
    "    seconds=settings.RATE_LIMIT_WINDOW * 5,\n",
    "    identifier=user_or_ip,\n",
    ")\n",
    "\n",
    "# Handy handle for debug & CI\n",
    "def get_redis():\n",
    "    from fastapi_limiter import FastAPILimiter as _L\n",
    "    return _L.redis \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a5b7392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/app/core/config.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/app/core/config.py\n",
    "\"\"\"\n",
    "Core configuration settings for the FastAPI application.\n",
    "Centralizes environment variables and provides sensible defaults.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "class Settings:\n",
    "    \"\"\"Application settings with environment-based configuration.\"\"\"\n",
    "\n",
    "    # Database\n",
    "    DATABASE_URL: str = os.getenv(\"DATABASE_URL\", \"sqlite+aiosqlite:///./app.db\")\n",
    "\n",
    "    # Security\n",
    "    SECRET_KEY: Optional[str] = os.getenv(\"SECRET_KEY\")\n",
    "    ACCESS_TOKEN_EXPIRE_MINUTES: int = int(os.getenv(\"ACCESS_TOKEN_EXPIRE_MINUTES\", \"30\"))\n",
    "\n",
    "    # CORS\n",
    "    ALLOWED_ORIGINS: str = os.getenv(\"ALLOWED_ORIGINS\", \"*\")\n",
    "\n",
    "    # Rate Limiting\n",
    "    REDIS_URL: str = os.getenv(\"REDIS_URL\", \"redis://localhost:6379\")\n",
    "    RATE_LIMIT_DEFAULT: int = int(os.getenv(\"RATE_LIMIT_DEFAULT\", \"60\"))\n",
    "    RATE_LIMIT_CANCER: int = int(os.getenv(\"RATE_LIMIT_CANCER\", \"30\"))\n",
    "    RATE_LIMIT_LOGIN: int = int(os.getenv(\"RATE_LIMIT_LOGIN\", \"3\"))\n",
    "    RATE_LIMIT_TRAINING: int = int(os.getenv(\"RATE_LIMIT_TRAINING\", \"2\"))\n",
    "    RATE_LIMIT_WINDOW: int = int(os.getenv(\"RATE_LIMIT_WINDOW\", \"60\"))  # seconds\n",
    "    RATE_LIMIT_WINDOW_LIGHT: int = int(os.getenv(\"RATE_LIMIT_WINDOW_LIGHT\", \"300\"))  # 5 minutes for light endpoint\n",
    "    RATE_LIMIT_LOGIN_WINDOW: int = int(os.getenv(\"RATE_LIMIT_LOGIN_WINDOW\", \"20\"))  # seconds\n",
    "\n",
    "    # MLflow in local-file mode by default\n",
    "    MLFLOW_TRACKING_URI: str = os.getenv(\n",
    "        \"MLFLOW_TRACKING_URI\",\n",
    "        \"file:./mlruns_local\"\n",
    "    )\n",
    "    MLFLOW_REGISTRY_URI: str = os.getenv(\n",
    "        \"MLFLOW_REGISTRY_URI\",\n",
    "        MLFLOW_TRACKING_URI\n",
    "    )\n",
    "\n",
    "    # Model training flags\n",
    "    SKIP_BACKGROUND_TRAINING: bool = os.getenv(\"SKIP_BACKGROUND_TRAINING\", \"0\") == \"1\"\n",
    "    AUTO_TRAIN_MISSING: bool = os.getenv(\"AUTO_TRAIN_MISSING\", \"1\") == \"1\"\n",
    "    UNIT_TESTING: bool = os.getenv(\"UNIT_TESTING\", \"0\") == \"1\"\n",
    "\n",
    "    # Debug flags\n",
    "    DEBUG_RATELIMIT: bool = os.getenv(\"DEBUG_RATELIMIT\", \"0\") == \"1\"\n",
    "\n",
    "settings = Settings() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49871cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/app/crud.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/app/crud.py\n",
    "from sqlalchemy import select\n",
    "from sqlalchemy.ext.asyncio import AsyncSession\n",
    "from .models import User\n",
    "\n",
    "async def get_user_by_username(db: AsyncSession, username: str):\n",
    "    stmt = select(User).where(User.username == username)\n",
    "    res = await db.execute(stmt)\n",
    "    return res.scalar_one_or_none() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61dc31d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/app/models.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/app/models.py\n",
    "from sqlalchemy import Column, Integer, String\n",
    "from sqlalchemy.orm import declarative_base\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "class User(Base):\n",
    "    __tablename__ = \"users\"\n",
    "    id = Column(Integer, primary_key=True, index=True)\n",
    "    username = Column(String, unique=True, index=True)\n",
    "    hashed_password = Column(String) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a86ef40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/app/db.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/app/db.py\n",
    "# api/app/db.py\n",
    "from contextlib import asynccontextmanager\n",
    "import os, logging, asyncio\n",
    "from sqlalchemy.ext.asyncio import (\n",
    "    AsyncSession,\n",
    "    create_async_engine,\n",
    "    async_sessionmaker,\n",
    ")\n",
    "from fastapi_limiter import FastAPILimiter\n",
    "import redis.asyncio as redis\n",
    "from .models import Base\n",
    "from .services.ml.model_service import model_service\n",
    "from .core.config import settings\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Database engine & session factory (module-level singletons – cheap & safe)\n",
    "# ---------------------------------------------------------------------------\n",
    "DATABASE_URL = os.getenv(\"DATABASE_URL\", \"sqlite+aiosqlite:///./app.db\")\n",
    "engine = create_async_engine(DATABASE_URL, echo=False, future=True)\n",
    "AsyncSessionLocal = async_sessionmaker(engine, expire_on_commit=False)\n",
    "\n",
    "# Global readiness flag\n",
    "_app_ready: bool = False\n",
    "\n",
    "def get_app_ready():\n",
    "    \"\"\"Get the current app ready status.\"\"\"\n",
    "    return _app_ready\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# FastAPI lifespan – runs ONCE at startup / shutdown\n",
    "# ---------------------------------------------------------------------------\n",
    "@asynccontextmanager\n",
    "async def lifespan(app):\n",
    "    \"\"\"\n",
    "    Application lifespan context-manager.\n",
    "\n",
    "    * creates DB tables\n",
    "    * initialises ML models\n",
    "    * (NEW) wires Redis-backed rate-limiter\n",
    "    * sets global _app_ready flag\n",
    "    * disposes resources on shutdown\n",
    "    \"\"\"\n",
    "    global _app_ready\n",
    "\n",
    "    logger.info(\"🗄️  Initializing database…  URL=%s\", DATABASE_URL)\n",
    "    try:\n",
    "        async with engine.begin() as conn:\n",
    "            # DDL is safe here; it blocks startup until complete\n",
    "            await conn.run_sync(Base.metadata.create_all)\n",
    "        logger.info(\"✅ Database tables created/verified successfully\")\n",
    "\n",
    "        # ── NEW: Initialize FastAPI-Limiter BEFORE serving traffic ──────────\n",
    "        try:\n",
    "            redis_conn = redis.from_url(\n",
    "                settings.REDIS_URL,\n",
    "                encoding=\"utf-8\",\n",
    "                decode_responses=True,\n",
    "            )\n",
    "            await FastAPILimiter.init(redis_conn, prefix=\"ratelimit\")\n",
    "            logger.info(\"🚦 Rate-limiter initialised (Redis %s)\", settings.REDIS_URL)\n",
    "            \n",
    "            # Optional: clean slate for CI\n",
    "            if os.getenv(\"FLUSH_TEST_LIMITS\") == \"1\":\n",
    "                try:\n",
    "                    flushed = await redis_conn.flushdb()\n",
    "                    logger.info(\"🧹 Redis FLUSHDB executed for test run, status=%s\", flushed)\n",
    "                except Exception as e:\n",
    "                    logger.warning(\"Could not flush Redis in test mode: %s\", e)\n",
    "        except Exception as e:\n",
    "            logger.warning(\"⚠️  Rate-limiter init failed: %s – continuing without limits\", e)\n",
    "\n",
    "        # Initialize application readiness\n",
    "        logger.info(\"🚀 Startup event starting - _app_ready=%s\", _app_ready)\n",
    "\n",
    "        if settings.UNIT_TESTING:\n",
    "            logger.info(\"🔒 UNIT_TESTING=1 – startup hooks bypassed\")\n",
    "            _app_ready = True\n",
    "            logger.info(\"✅ _app_ready set to True (unit testing)\")\n",
    "        else:\n",
    "            try:\n",
    "                # Initialize ModelService first\n",
    "                logger.info(\"🔧 Initializing ModelService\")\n",
    "                await model_service.initialize()\n",
    "                logger.info(\"✅ ModelService initialized successfully\")\n",
    "\n",
    "                # Start background training tasks\n",
    "                logger.info(\"🔄 Starting background training tasks\")\n",
    "                asyncio.create_task(model_service.startup())\n",
    "                logger.info(\"✅ Background training tasks started\")\n",
    "\n",
    "                # Set ready to true after initialization (models will load in background)\n",
    "                _app_ready = True\n",
    "                logger.info(\"🚀 FastAPI ready – _app_ready=%s, health probes will pass immediately\", _app_ready)\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(\"❌ Startup event failed: %s\", e)\n",
    "                import traceback\n",
    "                logger.error(\"❌ Startup traceback: %s\", traceback.format_exc())\n",
    "                # Set ready to true anyway so the API can serve requests\n",
    "                _app_ready = True\n",
    "                logger.warning(\"⚠️  Setting _app_ready=True despite startup errors\")\n",
    "\n",
    "        logger.info(\"🎯 Lifespan startup complete - _app_ready=%s\", _app_ready)\n",
    "        yield\n",
    "    finally:\n",
    "        logger.info(\"🔒 Shutting down…\")\n",
    "        try:\n",
    "            await FastAPILimiter.close()           # NEW – graceful shutdown\n",
    "        except Exception:\n",
    "            pass\n",
    "        await engine.dispose()\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Dependency injection helper\n",
    "# ---------------------------------------------------------------------------\n",
    "async def get_db() -> AsyncSession:\n",
    "    \"\"\"Yield a new DB session per request.\"\"\"\n",
    "    async with AsyncSessionLocal() as session:\n",
    "        yield session\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9fe2096b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/app/security.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/app/security.py\n",
    "from __future__ import annotations\n",
    "import os, logging, secrets\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Optional\n",
    "\n",
    "from fastapi import Depends, HTTPException, status, Request\n",
    "from fastapi.security import OAuth2PasswordBearer, OAuth2PasswordRequestForm\n",
    "from jose import jwt, JWTError\n",
    "from passlib.context import CryptContext\n",
    "from pydantic import BaseModel\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 1.  SECRET_KEY ***must*** be provided in the environment in production.\n",
    "# ---------------------------------------------------------------------------\n",
    "SECRET_KEY = os.getenv(\"SECRET_KEY\")\n",
    "if not SECRET_KEY:\n",
    "    log.critical(\n",
    "        \"ENV variable SECRET_KEY is missing -- generating a temporary key. \"\n",
    "        \"ALL issued JWTs will be invalid after a pod restart! \"\n",
    "        \"Set it in Railway → Variables to disable this warning.\"\n",
    "    )\n",
    "    SECRET_KEY = secrets.token_urlsafe(32)   # fallback only for dev\n",
    "\n",
    "ALGORITHM = \"HS256\"\n",
    "ACCESS_TOKEN_EXPIRE_MINUTES = int(os.getenv(\"ACCESS_TOKEN_EXPIRE_MINUTES\", 30))\n",
    "\n",
    "pwd_ctx = CryptContext(schemes=[\"bcrypt\"], deprecated=\"auto\")\n",
    "oauth2_scheme = OAuth2PasswordBearer(tokenUrl=\"/api/v1/token\")\n",
    "\n",
    "class TokenData(BaseModel):\n",
    "    username: Optional[str] = None\n",
    "\n",
    "class LoginPayload(BaseModel):\n",
    "    username: str\n",
    "    password: str\n",
    "\n",
    "async def get_credentials(request: Request) -> LoginPayload:\n",
    "    \"\"\"\n",
    "    Accept either JSON **or** classic form‑encoded credentials.\n",
    "\n",
    "    Order of precedence:\n",
    "    1. If the request media‑type is JSON → parse it with Pydantic.\n",
    "    2. Else parse as form-encoded data.\n",
    "    \"\"\"\n",
    "    content_type = request.headers.get(\"content-type\", \"\")\n",
    "    \n",
    "    if content_type.startswith(\"application/json\"):\n",
    "        # JSON branch\n",
    "        try:\n",
    "            body = await request.json()\n",
    "            return LoginPayload(**body)\n",
    "        except Exception as e:\n",
    "            raise HTTPException(\n",
    "                status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n",
    "                detail=f\"Invalid JSON credentials: {e}\",\n",
    "            )\n",
    "    else:\n",
    "        # Form-encoded branch\n",
    "        try:\n",
    "            form_data = await request.form()\n",
    "            username = form_data.get(\"username\")\n",
    "            password = form_data.get(\"password\")\n",
    "            \n",
    "            if not username or not password:\n",
    "                raise HTTPException(\n",
    "                    status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n",
    "                    detail=\"username and password are required\"\n",
    "                )\n",
    "            \n",
    "            return LoginPayload(username=username, password=password)\n",
    "        except Exception as e:\n",
    "            raise HTTPException(\n",
    "                status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n",
    "                detail=f\"Invalid form credentials: {e}\",\n",
    "            )\n",
    "\n",
    "def verify_password(raw: str, hashed: str) -> bool:\n",
    "    return pwd_ctx.verify(raw, hashed)\n",
    "\n",
    "def get_password_hash(pw: str) -> str:\n",
    "    return pwd_ctx.hash(pw)\n",
    "\n",
    "def create_access_token(subject: str) -> str:\n",
    "    expire = datetime.utcnow() + timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)\n",
    "    return jwt.encode({\"sub\": subject, \"exp\": expire}, SECRET_KEY, algorithm=ALGORITHM)\n",
    "\n",
    "async def get_current_user(token: str = Depends(oauth2_scheme)) -> str:\n",
    "    try:\n",
    "        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])\n",
    "        username: str = payload.get(\"sub\")\n",
    "        if not username:\n",
    "            raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED)\n",
    "        return username\n",
    "    except JWTError as exc:\n",
    "        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED) from exc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c6e5ee",
   "metadata": {},
   "source": [
    "# additional models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0cc65796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/app/schemas/cancer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/app/schemas/cancer.py\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "\n",
    "class CancerFeatures(BaseModel):\n",
    "    \"\"\"Breast cancer diagnostic features.\"\"\"\n",
    "    mean_radius: float = Field(..., description=\"Mean of distances from center to points on perimeter\")\n",
    "    mean_texture: float = Field(..., description=\"Standard deviation of gray-scale values\")\n",
    "    mean_perimeter: float = Field(..., description=\"Mean size of the core tumor\")\n",
    "    mean_area: float = Field(..., description=\"Mean area of the core tumor\")\n",
    "    mean_smoothness: float = Field(..., description=\"Mean of local variation in radius lengths\")\n",
    "    mean_compactness: float = Field(..., description=\"Mean of perimeter^2 / area - 1.0\")\n",
    "    mean_concavity: float = Field(..., description=\"Mean of severity of concave portions of the contour\")\n",
    "    mean_concave_points: float = Field(..., description=\"Mean for number of concave portions of the contour\")\n",
    "    mean_symmetry: float = Field(..., description=\"Mean symmetry\")\n",
    "    mean_fractal_dimension: float = Field(..., description=\"Mean for 'coastline approximation' - 1\")\n",
    "    \n",
    "    # SE features (standard error)\n",
    "    se_radius: float = Field(..., description=\"Standard error of radius\")\n",
    "    se_texture: float = Field(..., description=\"Standard error of texture\")\n",
    "    se_perimeter: float = Field(..., description=\"Standard error of perimeter\")\n",
    "    se_area: float = Field(..., description=\"Standard error of area\")\n",
    "    se_smoothness: float = Field(..., description=\"Standard error of smoothness\")\n",
    "    se_compactness: float = Field(..., description=\"Standard error of compactness\")\n",
    "    se_concavity: float = Field(..., description=\"Standard error of concavity\")\n",
    "    se_concave_points: float = Field(..., description=\"Standard error of concave points\")\n",
    "    se_symmetry: float = Field(..., description=\"Standard error of symmetry\")\n",
    "    se_fractal_dimension: float = Field(..., description=\"Standard error of fractal dimension\")\n",
    "    \n",
    "    # Worst features\n",
    "    worst_radius: float = Field(..., description=\"Worst radius\")\n",
    "    worst_texture: float = Field(..., description=\"Worst texture\")\n",
    "    worst_perimeter: float = Field(..., description=\"Worst perimeter\")\n",
    "    worst_area: float = Field(..., description=\"Worst area\")\n",
    "    worst_smoothness: float = Field(..., description=\"Worst smoothness\")\n",
    "    worst_compactness: float = Field(..., description=\"Worst compactness\")\n",
    "    worst_concavity: float = Field(..., description=\"Worst concavity\")\n",
    "    worst_concave_points: float = Field(..., description=\"Worst concave points\")\n",
    "    worst_symmetry: float = Field(..., description=\"Worst symmetry\")\n",
    "    worst_fractal_dimension: float = Field(..., description=\"Worst fractal dimension\")\n",
    "\n",
    "class CancerPredictRequest(BaseModel):\n",
    "    \"\"\"Cancer prediction request (allows 'rows' alias).\"\"\"\n",
    "    model_type: str = Field(\"bayes\", description=\"Model type: 'bayes', 'logreg', or 'rf'\")\n",
    "    samples: List[CancerFeatures] = Field(\n",
    "        ...,\n",
    "        description=\"Breast-cancer feature vectors\",\n",
    "        alias=\"rows\",\n",
    "    )\n",
    "    posterior_samples: Optional[int] = Field(\n",
    "        None, ge=10, le=10_000, description=\"Posterior draws for uncertainty\"\n",
    "    )\n",
    "\n",
    "    class Config:\n",
    "        populate_by_name = True\n",
    "        extra = \"forbid\"\n",
    "\n",
    "class CancerPredictResponse(BaseModel):\n",
    "    \"\"\"Cancer prediction response.\"\"\"\n",
    "    predictions: List[str] = Field(..., description=\"Predicted diagnosis (M=malignant, B=benign)\")\n",
    "    probabilities: List[float] = Field(..., description=\"Probability of malignancy\")\n",
    "    uncertainties: Optional[List[float]] = Field(None, description=\"Uncertainty estimates (if requested)\")\n",
    "    input_received: List[CancerFeatures] = Field(..., description=\"Echo of input features\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1fd0f1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/app/schemas/iris.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/app/schemas/iris.py\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "\n",
    "class IrisFeatures(BaseModel):\n",
    "    \"\"\"Iris measurement features.\"\"\"\n",
    "    sepal_length: float = Field(..., description=\"Sepal length in cm\", ge=0, le=10)\n",
    "    sepal_width: float = Field(..., description=\"Sepal width in cm\", ge=0, le=10)\n",
    "    petal_length: float = Field(..., description=\"Petal length in cm\", ge=0, le=10)\n",
    "    petal_width: float = Field(..., description=\"Petal width in cm\", ge=0, le=10)\n",
    "\n",
    "class IrisPredictRequest(BaseModel):\n",
    "    \"\"\"Iris prediction request (accepts legacy 'rows' alias).\"\"\"\n",
    "    model_type: str = Field(\"rf\", description=\"Model type: 'rf' or 'logreg'\")\n",
    "    samples: List[IrisFeatures] = Field(\n",
    "        ...,\n",
    "        description=\"List of iris measurements\",\n",
    "        alias=\"rows\",\n",
    "    )\n",
    "\n",
    "    class Config:\n",
    "        populate_by_name = True\n",
    "        extra = \"forbid\"\n",
    "\n",
    "class IrisPredictResponse(BaseModel):\n",
    "    \"\"\"Iris prediction response.\"\"\"\n",
    "    predictions: List[str] = Field(..., description=\"Predicted iris species\")\n",
    "    probabilities: List[List[float]] = Field(..., description=\"Class probabilities\")\n",
    "    input_received: List[IrisFeatures] = Field(..., description=\"Echo of input features\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70bc6c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/app/ml/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/app/ml/__init__.py\n",
    "\"\"\"\n",
    "ML sub-package – exposes built-in trainers so the service can import\n",
    "`app.ml.builtin_trainers` with an absolute import.\n",
    "\"\"\"\n",
    "\n",
    "from .builtin_trainers import (\n",
    "    train_iris_random_forest,\n",
    "    train_iris_logreg,\n",
    "    train_breast_cancer_bayes,\n",
    "    train_breast_cancer_stub,\n",
    ")\n",
    "\n",
    "__all__ = [\n",
    "    \"train_iris_random_forest\",\n",
    "    \"train_iris_logreg\",\n",
    "    \"train_breast_cancer_bayes\",\n",
    "    \"train_breast_cancer_stub\",\n",
    "] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "374475aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/app/ml/utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/app/ml/utils.py\n",
    "# app/ml/utils.py  (minimal, cross‑platform)\n",
    "\n",
    "def configure_pytensor_compiler(*_, **__):  # noqa: D401,E501\n",
    "    \"\"\"\n",
    "    Stub kept for backward‑compatibility.\n",
    "\n",
    "    The project now uses the **JAX backend**, so PyTensor never calls a C\n",
    "    compiler.  This function therefore does nothing and always returns True.\n",
    "    \"\"\"\n",
    "    return True \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "72a78a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/app/ml/builtin_trainers.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/app/ml/builtin_trainers.py\n",
    "# api/ml/builtin_trainers.py\n",
    "\"\"\"\n",
    "Built-in trainers for Iris RF and Breast-Cancer Bayesian LogReg.\n",
    "Executed automatically by ModelService when a model is missing.\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from pathlib import Path\n",
    "import mlflow, mlflow.sklearn, mlflow.pyfunc\n",
    "from sklearn.datasets import load_iris, load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import pickle\n",
    "import warnings\n",
    "import subprocess\n",
    "import os\n",
    "import platform\n",
    "\n",
    "# Conditional imports for heavy dependencies\n",
    "if os.getenv(\"UNIT_TESTING\") != \"1\" and os.getenv(\"SKIP_BACKGROUND_TRAINING\") != \"1\":\n",
    "    import pymc as pm\n",
    "    import arviz as az\n",
    "else:\n",
    "    pm = None\n",
    "    az = None\n",
    "\n",
    "# ── NEW: Configure MLflow to use local file storage ─────────────────────────\n",
    "# Set MLflow to use local file storage instead of remote server\n",
    "os.environ.setdefault(\"MLFLOW_TRACKING_URI\", \"file:./mlruns_local\")\n",
    "os.environ.setdefault(\"MLFLOW_REGISTRY_URI\", \"file:./mlruns_local\")\n",
    "\n",
    "# Configure MLflow tracking URI immediately\n",
    "mlflow.set_tracking_uri(\"file:./mlruns_local\")\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "MLFLOW_EXPERIMENT = \"ml_fullstack_models\"\n",
    "\n",
    "# Only set experiment if not in unit test mode and after tracking URI is set\n",
    "if os.getenv(\"UNIT_TESTING\") != \"1\":\n",
    "    try:\n",
    "        mlflow.set_experiment(MLFLOW_EXPERIMENT)\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Could not set MLflow experiment: {e}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "#  IRIS – point-estimate Random-Forest (enhanced with better parameters)\n",
    "# -----------------------------------------------------------------------------\n",
    "def train_iris_random_forest(\n",
    "    n_estimators: int = 300,\n",
    "    max_depth: int | None = None,\n",
    "    random_state: int = 42\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Train + register a Random-Forest on the Iris data and push it to MLflow.\n",
    "    Returns the run_id (string). Enhanced with better parameters and stratified split.\n",
    "    \"\"\"\n",
    "    iris = load_iris(as_frame=True)\n",
    "    X, y = iris.data, iris.target\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.25,\n",
    "                                              stratify=y, random_state=random_state)\n",
    "\n",
    "    # Enhanced Random Forest with better parameters\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1,  # Use all available cores\n",
    "        class_weight='balanced'  # Handle any class imbalance\n",
    "    ).fit(X_tr, y_tr)\n",
    "\n",
    "    preds = rf.predict(X_te)\n",
    "    metrics = {\n",
    "        \"accuracy\":  accuracy_score(y_te, preds),\n",
    "        \"f1_macro\":  f1_score(y_te, preds, average=\"macro\"),\n",
    "        \"precision_macro\": precision_score(y_te, preds, average=\"macro\"),\n",
    "        \"recall_macro\":    recall_score(y_te, preds, average=\"macro\"),\n",
    "    }\n",
    "\n",
    "    with mlflow.start_run(run_name=\"iris_random_forest\") as run:\n",
    "        # Log hyperparameters\n",
    "        mlflow.log_params({\n",
    "            \"n_estimators\": n_estimators,\n",
    "            \"max_depth\": max_depth,\n",
    "            \"random_state\": random_state\n",
    "        })\n",
    "\n",
    "        # Log metrics\n",
    "        mlflow.log_metrics(metrics)\n",
    "\n",
    "        # Create a custom pyfunc wrapper that exposes both predict and predict_proba\n",
    "        class IrisRFWrapper(mlflow.pyfunc.PythonModel):\n",
    "            def __init__(self, model):\n",
    "                self.model = model\n",
    "\n",
    "            def predict(self, model_input, params=None):\n",
    "                # Return class probabilities for pyfunc interface\n",
    "                # Convert to numpy array if it's a DataFrame\n",
    "                if hasattr(model_input, 'values'):\n",
    "                    X = model_input.values\n",
    "                else:\n",
    "                    X = model_input\n",
    "                return self.model.predict_proba(X)\n",
    "\n",
    "            def predict_proba(self, X):\n",
    "                # Expose predict_proba for direct access\n",
    "                if hasattr(X, 'values'):\n",
    "                    X = X.values\n",
    "                return self.model.predict_proba(X)\n",
    "\n",
    "            def predict_classes(self, X):\n",
    "                # Expose class prediction\n",
    "                if hasattr(X, 'values'):\n",
    "                    X = X.values\n",
    "                return self.model.predict(X)\n",
    "\n",
    "        iris_wrapper = IrisRFWrapper(rf)\n",
    "\n",
    "        # Log model with proper signature\n",
    "        mlflow.pyfunc.log_model(\n",
    "            artifact_path=\"model\",\n",
    "            python_model=iris_wrapper,\n",
    "            registered_model_name=\"iris_random_forest\",\n",
    "            input_example=X.head(),\n",
    "            signature=mlflow.models.signature.infer_signature(X, iris_wrapper.predict(X))\n",
    "        )\n",
    "        return run.info.run_id\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "#  IRIS – logistic-regression trainer (NEW)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def train_iris_logreg(\n",
    "    C: float = 1.0,\n",
    "    max_iter: int = 400,\n",
    "    random_state: int = 42,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Train and register a **multinomial Logistic Regression** model on the Iris\n",
    "    dataset.  Returns the MLflow run_id so the caller can reload the model.\n",
    "    \"\"\"\n",
    "    from sklearn.datasets import load_iris\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    # Load and split data (stratified)\n",
    "    iris = load_iris(as_frame=True)\n",
    "    X, y = iris.data, iris.target\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "        X, y, test_size=0.25, stratify=y, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Fit classifier\n",
    "    clf = LogisticRegression(\n",
    "        C=C,\n",
    "        max_iter=max_iter,\n",
    "        multi_class=\"multinomial\",\n",
    "        solver=\"lbfgs\",\n",
    "        n_jobs=-1,\n",
    "        random_state=random_state,\n",
    "    ).fit(X_tr, y_tr)\n",
    "\n",
    "    # ------------------- wrap in consistent pyfunc --------------------------\n",
    "    class IrisLogRegWrapper(mlflow.pyfunc.PythonModel):\n",
    "        \"\"\"Expose predict() as class probabilities so the service can rely on it.\"\"\"\n",
    "\n",
    "        def __init__(self, model):\n",
    "            self.model = model\n",
    "\n",
    "        def predict(self, model_input, params=None):  # noqa: D401 – MLflow signature\n",
    "            X_ = model_input.values if hasattr(model_input, \"values\") else model_input\n",
    "            return self.model.predict_proba(X_)\n",
    "\n",
    "        # Explicit alias so hasattr(model, \"predict_proba\") works post-load\n",
    "        def predict_proba(self, X):\n",
    "            X_ = X.values if hasattr(X, \"values\") else X\n",
    "            return self.model.predict_proba(X_)\n",
    "\n",
    "    preds = clf.predict(X_te)\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_te, preds),\n",
    "        \"f1_macro\": f1_score(y_te, preds, average=\"macro\"),\n",
    "        \"precision_macro\": precision_score(y_te, preds, average=\"macro\"),\n",
    "        \"recall_macro\": recall_score(y_te, preds, average=\"macro\"),\n",
    "    }\n",
    "\n",
    "    with mlflow.start_run(run_name=\"iris_logreg\") as run:\n",
    "        mlflow.log_params({\"C\": C, \"max_iter\": max_iter, \"random_state\": random_state})\n",
    "        mlflow.log_metrics(metrics)\n",
    "\n",
    "        mlflow.pyfunc.log_model(\n",
    "            artifact_path=\"model\",\n",
    "            python_model=IrisLogRegWrapper(clf),\n",
    "            registered_model_name=\"iris_logreg\",\n",
    "            input_example=X.head(),\n",
    "            signature=mlflow.models.signature.infer_signature(X, clf.predict_proba(X)),\n",
    "        )\n",
    "        return run.info.run_id\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "#  BREAST-CANCER STUB – ultra-fast fallback model\n",
    "# -----------------------------------------------------------------------------\n",
    "def train_breast_cancer_stub(random_state: int = 42) -> str:\n",
    "    \"\"\"\n",
    "    *Ultra-fast* fallback –  < 100 ms on any laptop.\n",
    "    Trains vanilla LogisticRegression so the API can\n",
    "    answer probability queries while the PyMC model cooks.\n",
    "    \"\"\"\n",
    "    from sklearn.datasets import load_breast_cancer\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    import mlflow, tempfile, pickle, pandas as pd\n",
    "\n",
    "    X, y = load_breast_cancer(return_X_y=True, as_frame=True)\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.3,\n",
    "                                          stratify=y, random_state=random_state)\n",
    "\n",
    "    clf = LogisticRegression(max_iter=200, n_jobs=-1).fit(Xtr, ytr)\n",
    "\n",
    "    class CancerStubWrapper(mlflow.pyfunc.PythonModel):\n",
    "        \"\"\"Return P(malignant) both via predict() and predict_proba().\"\"\"\n",
    "\n",
    "        def __init__(self, model):\n",
    "            self.model = model\n",
    "\n",
    "        def _pp(self, X):\n",
    "            X_ = X.values if hasattr(X, \"values\") else X\n",
    "            return self.model.predict_proba(X_)\n",
    "\n",
    "        def predict(self, model_input, params=None):\n",
    "            # Return 1-D array of malignant probabilities\n",
    "            return self._pp(model_input)[:, 1]\n",
    "\n",
    "        def predict_proba(self, X):\n",
    "            return self._pp(X)\n",
    "\n",
    "    acc = accuracy_score(yte, clf.predict(Xte))\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as td, mlflow.start_run(run_name=\"breast_cancer_stub\") as run:\n",
    "        mlflow.log_metric(\"accuracy\", acc)\n",
    "        mlflow.pyfunc.log_model(\n",
    "            \"model\",\n",
    "            python_model=CancerStubWrapper(clf),\n",
    "            registered_model_name=\"breast_cancer_stub\",\n",
    "            input_example=X.head(),\n",
    "            signature=mlflow.models.signature.infer_signature(X, clf.predict_proba(X)),\n",
    "        )\n",
    "        return run.info.run_id\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "#  BREAST-CANCER – hierarchical Bayesian logistic regression\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def train_breast_cancer_bayes(\n",
    "    draws: int = 1000,\n",
    "    tune: int = 1000,\n",
    "    target_accept: float = 0.90,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Hierarchical Bayesian logistic‑regression with varying intercepts by\n",
    "    **mean_texture quintile**.\n",
    "\n",
    "    * Uses **NumPyro NUTS** backend → **no C compilation** on Windows.  \n",
    "    * Logs model to MLflow exactly like before so FastAPI can reload it.\n",
    "    \"\"\"\n",
    "\n",
    "    import pymc as pm                      # PyMC ≥5.9\n",
    "    import pandas as pd, numpy as np\n",
    "    from sklearn.datasets import load_breast_cancer\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    import mlflow, tempfile, pickle\n",
    "    from pathlib import Path\n",
    "\n",
    "    # Note: PyTensor config is set by env_sanitizer before import\n",
    "    # No runtime config changes needed - they're already applied\n",
    "\n",
    "    # 1️⃣  data ----------------------------------------------------------------\n",
    "    X_df, y = load_breast_cancer(as_frame=True, return_X_y=True)\n",
    "    quint, edges = pd.qcut(X_df[\"mean texture\"], 5, labels=False, retbins=True)\n",
    "    g        = np.asarray(quint, dtype=\"int64\")          # 0‥4\n",
    "    scaler   = StandardScaler().fit(X_df)\n",
    "    Xs       = scaler.transform(X_df)\n",
    "\n",
    "    # 2️⃣  model ---------------------------------------------------------------\n",
    "    coords = {\"group\": np.arange(5)}\n",
    "    with pm.Model(coords=coords) as m:\n",
    "        α     = pm.Normal(\"α\", 0.0, 1.0, dims=\"group\")   # varying intercepts\n",
    "        β     = pm.Normal(\"β\", 0.0, 1.0, shape=Xs.shape[1])\n",
    "        logit = α[g] + pm.math.dot(Xs, β)\n",
    "        pm.Bernoulli(\"obs\", logit_p=logit, observed=y)\n",
    "\n",
    "        idata = pm.sample(\n",
    "            draws=draws,\n",
    "            tune=tune,\n",
    "            chains=4,\n",
    "            nuts_sampler=\"numpyro\",        # <-- magic line\n",
    "            target_accept=target_accept,\n",
    "            progressbar=False,\n",
    "        )\n",
    "\n",
    "    # 3️⃣  lightweight pyfunc wrapper -----------------------------------------\n",
    "    class _HierBayesWrapper(mlflow.pyfunc.PythonModel):\n",
    "        def __init__(self, trace, sc, ed, cols):\n",
    "            self.trace, self.scaler, self.edges, self.cols = trace, sc, ed, cols\n",
    "\n",
    "        def _quint(self, df):\n",
    "            tex = df[\"mean texture\"].to_numpy()\n",
    "            return np.clip(np.digitize(tex, self.edges, right=False), 0, 4)\n",
    "\n",
    "        def predict(self, X, params=None):\n",
    "            df  = X if isinstance(X, pd.DataFrame) else pd.DataFrame(X, columns=self.cols)\n",
    "            xs  = self.scaler.transform(df)\n",
    "            g   = self._quint(df)\n",
    "            αg  = self.trace.posterior[\"α\"].median((\"chain\", \"draw\")).values\n",
    "            β   = self.trace.posterior[\"β\"].median((\"chain\", \"draw\")).values\n",
    "            log = αg[g] + np.dot(xs, β)\n",
    "            return 1.0 / (1.0 + np.exp(-log))\n",
    "\n",
    "    wrapper = _HierBayesWrapper(idata, scaler, edges[1:-1], X_df.columns.tolist())\n",
    "    acc     = float(((wrapper.predict(X_df) > .5).astype(int) == y).mean())\n",
    "\n",
    "    # 4️⃣  MLflow logging (unchanged) -----------------------------------------\n",
    "    with tempfile.TemporaryDirectory() as td, mlflow.start_run(run_name=\"breast_cancer_bayes\") as run:\n",
    "        sc_path = Path(td) / \"scaler.pkl\"\n",
    "        pickle.dump(scaler, open(sc_path, \"wb\"))\n",
    "        mlflow.log_params(dict(draws=draws, tune=tune, target_accept=target_accept))\n",
    "        mlflow.log_metric(\"accuracy\", acc)\n",
    "        mlflow.pyfunc.log_model(\n",
    "            \"model\",\n",
    "            python_model=wrapper,\n",
    "            artifacts={\"scaler\": str(sc_path)},\n",
    "            registered_model_name=\"breast_cancer_bayes\",\n",
    "            input_example=X_df.head(),\n",
    "            signature=mlflow.models.signature.infer_signature(X_df, wrapper.predict(X_df)),\n",
    "        )\n",
    "        return run.info.run_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fe3394a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/app/services/ml/model_service.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/app/services/ml/model_service.py\n",
    "\"\"\"\n",
    "Model service – self-healing startup with background training.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import asyncio, logging, os, time, socket\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "\n",
    "import mlflow, pandas as pd, numpy as np\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.exceptions import MlflowException\n",
    "\n",
    "from app.core.config import settings\n",
    "from app.ml.builtin_trainers import (\n",
    "    train_iris_random_forest,\n",
    "    train_iris_logreg,  # NEW\n",
    "    train_breast_cancer_bayes,\n",
    "    train_breast_cancer_stub,\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Cancer column mapping: Pydantic field names ➜ training column names\n",
    "# ---------------------------------------------------------------------------\n",
    "_CANCER_COLMAP: dict[str, str] = {\n",
    "    # Means\n",
    "    \"mean_radius\": \"mean radius\",\n",
    "    \"mean_texture\": \"mean texture\",\n",
    "    \"mean_perimeter\": \"mean perimeter\",\n",
    "    \"mean_area\": \"mean area\",\n",
    "    \"mean_smoothness\": \"mean smoothness\",\n",
    "    \"mean_compactness\": \"mean compactness\",\n",
    "    \"mean_concavity\": \"mean concavity\",\n",
    "    \"mean_concave_points\": \"mean concave points\",\n",
    "    \"mean_symmetry\": \"mean symmetry\",\n",
    "    \"mean_fractal_dimension\": \"mean fractal dimension\",\n",
    "    # SE\n",
    "    \"se_radius\": \"radius error\",\n",
    "    \"se_texture\": \"texture error\",\n",
    "    \"se_perimeter\": \"perimeter error\",\n",
    "    \"se_area\": \"area error\",\n",
    "    \"se_smoothness\": \"smoothness error\",\n",
    "    \"se_compactness\": \"compactness error\",\n",
    "    \"se_concavity\": \"concavity error\",\n",
    "    \"se_concave_points\": \"concave points error\",\n",
    "    \"se_symmetry\": \"symmetry error\",\n",
    "    \"se_fractal_dimension\": \"fractal dimension error\",\n",
    "    # Worst\n",
    "    \"worst_radius\": \"worst radius\",\n",
    "    \"worst_texture\": \"worst texture\",\n",
    "    \"worst_perimeter\": \"worst perimeter\",\n",
    "    \"worst_area\": \"worst area\",\n",
    "    \"worst_smoothness\": \"worst smoothness\",\n",
    "    \"worst_compactness\": \"worst compactness\",\n",
    "    \"worst_concavity\": \"worst concavity\",\n",
    "    \"worst_concave_points\": \"worst concave points\",\n",
    "    \"worst_symmetry\": \"worst symmetry\",\n",
    "    \"worst_fractal_dimension\": \"worst fractal dimension\",\n",
    "}\n",
    "\n",
    "def _rename_cancer_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ensure DataFrame columns match the training schema used by MLflow artefacts.\n",
    "    Unknown columns are left untouched so legacy models still work.\n",
    "    \"\"\"\n",
    "    return df.rename(columns=_CANCER_COLMAP)\n",
    "\n",
    "# Trainer mapping for self-healing\n",
    "TRAINERS = {\n",
    "    \"iris_random_forest\": train_iris_random_forest,\n",
    "    \"iris_logreg\":        train_iris_logreg,  # NEW\n",
    "    \"breast_cancer_bayes\": train_breast_cancer_bayes,\n",
    "    \"breast_cancer_stub\":  train_breast_cancer_stub,\n",
    "}\n",
    "\n",
    "class ModelService:\n",
    "    \"\"\"\n",
    "    Self-healing model service that loads existing models and schedules\n",
    "    background training for missing ones.\n",
    "    \"\"\"\n",
    "\n",
    "    _EXECUTOR = ThreadPoolExecutor(max_workers=2)\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self._unit_test_mode = settings.UNIT_TESTING\n",
    "        self.initialized = False\n",
    "\n",
    "        # 🚫 Heavy clients only when NOT unit-testing\n",
    "        self.client = None if self._unit_test_mode else None  # Will be set in initialize()\n",
    "        self.mlflow_client = None\n",
    "\n",
    "        self.models: Dict[str, Any] = {}\n",
    "        self.status: Dict[str, str] = {\n",
    "            \"iris_random_forest\": \"missing\",\n",
    "            \"iris_logreg\":        \"missing\",  # NEW\n",
    "            \"breast_cancer_bayes\": \"missing\",\n",
    "            \"breast_cancer_stub\": \"missing\",\n",
    "        }\n",
    "\n",
    "    async def initialize(self) -> None:\n",
    "        \"\"\"\n",
    "        Connect to MLflow – fall back to local file store if the configured\n",
    "        tracking URI is unreachable *or* the client is missing critical methods\n",
    "        (e.g. when mlflow-skinny accidentally shadows the full package).\n",
    "        \"\"\"\n",
    "        if self.initialized:\n",
    "            return\n",
    "\n",
    "        # Log critical dependency versions for diagnostics\n",
    "        try:\n",
    "            import pytensor\n",
    "            logger.info(\"📦 PyTensor version: %s\", pytensor.__version__)\n",
    "        except ImportError:\n",
    "            logger.warning(\"⚠️  PyTensor not available\")\n",
    "        except Exception as e:\n",
    "            logger.warning(\"⚠️  Could not determine PyTensor version: %s\", e)\n",
    "\n",
    "        def _needs_fallback(client) -> bool:\n",
    "            # any missing attr is a strong signal we are on mlflow-skinny\n",
    "            return not callable(getattr(client, \"list_experiments\", None))\n",
    "\n",
    "        try:\n",
    "            mlflow.set_tracking_uri(settings.MLFLOW_TRACKING_URI)\n",
    "            self.mlflow_client = MlflowClient(settings.MLFLOW_TRACKING_URI)\n",
    "\n",
    "            if _needs_fallback(self.mlflow_client):\n",
    "                raise AttributeError(\"list_experiments not implemented – skinny build detected\")\n",
    "\n",
    "            # minimal probe (cheap & always present)\n",
    "            self.mlflow_client.search_experiments(max_results=1)\n",
    "            logger.info(\"🟢  Connected to MLflow @ %s\", settings.MLFLOW_TRACKING_URI)\n",
    "\n",
    "        except (MlflowException, socket.gaierror, AttributeError) as exc:\n",
    "            logger.warning(\"🔄  Falling back to local MLflow store – %s\", exc)\n",
    "            mlflow.set_tracking_uri(\"file:./mlruns_local\")\n",
    "            self.mlflow_client = MlflowClient(\"file:./mlruns_local\")\n",
    "            logger.info(\"📂  Using local file store ./mlruns_local\")\n",
    "\n",
    "        await self._load_models()\n",
    "        self.initialized = True\n",
    "\n",
    "    async def _load_models(self) -> None:\n",
    "        \"\"\"Load existing models from MLflow.\"\"\"\n",
    "        await self._try_load(\"iris_random_forest\")\n",
    "        await self._try_load(\"iris_logreg\")      # NEW\n",
    "        await self._try_load(\"breast_cancer_bayes\")\n",
    "        await self._try_load(\"breast_cancer_stub\")\n",
    "\n",
    "    async def startup(self, auto_train: bool | None = None) -> None:\n",
    "        \"\"\"\n",
    "        Faster: serve stub immediately; heavy Bayesian job in background.\n",
    "        \"\"\"\n",
    "        if self._unit_test_mode:\n",
    "            logger.info(\"🔒 UNIT_TESTING=1 – skipping model loading\")\n",
    "            return                      # 👉 nothing else runs\n",
    "\n",
    "        # Initialize MLflow connection first\n",
    "        await self.initialize()\n",
    "\n",
    "        if settings.SKIP_BACKGROUND_TRAINING:\n",
    "            logger.warning(\"⏩ SKIP_BACKGROUND_TRAINING=1 – models will load on-demand\")\n",
    "            # We still *try* to load existing artefacts so prod works\n",
    "            await self._try_load(\"iris_random_forest\")\n",
    "            await self._try_load(\"iris_logreg\")\n",
    "            await self._try_load(\"breast_cancer_bayes\")\n",
    "            return\n",
    "\n",
    "        auto = auto_train if auto_train is not None else settings.AUTO_TRAIN_MISSING\n",
    "        logger.info(\"🔄 Model-service startup (auto_train=%s)\", auto)\n",
    "\n",
    "        # 1️⃣ try to load whatever already exists\n",
    "        await self._try_load(\"iris_random_forest\")\n",
    "        await self._try_load(\"iris_logreg\")\n",
    "\n",
    "        # 2️⃣ Load bayes – if exists we're done\n",
    "        if not await self._try_load(\"breast_cancer_bayes\"):\n",
    "            # 3️⃣ Ensure stub is *synchronously* available\n",
    "            if not await self._try_load(\"breast_cancer_stub\"):\n",
    "                logger.info(\"Training stub cancer model …\")\n",
    "                await asyncio.get_running_loop().run_in_executor(\n",
    "                    self._EXECUTOR, train_breast_cancer_stub\n",
    "                )\n",
    "                await self._try_load(\"breast_cancer_stub\")\n",
    "\n",
    "            # 4️⃣ Fire full PyMC build in background unless disabled\n",
    "            if not settings.SKIP_BACKGROUND_TRAINING:\n",
    "                logger.info(\"Scheduling full Bayesian retrain in background\")\n",
    "                asyncio.create_task(\n",
    "                    self._train_and_reload(\"breast_cancer_bayes\", train_breast_cancer_bayes)\n",
    "                )\n",
    "\n",
    "        # 5️⃣ Train iris models if missing\n",
    "        if not await self._try_load(\"iris_random_forest\"):\n",
    "            logger.info(\"Training iris random-forest …\")\n",
    "            await asyncio.get_running_loop().run_in_executor(\n",
    "                self._EXECUTOR, train_iris_random_forest\n",
    "            )\n",
    "            await self._try_load(\"iris_random_forest\")\n",
    "\n",
    "        if not await self._try_load(\"iris_logreg\"):\n",
    "            logger.info(\"Training iris logistic-regression …\")\n",
    "            await asyncio.get_running_loop().run_in_executor(\n",
    "                self._EXECUTOR, train_iris_logreg\n",
    "            )\n",
    "            await self._try_load(\"iris_logreg\")\n",
    "\n",
    "    async def _try_load(self, name: str) -> None:\n",
    "        \"\"\"Try to load a model and update status.\"\"\"\n",
    "        model = await self._load_production_model(name)\n",
    "        if model:\n",
    "            self.models[name] = model\n",
    "            self.status[name] = \"loaded\"\n",
    "            logger.info(\"✅ %s loaded\", name)\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    async def _train_and_reload(self, name: str, trainer) -> None:\n",
    "        \"\"\"Train a model in background and reload it, with verbose phase logs.\"\"\"\n",
    "        try:\n",
    "            t0 = time.perf_counter()\n",
    "            logger.info(\"🏗️  BEGIN training %s\", name)\n",
    "            self.status[name] = \"training\"\n",
    "\n",
    "            loop = asyncio.get_running_loop()\n",
    "            await loop.run_in_executor(self._EXECUTOR, trainer)\n",
    "\n",
    "            logger.info(\"📦 Training %s complete in %.1fs – re-loading\", name,\n",
    "                        time.perf_counter() - t0)\n",
    "            model = await self._load_production_model(name)\n",
    "            if not model:\n",
    "                raise RuntimeError(f\"{name} trained but could not be re-loaded\")\n",
    "\n",
    "            self.models[name] = model\n",
    "            self.status[name] = \"loaded\"\n",
    "            logger.info(\"✅ %s trained & loaded\", name)\n",
    "\n",
    "        except Exception as exc:\n",
    "            self.status[name] = \"failed\"\n",
    "            logger.error(\"❌ %s failed: %s\", name, exc, exc_info=True)  # ← keeps trace\n",
    "            # NEW: persist last_error for UI / debug endpoint\n",
    "            self.status[f\"{name}_last_error\"] = str(exc)\n",
    "\n",
    "    async def _load_production_model(self, name: str) -> Optional[Any]:\n",
    "        \"\"\"\n",
    "        1. Registry 'Production' stage → load.  \n",
    "        2. Otherwise most recent run with runName == name.\n",
    "        Returns None if not found.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            versions = self.mlflow_client.search_model_versions(f\"name='{name}'\")\n",
    "            prod = [v for v in versions if v.current_stage == \"Production\"]\n",
    "            if prod:\n",
    "                uri = f\"models:/{name}/{prod[0].version}\"\n",
    "                logger.info(\"↪︎  Loading %s from registry:%s\", name, prod[0].version)\n",
    "                return mlflow.pyfunc.load_model(uri)\n",
    "        except MlflowException:\n",
    "            pass\n",
    "\n",
    "        # Fallback – scan experiments for latest run\n",
    "        runs = []\n",
    "        for exp in self.mlflow_client.search_experiments():\n",
    "            runs.extend(self.mlflow_client.search_runs(\n",
    "                [exp.experiment_id],\n",
    "                f\"tags.mlflow.runName = '{name}'\",\n",
    "                order_by=[\"attributes.start_time DESC\"],\n",
    "                max_results=1))\n",
    "        if runs:\n",
    "            uri = f\"runs:/{runs[0].info.run_id}/model\"\n",
    "            logger.info(\"↪︎  Loading %s from latest run:%s\", name, runs[0].info.run_id)\n",
    "            return mlflow.pyfunc.load_model(uri)\n",
    "        return None\n",
    "\n",
    "    # Manual training endpoints (for UI)\n",
    "    async def train_iris(self) -> None:\n",
    "        await self._train_and_reload(\"iris_random_forest\", TRAINERS[\"iris_random_forest\"])\n",
    "\n",
    "    async def train_cancer(self) -> None:\n",
    "        await self._train_and_reload(\"breast_cancer_bayes\", TRAINERS[\"breast_cancer_bayes\"])\n",
    "\n",
    "    # Predict methods (unchanged from your previous version)\n",
    "    async def predict_iris(\n",
    "        self,\n",
    "        features: List[Dict[str, float]],\n",
    "        model_type: str = \"rf\",\n",
    "    ) -> Tuple[List[str], List[List[float]]]:\n",
    "        \"\"\"\n",
    "        Predict Iris species from measurements.\n",
    "\n",
    "        Args:\n",
    "            features: List of iris measurements as dictionaries\n",
    "            model_type: Model type to use (only 'rf' supported)\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (predicted_class_names, class_probabilities)\n",
    "        \"\"\"\n",
    "        if model_type not in (\"rf\", \"logreg\"):\n",
    "            raise ValueError(\"model_type must be 'rf' or 'logreg'\")\n",
    "\n",
    "        model_name = \"iris_random_forest\" if model_type == \"rf\" else \"iris_logreg\"\n",
    "        model = self.models.get(model_name)\n",
    "        if not model:\n",
    "            raise RuntimeError(f\"{model_name} not loaded\")\n",
    "\n",
    "        # Convert to DataFrame with proper column names (matching training data)\n",
    "        X_df = pd.DataFrame([{\n",
    "            \"sepal length (cm)\": sample[\"sepal_length\"],\n",
    "            \"sepal width (cm)\": sample[\"sepal_width\"], \n",
    "            \"petal length (cm)\": sample[\"petal_length\"],\n",
    "            \"petal width (cm)\": sample[\"petal_width\"]\n",
    "        } for sample in features])\n",
    "\n",
    "        # Obtain probabilities in a backward-compatible way\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            probs = model.predict_proba(X_df)\n",
    "        else:\n",
    "            # Legacy artefact – derive 1-hot probas from class indices\n",
    "            preds_idx = model.predict(X_df)\n",
    "            import numpy as _np\n",
    "            probs = _np.zeros((len(preds_idx), 3), dtype=float)\n",
    "            probs[_np.arange(len(preds_idx)), preds_idx.astype(int)] = 1.0\n",
    "\n",
    "        # Ensure numpy array then list list\n",
    "        preds = probs.argmax(axis=1)                 # numerical class indices\n",
    "\n",
    "        # Map numerical classes to species names\n",
    "        class_names = [\"setosa\", \"versicolor\", \"virginica\"]\n",
    "        pred_names = [class_names[i] for i in preds]\n",
    "\n",
    "        return pred_names, probs.tolist()\n",
    "\n",
    "    async def predict_cancer(\n",
    "        self,\n",
    "        features: List[Dict[str, float]],\n",
    "        model_type: str = \"bayes\",\n",
    "        posterior_samples: Optional[int] = None,\n",
    "    ) -> Tuple[List[str], List[float], Optional[List[Tuple[float, float]]]]:\n",
    "        \"\"\"\n",
    "        Predict breast cancer diagnosis from features using hierarchical Bayesian model.\n",
    "        Falls back to stub model if Bayesian model is not available.\n",
    "\n",
    "        Args:\n",
    "            features: List of cancer measurements as dictionaries\n",
    "            model_type: Model type to use ('bayes' or 'stub')\n",
    "            posterior_samples: Number of posterior samples for uncertainty (Bayesian only)\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (predicted_labels, probabilities, uncertainty_intervals)\n",
    "        \"\"\"\n",
    "        # Determine which model to use\n",
    "        if model_type == \"bayes\":\n",
    "            model = self.models.get(\"breast_cancer_bayes\")\n",
    "            if not model:\n",
    "                # Fall back to stub model\n",
    "                model = self.models.get(\"breast_cancer_stub\")\n",
    "                if not model:\n",
    "                    raise RuntimeError(\"No cancer model available\")\n",
    "                logger.info(\"Using stub cancer model (Bayesian model not ready)\")\n",
    "        elif model_type == \"stub\":\n",
    "            model = self.models.get(\"breast_cancer_stub\")\n",
    "            if not model:\n",
    "                raise RuntimeError(\"Stub cancer model not loaded\")\n",
    "        else:\n",
    "            raise ValueError(\"model_type must be 'bayes' or 'stub'\")\n",
    "\n",
    "        # Convert to DataFrame with proper column names\n",
    "        X_df_raw = pd.DataFrame(features)\n",
    "        X_df = _rename_cancer_columns(X_df_raw)\n",
    "\n",
    "        # Get predictions\n",
    "        if model_type == \"bayes\" and \"breast_cancer_bayes\" in self.models:\n",
    "            # Use Bayesian model with uncertainty\n",
    "            probs = model.predict(X_df)\n",
    "            labels = [\"malignant\" if p > 0.5 else \"benign\" for p in probs]\n",
    "        else:\n",
    "            # Use stub model (sklearn LogisticRegression)\n",
    "            if hasattr(model, \"predict_proba\"):\n",
    "                probs_full = model.predict_proba(X_df)\n",
    "                probs = probs_full[:, 1]\n",
    "            else:\n",
    "                # Legacy artefact: model.predict returns hard class 0/1\n",
    "                preds_bin = model.predict(X_df).astype(float)\n",
    "                probs = preds_bin  # deterministic 0/1 acts as prob\n",
    "            labels = [\"malignant\" if p > 0.5 else \"benign\" for p in probs]\n",
    "\n",
    "        # Compute uncertainty intervals if requested (Bayesian model only)\n",
    "        ci = None\n",
    "        if posterior_samples and model_type == \"bayes\" and \"breast_cancer_bayes\" in self.models:\n",
    "            try:\n",
    "                # Access the underlying python model to get the trace\n",
    "                python_model = model.unwrap_python_model()\n",
    "\n",
    "                # Access posterior samples for uncertainty quantification\n",
    "                draws = python_model.trace.posterior\n",
    "                αg = draws[\"α_group\"].stack(samples=(\"chain\", \"draw\"))\n",
    "                β = draws[\"β\"].stack(samples=(\"chain\", \"draw\"))\n",
    "\n",
    "                # Get group indices and standardized features\n",
    "                g = python_model._group_index(X_df)\n",
    "                Xs = python_model.scaler.transform(X_df)\n",
    "\n",
    "                # Compute posterior predictive samples\n",
    "                logits = αg.values[:, g] + np.dot(β.values.T, Xs.T)      # shape (S, N)\n",
    "                pp = 1 / (1 + np.exp(-logits))\n",
    "\n",
    "                # Compute 95% credible intervals\n",
    "                lo, hi = np.percentile(pp, [2.5, 97.5], axis=0)\n",
    "                ci = list(zip(lo.tolist(), hi.tolist()))\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to compute uncertainty intervals: {e}\")\n",
    "                ci = None\n",
    "\n",
    "        return labels, probs.tolist(), ci\n",
    "\n",
    "\n",
    "# Global singleton\n",
    "model_service = ModelService()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "01981457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/app/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/app/main.py\n",
    "import logging\n",
    "import os\n",
    "import asyncio\n",
    "from fastapi import FastAPI, Request, Depends, BackgroundTasks, status, HTTPException\n",
    "from fastapi.security import OAuth2PasswordRequestForm\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.responses import JSONResponse\n",
    "from sqlalchemy.ext.asyncio import AsyncSession\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "import time\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# ── NEW: Fix ML backend configuration before any JAX imports ───────────────────────────\n",
    "from .utils.env_sanitizer import fix_ml_backends\n",
    "fix_ml_backends()\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# ── NEW: Rate limiting imports ─────────────────────────────────────────────────────────\n",
    "from fastapi_limiter import FastAPILimiter\n",
    "import redis.asyncio as redis\n",
    "# ────────────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# ── NEW: Concurrency limiting imports ────────────────────────────────────────────────\n",
    "from .middleware.concurrency import ConcurrencyLimiter\n",
    "# ────────────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "from .db import lifespan, get_db, get_app_ready\n",
    "from .security import create_access_token, get_current_user, verify_password\n",
    "from .crud import get_user_by_username\n",
    "from .schemas.iris import IrisPredictRequest, IrisPredictResponse, IrisFeatures\n",
    "from .schemas.cancer import CancerPredictRequest, CancerPredictResponse, CancerFeatures\n",
    "from .services.ml.model_service import model_service\n",
    "from .core.config import settings\n",
    "from .deps.limits import default_limit, heavy_limit, login_limit, training_limit, light_limit\n",
    "from .security import LoginPayload, get_credentials\n",
    "\n",
    "# ── NEW: guarantee log directory exists ───────────────────────────\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "# ──────────────────────────────────────────────────────────────────\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Pydantic models\n",
    "class Payload(BaseModel):\n",
    "    count: int\n",
    "\n",
    "class PredictionRequest(BaseModel):\n",
    "    data: Payload\n",
    "\n",
    "class PredictionResponse(BaseModel):\n",
    "    prediction: str\n",
    "    confidence: float\n",
    "    input_received: Payload  # Echo back the input for verification\n",
    "\n",
    "class Token(BaseModel):\n",
    "    access_token: str\n",
    "    token_type: str\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"FastAPI + React ML App\",\n",
    "    version=\"1.0.0\",\n",
    "    docs_url=\"/api/v1/docs\",\n",
    "    redoc_url=\"/api/v1/redoc\",\n",
    "    openapi_url=\"/api/v1/openapi.json\",\n",
    "    swagger_ui_parameters={\"persistAuthorization\": True},\n",
    "    lifespan=lifespan,  # register startup/shutdown events\n",
    ")\n",
    "\n",
    "# ── Rate limiting is now initialized in lifespan() ────────────────────────────────────\n",
    "# ────────────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# Configure CORS with environment-based origins\n",
    "origins_env = settings.ALLOWED_ORIGINS\n",
    "origins: list[str] = [o.strip() for o in origins_env.split(\",\")] if origins_env != \"*\" else [\"*\"]\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],  # In production, replace with specific origins\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# ── NEW: Add concurrency limiting middleware ──────────────────────────────────────────\n",
    "app.add_middleware(ConcurrencyLimiter, max_concurrent=4)\n",
    "# ────────────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "@app.middleware(\"http\")\n",
    "async def add_process_time_header(request: Request, call_next):\n",
    "    \"\"\"Measure request time and add X-Process-Time header.\"\"\"\n",
    "    start = time.perf_counter()\n",
    "    response = await call_next(request)\n",
    "    elapsed = time.perf_counter() - start\n",
    "    response.headers[\"X-Process-Time\"] = f\"{elapsed:.4f}\"\n",
    "    return response\n",
    "\n",
    "# Health check endpoint\n",
    "@app.get(\"/api/v1/health\")\n",
    "async def health_check():\n",
    "    \"\"\"Basic health check - always returns 200 if server is running.\"\"\"\n",
    "    return {\"status\": \"healthy\", \"timestamp\": time.time()}\n",
    "\n",
    "@app.get(\"/api/v1/hello\")\n",
    "async def hello(current_user: str = Depends(get_current_user)):\n",
    "    \"\"\"Simple endpoint for token validation.\"\"\"\n",
    "    return {\"message\": f\"Hello {current_user}!\", \"status\": \"authenticated\"}\n",
    "\n",
    "@app.get(\"/api/v1/ready\")\n",
    "async def ready():\n",
    "    \"\"\"Basic readiness check.\"\"\"\n",
    "    return {\"ready\": get_app_ready()}\n",
    "\n",
    "@app.post(\"/api/v1/token\", response_model=Token, dependencies=[Depends(login_limit)])\n",
    "async def login(\n",
    "    creds: LoginPayload = Depends(get_credentials),\n",
    "    db: AsyncSession = Depends(get_db),\n",
    "):\n",
    "    \"\"\"\n",
    "    Issue a JWT. Accepts **either**\n",
    "    • JSON {\"username\": \"...\", \"password\": \"...\"}  *or*\n",
    "    • classic x‑www‑form‑urlencoded.\n",
    "    \"\"\"\n",
    "    # 1️⃣ readiness gate\n",
    "    if not get_app_ready():\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,\n",
    "            detail=\"Backend still loading models. Try again in a moment.\",\n",
    "            headers={\"Retry‑After\": \"10\"},\n",
    "        )\n",
    "\n",
    "    # 2️⃣ verify credentials\n",
    "    user = await get_user_by_username(db, creds.username)\n",
    "    if not user or not verify_password(creds.password, user.hashed_password):\n",
    "        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED,\n",
    "                            detail=\"Invalid credentials\")\n",
    "\n",
    "    # 3️⃣ issue token\n",
    "    token = create_access_token(subject=user.username)\n",
    "    return Token(access_token=token, token_type=\"bearer\")\n",
    "\n",
    "@app.get(\"/api/v1/ready/full\")\n",
    "async def ready_full() -> dict:\n",
    "    \"\"\"\n",
    "    Extended readiness probe:\n",
    "    - ready: API server is ready to accept requests (login allowed)\n",
    "    - model_status: dict of {model_name: status} where status is 'loaded'|'training'|'failed'|'missing'\n",
    "    - all_models_loaded: true if all models are in 'loaded' state\n",
    "    \"\"\"\n",
    "    # Allow login if API is ready, regardless of model status\n",
    "    ready_for_login = get_app_ready()\n",
    "\n",
    "    expected = {\"iris_random_forest\", \"breast_cancer_bayes\"}\n",
    "    loaded = set(model_service.models.keys())\n",
    "    training = set(model_service.status.keys()) - loaded\n",
    "\n",
    "    response = {\n",
    "        \"ready\": ready_for_login,  # Allow login immediately\n",
    "        \"model_status\": model_service.status,\n",
    "        \"all_models_loaded\": all(s == \"loaded\" for s in model_service.status.values()),\n",
    "        \"models\": {m: (m in loaded) for m in expected},\n",
    "        \"training\": list(training)\n",
    "    }\n",
    "\n",
    "    logger.debug(\"READY endpoint – _app_ready=%s, response=%s\", get_app_ready(), response)\n",
    "    return response\n",
    "\n",
    "# ── Alias routes (no auth, not shown in OpenAPI) ────────────────────────────\n",
    "@app.get(\"/ready/full\", include_in_schema=False)\n",
    "async def ready_full_alias():\n",
    "    \"\"\"Alias for front-end calls that miss the /api/v1 prefix.\"\"\"\n",
    "    return await ready_full()\n",
    "\n",
    "@app.get(\"/health\", include_in_schema=False)\n",
    "async def health_alias():\n",
    "    \"\"\"Alias for plain /health (SPA hits it before it knows the prefix).\"\"\"\n",
    "    return await health_check()\n",
    "\n",
    "@app.post(\"/token\", include_in_schema=False)\n",
    "async def login_alias(request: Request):\n",
    "    \"\"\"\n",
    "    Alias: accept /token like /api/v1/token.\n",
    "    Keeps the OAuth2PasswordRequestForm semantics without exposing clutter in docs.\n",
    "    \"\"\"\n",
    "    from fastapi import Form\n",
    "\n",
    "    # Parse form data manually to match OAuth2PasswordRequestForm behavior\n",
    "    form_data = await request.form()\n",
    "    username = form_data.get(\"username\")\n",
    "    password = form_data.get(\"password\")\n",
    "\n",
    "    if not username or not password:\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n",
    "            detail=\"username and password are required\"\n",
    "        )\n",
    "\n",
    "    # Create a mock OAuth2PasswordRequestForm object\n",
    "    class MockForm:\n",
    "        def __init__(self, username, password):\n",
    "            self.username = username\n",
    "            self.password = password\n",
    "\n",
    "    mock_form = MockForm(username, password)\n",
    "\n",
    "    # Reuse the existing login logic\n",
    "    db = await get_db().__anext__()\n",
    "    return await login(mock_form, db)\n",
    "\n",
    "@app.post(\"/iris/predict\", include_in_schema=False)\n",
    "async def iris_predict_alias(request: Request):\n",
    "    \"\"\"Alias for /api/v1/iris/predict\"\"\"\n",
    "    from .schemas.iris import IrisPredictRequest\n",
    "\n",
    "    # Parse JSON body\n",
    "    body = await request.json()\n",
    "    iris_request = IrisPredictRequest(**body)\n",
    "\n",
    "    # Reuse the existing prediction logic without authentication for testing\n",
    "    background_tasks = BackgroundTasks()\n",
    "    current_user = \"test_user\"  # Skip authentication for alias endpoints\n",
    "    return await predict_iris(iris_request, background_tasks, current_user)\n",
    "\n",
    "@app.post(\"/cancer/predict\", include_in_schema=False)\n",
    "async def cancer_predict_alias(request: Request):\n",
    "    \"\"\"Alias for /api/v1/cancer/predict\"\"\"\n",
    "    from .schemas.cancer import CancerPredictRequest\n",
    "\n",
    "    # Parse JSON body\n",
    "    body = await request.json()\n",
    "    cancer_request = CancerPredictRequest(**body)\n",
    "\n",
    "    # Reuse the existing prediction logic without authentication for testing\n",
    "    background_tasks = BackgroundTasks()\n",
    "    current_user = \"test_user\"  # Skip authentication for alias endpoints\n",
    "    return await predict_cancer(cancer_request, background_tasks, current_user)\n",
    "\n",
    "# ----- on-demand training endpoints ----------------------------------\n",
    "@app.post(\"/api/v1/iris/train\", status_code=202, dependencies=[Depends(training_limit)])\n",
    "async def train_iris(background_tasks: BackgroundTasks,\n",
    "                     current_user: str = Depends(get_current_user)):\n",
    "    background_tasks.add_task(model_service.train_iris)\n",
    "    return {\"status\": \"started\"}\n",
    "\n",
    "@app.post(\"/api/v1/cancer/train\", status_code=202, dependencies=[Depends(training_limit)])\n",
    "async def train_cancer(background_tasks: BackgroundTasks,\n",
    "                       current_user: str = Depends(get_current_user)):\n",
    "    background_tasks.add_task(model_service.train_cancer)\n",
    "    return {\"status\": \"started\"}\n",
    "\n",
    "@app.get(\"/api/v1/iris/ready\")\n",
    "async def iris_ready():\n",
    "    \"\"\"Check if Iris model is loaded and ready.\"\"\"\n",
    "    return {\"loaded\": \"iris_random_forest\" in model_service.models}\n",
    "\n",
    "@app.get(\"/api/v1/cancer/ready\")\n",
    "async def cancer_ready():\n",
    "    \"\"\"Check if Cancer model is loaded and ready.\"\"\"\n",
    "    return {\"loaded\": \"breast_cancer_bayes\" in model_service.models}\n",
    "\n",
    "@app.post(\n",
    "    \"/api/v1/iris/predict\",\n",
    "    response_model=IrisPredictResponse,\n",
    "    status_code=status.HTTP_200_OK,\n",
    "    dependencies=[Depends(light_limit)]\n",
    ")\n",
    "async def predict_iris(\n",
    "    request: IrisPredictRequest,\n",
    "    background_tasks: BackgroundTasks,\n",
    "    current_user: str = Depends(get_current_user),\n",
    "):\n",
    "    \"\"\"\n",
    "    Predict iris species from measurements.\n",
    "\n",
    "    Example request:\n",
    "        {\n",
    "            \"model_type\": \"rf\",\n",
    "            \"samples\": [\n",
    "                {\n",
    "                    \"sepal_length\": 5.1,\n",
    "                    \"sepal_width\": 3.5,\n",
    "                    \"petal_length\": 1.4,\n",
    "                    \"petal_width\": 0.2\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    \"\"\"\n",
    "    logger.info(f\"User {current_user} called /iris/predict with {len(request.samples)} samples\")\n",
    "    logger.debug(f\"→ Iris payload: {request.samples}\")\n",
    "\n",
    "    # Check if requested iris model is loaded; return 503 if not yet available\n",
    "    if (\n",
    "        request.model_type == \"rf\" and \"iris_random_forest\" not in model_service.models\n",
    "    ) or (\n",
    "        request.model_type == \"logreg\" and \"iris_logreg\" not in model_service.models\n",
    "    ):\n",
    "        logger.warning(\"Iris model not ready - returning 503\")\n",
    "        raise HTTPException(\n",
    "            status_code=503,\n",
    "            detail=\"Iris model is still loading. Please try again in a few seconds.\",\n",
    "            headers={\"Retry-After\": \"30\"},\n",
    "        )\n",
    "\n",
    "    # Convert Pydantic models to dicts\n",
    "    features = [sample.dict() for sample in request.samples]\n",
    "    logger.debug(f\"→ Iris features: {features}\")\n",
    "\n",
    "    # Get predictions\n",
    "    predictions, probabilities = await model_service.predict_iris(\n",
    "        features=features,\n",
    "        model_type=request.model_type\n",
    "    )\n",
    "    logger.debug(f\"← Iris predictions: {predictions}\")\n",
    "    logger.debug(f\"← Iris probabilities: {probabilities}\")\n",
    "\n",
    "    result = {\n",
    "        \"predictions\": predictions,\n",
    "        \"probabilities\": probabilities,\n",
    "        \"input_received\": request.samples\n",
    "    }\n",
    "\n",
    "    # Background task for audit logging\n",
    "    background_tasks.add_task(\n",
    "        logger.info,\n",
    "        f\"[audit] user={current_user} endpoint=iris input={request.samples} output={predictions}\"\n",
    "    )\n",
    "\n",
    "    return IrisPredictResponse(**result)\n",
    "\n",
    "@app.post(\n",
    "    \"/api/v1/cancer/predict\",\n",
    "    response_model=CancerPredictResponse,\n",
    "    status_code=status.HTTP_200_OK,\n",
    "    dependencies=[Depends(heavy_limit)]\n",
    ")\n",
    "async def predict_cancer(\n",
    "    request: CancerPredictRequest,\n",
    "    background_tasks: BackgroundTasks,\n",
    "    current_user: str = Depends(get_current_user),\n",
    "):\n",
    "    \"\"\"\n",
    "    Predict breast cancer diagnosis from features.\n",
    "\n",
    "    Example request:\n",
    "        {\n",
    "            \"model_type\": \"bayes\",\n",
    "            \"samples\": [\n",
    "                {\n",
    "                    \"mean_radius\": 17.99,\n",
    "                    \"mean_texture\": 10.38,\n",
    "                    ...\n",
    "                }\n",
    "            ],\n",
    "            \"posterior_samples\": 1000  # optional\n",
    "        }\n",
    "    \"\"\"\n",
    "    logger.info(f\"User {current_user} called /cancer/predict with {len(request.samples)} samples\")\n",
    "    logger.debug(f\"→ Cancer payload: {request.samples}\")\n",
    "\n",
    "    # No early 503 here – model_service will transparently fall back to stub if Bayes not yet ready\n",
    "\n",
    "    # Convert Pydantic models to dicts\n",
    "    features = [sample.dict() for sample in request.samples]\n",
    "    logger.debug(f\"→ Cancer features: {features}\")\n",
    "\n",
    "    # Get predictions\n",
    "    predictions, probabilities, uncertainties = await model_service.predict_cancer(\n",
    "        features=features,\n",
    "        model_type=request.model_type,\n",
    "        posterior_samples=request.posterior_samples\n",
    "    )\n",
    "    logger.debug(f\"← Cancer predictions: {predictions}\")\n",
    "    logger.debug(f\"← Cancer probabilities: {probabilities}\")\n",
    "    logger.debug(f\"← Cancer uncertainties: {uncertainties}\")\n",
    "\n",
    "    result = {\n",
    "        \"predictions\": predictions,\n",
    "        \"probabilities\": probabilities,\n",
    "        \"uncertainties\": uncertainties,\n",
    "        \"input_received\": request.samples\n",
    "    }\n",
    "\n",
    "    # Background task for audit logging\n",
    "    background_tasks.add_task(\n",
    "        logger.info,\n",
    "        f\"[audit] user={current_user} endpoint=cancer input={request.samples} output={predictions}\"\n",
    "    )\n",
    "\n",
    "    return CancerPredictResponse(**result) \n",
    "\n",
    "@app.get(\"/api/v1/debug/ready\")\n",
    "async def debug_ready():\n",
    "    \"\"\"Debug endpoint to check _app_ready status.\"\"\"\n",
    "    return {\n",
    "        \"app_ready\": get_app_ready(),\n",
    "        \"model_service_initialized\": model_service.initialized,\n",
    "        \"models\": list(model_service.models.keys()),\n",
    "        \"status\": model_service.status,\n",
    "        \"errors\": {k: v for k, v in model_service.status.items() if k.endswith(\"_last_error\")}\n",
    "    }\n",
    "\n",
    "@app.get(\"/api/v1/debug/compiler\")\n",
    "async def debug_compiler():\n",
    "    \"\"\"\n",
    "    Debug endpoint to check JAX/NumPyro backend configuration.\n",
    "    Returns information about the JAX backend setup.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import jax\n",
    "        import numpyro\n",
    "        import pymc as pm\n",
    "\n",
    "        return {\n",
    "            \"backend\": \"jax_numpyro\",\n",
    "            \"jax_version\": jax.__version__,\n",
    "            \"numpyro_version\": numpyro.__version__,\n",
    "            \"pymc_version\": pm.__version__,\n",
    "            \"jax_devices\": str(jax.devices()),\n",
    "            \"jax_platform\": jax.default_backend(),\n",
    "            \"status\": \"jax_backend_configured\"\n",
    "        }\n",
    "    except ImportError as e:\n",
    "        return {\n",
    "            \"backend\": \"unknown\",\n",
    "            \"error\": f\"Import error: {e}\",\n",
    "            \"status\": \"missing_dependencies\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"backend\": \"unknown\", \n",
    "            \"error\": f\"Configuration error: {e}\",\n",
    "            \"status\": \"configuration_failed\"\n",
    "        } \n",
    "\n",
    "@app.get(\"/api/v1/test/401\")\n",
    "async def test_401():\n",
    "    \"\"\"Test endpoint that returns 401 for testing session expiry.\"\"\"\n",
    "    raise HTTPException(\n",
    "        status_code=status.HTTP_401_UNAUTHORIZED,\n",
    "        detail=\"Test 401 response\"\n",
    "    )\n",
    "\n",
    "# ── Debug‑only ratelimit helpers ─────────────────────────────────────────────\n",
    "from .deps.limits import get_redis, user_or_ip\n",
    "\n",
    "@app.post(\"/api/v1/debug/ratelimit/reset\", include_in_schema=False)\n",
    "async def rl_reset(request: Request):\n",
    "    \"\"\"\n",
    "    Flush **all** rate‑limit counters bound to the caller (JWT _or_ IP).\n",
    "\n",
    "    We match every fragment that contains the identifier to survive\n",
    "    future changes in FastAPI‑Limiter's key schema.\n",
    "    \"\"\"\n",
    "    r = get_redis()\n",
    "    if not r:\n",
    "        raise HTTPException(status_code=503, detail=\"Rate‑limiter not initialised\")\n",
    "\n",
    "    ident = await user_or_ip(request)\n",
    "    keys = await r.keys(f\"ratelimit:*{ident}*\")        # <— broader pattern\n",
    "    if keys:\n",
    "        await r.delete(*keys)\n",
    "    return {\"reset\": len(keys)}\n",
    "\n",
    "if settings.DEBUG_RATELIMIT:          # OFF by default\n",
    "    @app.get(\"/api/v1/debug/ratelimit/{bucket}\", include_in_schema=False)\n",
    "    async def rl_status(bucket: str, request: Request):\n",
    "        \"\"\"\n",
    "        Inspect Redis keys for the current identifier + bucket.\n",
    "        Handy for CI tests – **never enable in prod**.\n",
    "        \"\"\"\n",
    "        key_prefix = f\"ratelimit:{bucket}:{await user_or_ip(request)}\"\n",
    "        r = get_redis()\n",
    "        keys = await r.keys(f\"{key_prefix}*\")\n",
    "        values = await r.mget(keys) if keys else []\n",
    "        return dict(zip(keys, values)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f687bc30",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2200c56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/scripts/ensure_models.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/scripts/ensure_models.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Ensure models script - pre-trains all models before starting the API.\n",
    "This can be used in development or CI to ensure models are ready.\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "import logging\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the api directory to the Python path\n",
    "sys.path.insert(0, str(Path(__file__).parent.parent))\n",
    "\n",
    "from app.services.ml.model_service import TRAINERS, ModelService\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "async def main():\n",
    "    \"\"\"Ensure all models are trained and loaded.\"\"\"\n",
    "    logger.info(\"🚀 Starting model ensure script...\")\n",
    "    \n",
    "    svc = ModelService()\n",
    "    \n",
    "    # Start the self-healing process\n",
    "    await svc.startup(auto_train=True)\n",
    "    \n",
    "    # Wait until all models are loaded\n",
    "    max_wait = 300  # 5 minutes max\n",
    "    start_time = asyncio.get_event_loop().time()\n",
    "    \n",
    "    while len(svc.models) < len(TRAINERS):\n",
    "        if asyncio.get_event_loop().time() - start_time > max_wait:\n",
    "            logger.error(\"❌ Timeout waiting for models to load\")\n",
    "            return False\n",
    "            \n",
    "        logger.info(f\"⏳ Waiting for models... ({len(svc.models)}/{len(TRAINERS)} loaded)\")\n",
    "        \n",
    "        # Check for failed models\n",
    "        failed = [name for name, status in svc.status.items() if status == \"failed\"]\n",
    "        if failed:\n",
    "            logger.error(f\"❌ Models failed to train: {failed}\")\n",
    "            return False\n",
    "            \n",
    "        await asyncio.sleep(5)\n",
    "    \n",
    "    logger.info(\"✅ All models loaded successfully!\")\n",
    "    return True\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        success = asyncio.run(main())\n",
    "        sys.exit(0 if success else 1)\n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"⏹️  Interrupted by user\")\n",
    "        sys.exit(1)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Unexpected error: {e}\")\n",
    "        sys.exit(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9a1f7094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/__init__.py\n",
    "# Create logs dir early when package is imported by Uvicorn workers\n",
    "import os\n",
    "os.makedirs(\"logs\", exist_ok=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "be690efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tests/test_rate_limits.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tests/test_rate_limits.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Test script for rate limiting functionality.\n",
    "Run this to verify that rate limits are working correctly.\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "import httpx\n",
    "import time\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "class RateLimitTester:\n",
    "    def __init__(self, base_url: str = \"http://localhost:8000\"):\n",
    "        self.base_url = base_url\n",
    "        self.client = httpx.AsyncClient(timeout=10.0)\n",
    "        self.token: Optional[str] = None\n",
    "\n",
    "    async def login(self) -> bool:\n",
    "        \"\"\"Login to get a JWT token.\"\"\"\n",
    "        try:\n",
    "            response = await self.client.post(\n",
    "                f\"{self.base_url}/api/v1/token\",\n",
    "                data={\"username\": \"alice\", \"password\": \"supersecretvalue\"}\n",
    "            )\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                self.token = data[\"access_token\"]\n",
    "                print(\"✅ Login successful\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"❌ Login failed: {response.status_code}\")\n",
    "                return False\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Login error: {e}\")\n",
    "            return False\n",
    "\n",
    "    async def test_endpoint(self, endpoint: str, payload: dict, name: str, expected_limit: int):\n",
    "        \"\"\"Test rate limiting on a specific endpoint.\"\"\"\n",
    "        print(f\"\\n🔍 Testing {name} endpoint: {endpoint}\")\n",
    "        print(f\"Expected limit: {expected_limit} requests per window\")\n",
    "        \n",
    "        headers = {}\n",
    "        if self.token:\n",
    "            headers[\"Authorization\"] = f\"Bearer {self.token}\"\n",
    "\n",
    "        success_count = 0\n",
    "        rate_limited_count = 0\n",
    "        \n",
    "        for i in range(expected_limit + 5):  # Try a few extra requests\n",
    "            try:\n",
    "                response = await self.client.post(\n",
    "                    f\"{self.base_url}{endpoint}\",\n",
    "                    json=payload,\n",
    "                    headers=headers\n",
    "                )\n",
    "                \n",
    "                # Check rate limit headers\n",
    "                remaining = response.headers.get(\"X-RateLimit-Remaining\")\n",
    "                limit = response.headers.get(\"X-RateLimit-Limit\")\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    success_count += 1\n",
    "                    print(f\"  ✅ Request {i+1}: Success (Remaining: {remaining}/{limit})\")\n",
    "                elif response.status_code == 429:\n",
    "                    rate_limited_count += 1\n",
    "                    retry_after = response.headers.get(\"Retry-After\", \"unknown\")\n",
    "                    print(f\"  🚫 Request {i+1}: Rate limited (Retry-After: {retry_after}s)\")\n",
    "                    break\n",
    "                else:\n",
    "                    print(f\"  ❌ Request {i+1}: Error {response.status_code}\")\n",
    "                    break\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  ❌ Request {i+1}: Exception {e}\")\n",
    "                break\n",
    "\n",
    "        print(f\"📊 Results: {success_count} successful, {rate_limited_count} rate limited\")\n",
    "        return success_count, rate_limited_count\n",
    "\n",
    "    async def test_login_rate_limit(self):\n",
    "        \"\"\"Test login rate limiting.\"\"\"\n",
    "        print(\"\\n🔍 Testing login rate limiting\")\n",
    "        \n",
    "        rate_limited_count = 0\n",
    "        for i in range(10):  # Try more than the limit\n",
    "            try:\n",
    "                response = await self.client.post(\n",
    "                    f\"{self.base_url}/api/v1/token\",\n",
    "                    data={\"username\": \"alice\", \"password\": \"wrongpassword\"}\n",
    "                )\n",
    "                \n",
    "                if response.status_code == 401:\n",
    "                    print(f\"  ✅ Login attempt {i+1}: Expected 401 (invalid credentials)\")\n",
    "                elif response.status_code == 429:\n",
    "                    rate_limited_count += 1\n",
    "                    retry_after = response.headers.get(\"Retry-After\", \"unknown\")\n",
    "                    print(f\"  🚫 Login attempt {i+1}: Rate limited (Retry-After: {retry_after}s)\")\n",
    "                    break\n",
    "                else:\n",
    "                    print(f\"  ❌ Login attempt {i+1}: Unexpected {response.status_code}\")\n",
    "                    break\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  ❌ Login attempt {i+1}: Exception {e}\")\n",
    "                break\n",
    "\n",
    "        print(f\"📊 Login rate limit results: {rate_limited_count} rate limited\")\n",
    "        return rate_limited_count > 0\n",
    "\n",
    "    async def close(self):\n",
    "        \"\"\"Close the HTTP client.\"\"\"\n",
    "        await self.client.aclose()\n",
    "\n",
    "async def main():\n",
    "    \"\"\"Run all rate limiting tests.\"\"\"\n",
    "    print(\"🚀 Starting rate limiting tests...\")\n",
    "    \n",
    "    tester = RateLimitTester()\n",
    "    \n",
    "    try:\n",
    "        # Test login rate limiting first\n",
    "        login_rate_limited = await tester.test_login_rate_limit()\n",
    "        \n",
    "        # Login to get token for authenticated endpoints\n",
    "        if not await tester.login():\n",
    "            print(\"❌ Cannot proceed without login\")\n",
    "            return\n",
    "        \n",
    "        # Test iris prediction (light limit)\n",
    "        iris_success, iris_rate_limited = await tester.test_endpoint(\n",
    "            \"/api/v1/iris/predict\",\n",
    "            {\n",
    "                \"model_type\": \"rf\",\n",
    "                \"samples\": [{\"sepal_length\": 5.1, \"sepal_width\": 3.5, \"petal_length\": 1.4, \"petal_width\": 0.2}]\n",
    "            },\n",
    "            \"Iris Prediction\",\n",
    "            120  # Should be 2x default limit\n",
    "        )\n",
    "        \n",
    "        # Test cancer prediction (heavy limit)\n",
    "        cancer_success, cancer_rate_limited = await tester.test_endpoint(\n",
    "            \"/api/v1/cancer/predict\",\n",
    "            {\n",
    "                \"model_type\": \"bayes\",\n",
    "                \"samples\": [{\"mean_radius\": 17.99, \"mean_texture\": 10.38, \"mean_perimeter\": 122.8, \"mean_area\": 1001, \"mean_smoothness\": 0.1184, \"mean_compactness\": 0.2776, \"mean_concavity\": 0.3001, \"mean_concave_points\": 0.1471, \"mean_symmetry\": 0.2419, \"mean_fractal_dimension\": 0.07871, \"se_radius\": 1.095, \"se_texture\": 0.9053, \"se_perimeter\": 8.589, \"se_area\": 153.4, \"se_smoothness\": 0.006399, \"se_compactness\": 0.04904, \"se_concavity\": 0.05373, \"se_concave_points\": 0.01587, \"se_symmetry\": 0.03003, \"se_fractal_dimension\": 0.006193, \"worst_radius\": 25.38, \"worst_texture\": 17.33, \"worst_perimeter\": 184.6, \"worst_area\": 2019, \"worst_smoothness\": 0.1622, \"worst_compactness\": 0.6656, \"worst_concavity\": 0.7119, \"worst_concave_points\": 0.2654, \"worst_symmetry\": 0.4601, \"worst_fractal_dimension\": 0.1189}]\n",
    "            },\n",
    "            \"Cancer Prediction\",\n",
    "            30  # Should be cancer limit\n",
    "        )\n",
    "        \n",
    "        # Test training endpoints (very limited)\n",
    "        training_success, training_rate_limited = await tester.test_endpoint(\n",
    "            \"/api/v1/iris/train\",\n",
    "            {},\n",
    "            \"Iris Training\",\n",
    "            2  # Should be training limit\n",
    "        )\n",
    "        \n",
    "        # Summary\n",
    "        print(\"\\n📋 Test Summary:\")\n",
    "        print(f\"  Login rate limiting: {'✅ Working' if login_rate_limited else '❌ Not working'}\")\n",
    "        print(f\"  Iris prediction rate limiting: {'✅ Working' if iris_rate_limited else '❌ Not working'}\")\n",
    "        print(f\"  Cancer prediction rate limiting: {'✅ Working' if cancer_rate_limited else '❌ Not working'}\")\n",
    "        print(f\"  Training rate limiting: {'✅ Working' if training_rate_limited else '❌ Not working'}\")\n",
    "        \n",
    "        all_working = login_rate_limited and iris_rate_limited and cancer_rate_limited and training_rate_limited\n",
    "        print(f\"\\n🎯 Overall: {'✅ All rate limits working' if all_working else '❌ Some rate limits not working'}\")\n",
    "        \n",
    "    finally:\n",
    "        await tester.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main()) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
